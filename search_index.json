[["index.html", "Statistics I I Descriptive Statistics Introduction", " Statistics I Maria Osipenko 2024-09-17 I Descriptive Statistics Introduction Why Statistics? Example 0.1 (Analysing customer behavior) Imaging that you work in a data analysts team for some company running online shops. Your customers order stuff online using either a mobile application (app) or web interface (web). They login, spend some time in the shop and then they (hopefully) buy a lot of your products. Your task is to improve customer experience in order to boost the sales. You may need to answer the following questions: Do the customers order more, if they are using the app or the web? (\\(\\leadsto\\) analyse features orders and app usage in the population of all customers or in a sample if the data on the whole population is not available) Does the time spend in the shop by a person has anything to do with the amount of her orders? (\\(\\leadsto\\) analyse features time spent and orders in the population of all customers or in a sample if the data on the whole population is not available) Is the current discount campaign successful (meaning that it persistently increases the mean sales)? (\\(\\leadsto\\) analyse feature sales in a sample (pilot group of campaign customers) to conclude on the same feature in the whole population (all customers)) Can we predict app usage/ sales amount in a new market segment based on a customer survey in this new market segment? (\\(\\leadsto\\) analyse app usage and sales in a sample (survey) to conclude on the population of potential customers in the new market segment) Data analysis with statistical methods: Confirm or specify assumptions with data Uncover new correlations, determine influencing factors (Further) developing products/advertising, modeling behavior, predicting business development, ‚Ä¶ Definition 0.1 (Key Concepts) \\(\\Omega\\): Population, is a set of uniform objects, also referred to as statistical units \\(\\omega\\in\\Omega\\), which are characterized by the same characteristics. \\(X\\): feature or characteristic, is an observable property of the elements of \\(\\Omega\\). Each feature can take on different feature values. \\(A_X\\): Set of expressions of the characteristic \\(X\\). \\(X_1, X_2, \\ldots , X_n\\): Sample is the random selection of objects from the population. After drawing a sample, the measurement series (the determination of the respective characteristics of one or more features) is created on the randomly selected objects. \\(n\\): sample size \\(x_i\\): \\(i\\)-th sample value/measurement value Example 0.2 (Sampling in a customer survey) The plan is to randomly select \\(n\\) customers and survey them. App Usage: The sample values are: \\(\\color{blue}{y_1, y_2,\\ldots,y_n},\\) where \\(y_i\\) represents the app usage of the \\(i\\)th selected person. Sales: The sample values are: \\({\\color{red}{x_1, x_2,\\ldots,x_n}},\\) where \\(x_i\\) represents the estimate of the sales amount of the \\(i\\)th selected person. Sample size \\(n\\): 10 20 50 100 Sample realizations: The survey results for App usage (yes/no) as realized sample values: \\(\\color{blue}{\\{1,2,\\ldots, n\\}=} ~\\) As realizations of the corresponding 0-1 Bernoulli random variable: \\(\\color{blue}{\\{y_1,y_2,\\ldots,y_n\\}=}~\\) Willingness to pay in form of self-estimated invoice amount with sample values: \\(\\color{red}{\\{x_1,x_2,\\ldots,x_n\\}=}~\\) Calculation of the resulting estimates: The estimate for the proportion of app users: \\(\\color{blue}{\\bar y=\\frac 1n(y_1+y_2 + \\ldots + y_n)=}\\) The estimate for the average invoice amount: \\(\\color{red}{\\bar x=\\frac 1n(x_1+x_2 + \\ldots + x_n)= }\\) draw a new sample "],["features-frequencies-empirical-distribution.html", "Chapter 1 Features, frequencies, empirical distribution 1.1 Features 1.2 Frequencies 1.3 Empirical distribution", " Chapter 1 Features, frequencies, empirical distribution We‚Äôll start by looking at different data and how they differ, and how to create clear visualizations for different types of data. In this chapter we will learn What features and feature values are and what kinds of feature scale types exist. How to calculate frequencies for features depending on the scale and represent them in afunctional form as empirical probability, density and distribution functions. 1.1 Features Features or Characteristics are different aspects of objects in the population. We want to examine these aspects. The characteristics of interest are determined before a sample is drawn. The values of the features are then measured on the objects randomly selected from the population. Example 1.1 (Customer characteristics) \\(\\Omega\\): Customers, i.e.¬†objects \\(w\\) are customers. Characteristics: \\(A=\\) Payment (means of payment), \\(B=\\) Rating (product rating: stars 1 - 5), \\(C=\\) Orders (number of orders in the last month), \\(D=\\) Usage (app usage or web interface usage), \\(X=\\) Time_spent (average time spent in the online store in minutes) and \\(Y=\\) Sales amount (average sales per order). Table 1.1: Sample of size \\(n=10\\) for the characteristics \\(A, B, C, D, X\\) and \\(Y\\) of customers. customer payment rating orders usage time_spent sales 1 Credit card 1 2 Web 14.94 46.75 2 Debit payment 2 2 App 62.79 99.48 3 PayPal 3 3 App 79.12 6.29 4 PayPal 4 5 Web 25.48 19.06 5 Bill payment 2 2 App 20.55 64.63 6 Bill payment 5 5 App 11.95 5.08 7 PayPal 3 6 Web 21.00 7.89 8 PayPal 4 4 App 47.81 8.77 9 Debit payment 5 3 App 10.71 6.10 10 Credit card 4 1 App 8.44 26.34 Scale types The values of different features can have different ‚Äòdepth of information‚Äô. Depending on which values a characteristic has, they can be assigned to one of the following scale types. Nominal scale: the values are equal, no relation of the values (\\(&gt;,&lt;\\)) or ranking possible. Ordinal scale: the values can be ranked, e.g.¬†greater, better, etc. Metric scale: each characteristic represents a value on the number line. Not only comparisons (larger, smaller) but also the distances between the values make sense. discrete: \\(A_X = \\{a_1, a_2,\\ldots,a_J\\}\\) or \\(A_X = \\{a_1, a_2,\\ldots\\}\\), continuous: \\(A_X\\subseteq \\mathbb R\\) an interval on the real numbers. Example 1.2 (Merkmale von Kunden) Example 1.1 cont. \\(A=\\) Payment: nominal, \\(B=\\) Rating: ordinal, \\(C=\\) Orders: metric discrete, \\(D=\\) Usage: metric discrete, \\(X=\\) Time_spent: metric continuous, \\(Y=\\) Sales amount: metric continuous. Exercise 1.1 (Features of corporate customers.) \\(\\Omega\\): Corporate customers with different characteristics: \\(X_1\\): Number of employees, \\(X_2\\): Debt-equity ratio, \\(X_3\\): Type of company, \\(X_4\\): Rating category. Turn ‚Äúnominal‚Äù into ‚Äúmetric‚Äù‚Ä¶ A nominally scaled feature \\(X\\) with sample values \\(x_1,\\ldots, x_n\\) can be transformed into a feature with a metric scale if required. For each value \\(a_j,j=1,\\ldots, J\\) in the value set \\(A_X\\), a new variable \\(D_{a_j}\\) (dummy variable) is defined with the values \\(d_{i,a_j}, i=1,\\ldots, n\\), which are determined on the following basis: \\[\\begin{align} d_{i,a_j} = \\begin{cases}1, &amp;\\text{ if } x_i = a_j,\\\\ 0, &amp;\\text{ else. }\\end{cases} \\end{align}\\] Example 1.3 (App or web usage.) Feature \\(D\\) ‚ÄúUsage‚Äù has two values \\(A_D = \\{\\text{App}, \\text{Web}\\}\\). We define two new variables \\(D_{\\text{App}}\\) with \\[\\begin{align} d_{i,\\text{App}} = \\begin{cases}1, &amp;\\text{ if } x_i = \\text{App},\\\\ 0, &amp;\\text{ else. }\\end{cases} \\end{align}\\] \\(D_{\\text{Web}}\\) with \\[\\begin{align} d_{i,\\text{Web}} = \\begin{cases}1, &amp;\\text{ if } x_i = \\text{Web},\\\\ 0, &amp;\\text{ else. }\\end{cases} \\end{align}\\] All values are summarized again in the following table Table 1.2: Sample of size \\(n=10\\) to feature \\(D\\) ‚ÄúUsage‚Äù and the dummy variables with metric scale and values \\(0\\) and \\(1\\). Customer \\(i\\) Usage \\(x_i\\) \\(D_{App}\\) \\(D_{Web}\\) 1 Web 0 1 2 App 1 0 3 App 1 0 4 Web 0 1 5 App 1 0 6 App 1 0 7 Web 0 1 8 App 1 0 9 App 1 0 10 App 1 0 Such a transformation can be useful if you want to use the data as input into a model (e.g.¬†linear regression). 1.2 Frequencies Objective: Starting from a sample with size \\(n\\) for a characteristic \\(X\\) with: the sample values \\(x_1, x_2, \\ldots, x_n\\), the value set \\(A_x = \\{a_1,a_2,\\ldots, a_J\\}\\), with a total of \\(J\\) different values, calculate the frequencies for the values. Table 1.3: Sample of size \\(n=10\\) for the characteristics \\(A, B, C, D, X\\) and \\(Y\\) of customers. customer payment rating orders usage time_spent sales 1 Credit card 1 2 Web 14.94 46.75 2 Debit payment 2 2 App 62.79 99.48 3 PayPal 3 3 App 79.12 6.29 4 PayPal 4 5 Web 25.48 19.06 5 Bill payment 2 2 App 20.55 64.63 6 Bill payment 5 5 App 11.95 5.08 7 PayPal 3 6 Web 21.00 7.89 8 PayPal 4 4 App 47.81 8.77 9 Debit payment 5 3 App 10.71 6.10 10 Credit card 4 1 App 8.44 26.34 1.2.1 Frequencies without class formation \\(\\leadsto\\) for features with nominal, ordinal or discrete metric scale (the values are repeated several times). Absolute frequency of value \\(a_j\\): \\[H_j=H(a_j)=number~of~measured~values~with~x_i=a_j.\\] Relative frequency of value \\(a_j\\): \\[h_j=h(a_j)=\\frac{H_j}{n}.\\] Frequency distribution: \\[\\begin{array}{cccc} a_{1} &amp; a_{2} &amp;\\ldots &amp;a_{J}\\\\ h_{1} &amp; h_{2} &amp;\\ldots &amp;h_{J} \\end{array}\\] Example 1.4 (Payment.) See Table 1.3. Table 1.4: Relative frequencies for the feature \\(A\\): Means of payment. Bill payment Credit card Debit payment PayPal 0.2 0.2 0.2 0.4 Figure 1.1: Balkendiagramm Exercise 1.2 (Frequency of ratings) See Table 1.3. 1.2.2 Frequencies with class formation With continuous or discrete metric scaling (the values are rarely repeated), the actual values are summarized in classes. For example, sales from 0 to 25 euros could be included in a class \\([0;25)\\), etc. Class formation \\(A_X=K_1\\cup K_2 \\cup \\ldots \\cup K_J\\) with \\(K_i\\cap K_j=\\emptyset\\) for \\(i\\not=j\\), (i.e.¬†divide the value range into \\(J\\) intervals that do not overlap). Class \\(K_j=[b_{j-1}; b_j)\\) has the class width \\(d_j=b_j - b_{j-1}\\), \\(b_{j-1}\\) is the lower limit (contained in the class); \\(b_j\\) is the upper limit (lies outside the class). Absolute frequency of \\(K_j\\): \\[H_j=H(K_j)=number~of~measured~values~with~x_i\\in K_j.\\] Relative frequency of \\(K_j\\): \\[h_j=h(K_j)=\\frac{H_j}{n}.\\] Frequency distribution: \\[\\begin{array}{cccc} K_{1} &amp; K_{2} &amp;\\ldots &amp;K_{J}\\\\ h_{1} &amp; h_{2} &amp;\\ldots &amp;h_{J} \\end{array}\\] Class mean: arithmetic mean of the sample values in \\(K_j\\) (class representative) \\[a_j = \\frac 1{H_{j}}\\sum_{x_i\\in K_j}x_i\\] Class center: Center of the interval (as a substitute for class means if they cannot be calculated) \\[a_j = \\frac{b_j + b_{j-1}}{2}\\] Example 1.5 (Sales) See Table 1.3. Several class divisions are conceivable for characteristic \\(Y\\). Equal class width: Table 1.5: Class devision (variant 1) for feature \\(Y\\). \\(j\\) \\(K_j\\) \\(H_j\\) \\(h_j\\) \\(a_j\\) Class_mean \\(a_j\\) Class_center 1 [0,25) 6 0.6 8.87 12.5 2 [25,50) 2 0.2 36.55 37.5 3 [50,75) 1 0.1 64.63 62.5 4 [75,100] 1 0.1 99.48 87.5 Histogram: bars Width of the bars: \\(d_j=b_j - b_{j-1}\\) Height of the bars: density \\(\\frac{h_j}{d_j}\\) of the class \\(K_j\\) Area of the beams: \\(h_j\\). Figure 1.2: Histogram for Y: equal class width Unequal class width: Several class divisions are conceivable for characteristic Table 1.6: Class devision (variant 2) for feature \\(Y\\). \\(j\\) \\(K_j\\) \\(H_j\\) \\(h_j\\) \\(a_j\\) Class_mean \\(a_j\\) Class_center 1 [0,10) 5 0.5 6.83 5 2 [10,30) 2 0.2 22.7 20 3 [30,50) 1 0.1 46.75 40 4 [50,100] 2 0.2 82.06 75 Figure 1.3: Histogram for Y: unequal class width Exercise 1.3 (Employee age.) Classification of the feature ‚Äúage of employees‚Äù. 1.3 Empirical distribution Empirical distribution is given in functional form by the empirical probability and distribution functions (discrete case) and by the empirical density and distribution functions (continuous case). 1.3.1 Empirical probability and distribution functions (discrete case) \\(X\\) is a metrically scaled characteristic with possible values \\(A_X=\\{a_1,a_2,\\ldots, a_J\\}\\). Values \\(x_1,x_2,\\ldots, x_n\\) represent a random sample of \\(X\\). Definition 1.1 (Empirical probability and distribution functions) The empirical probability function is the function \\(\\hat f_X:\\mathbb R\\rightarrow[0,1]\\) with \\[ \\hat{f}_X(x)=\\hat{f}(x) = \\frac{\\text{number of sample values with } x_i= x}{n} = \\begin{cases} h(a_j), &amp; x=a_j,\\\\ 0 ,&amp;\\text{other} \\end{cases} \\] The empirical distribution function is the function \\(\\hat F_X:\\mathbb R\\rightarrow[0,1]\\) with \\[ \\hat{F}_X(x)=\\hat{F}(x) = \\frac{\\text{Anzahl der Stichprobenwerte mit } x_i\\leq x}{n} = \\sum_{j: a_j\\leq x} h(a_j). \\] Example 1.6 (Probability and distribution functions for feature: Orders.) See Table 1.3. Metrically scaled characteristic \\(C\\), the number of orders Sample of size \\(n=10\\): 2,2,3,5,2,5,6,4,3,1 \\[\\hat f(x)=\\begin{cases}0.1 &amp;\\text{for} ~x\\in\\{1;4;6\\},\\\\ 0.2 &amp;\\text{for}~x\\in\\{3;5\\},\\\\ 0.3 &amp;\\text{for}~x=2,\\\\ 0 &amp;\\text{otherwise}\\end{cases}\\] \\[\\hat F(x)=\\begin{cases}0&amp;\\text{for} ~x&lt;1,\\\\ 0.1 &amp;\\text{for}~1\\leq x&lt;2,\\\\ 0.4 &amp;\\text{for}~2\\leq x&lt;3,\\\\ 0.6 &amp;\\text{for}~3\\leq x&lt;4,\\\\ 0.7 &amp;\\text{for}~4\\leq x&lt;5,\\\\ 0.9 &amp;\\text{for}~5\\leq x&lt;6,\\\\ 1 &amp;\\text{for}~x\\leq 6.\\\\\\end{cases}\\] Figure 1.4: Empirical probability and distribution functions for \\(C\\): Orders 1.3.2 Empirical density and distribution functions (continuous case) Assumption: Density of a class is evenly distributed over the entire width of the class: \\[\\hat f(x) = \\frac{h_j}{d_j}, x\\in K_j=[b_{j-1};~b_j),\\] where \\(h_j\\) is the relative frequency and \\(d_j=b_j - b_{j-1}\\) is the width of \\(K_j\\). Relative frequencies = empirical probabilities are areas under the density function. If \\(\\hat f\\) is constant within a class \\(K_j\\), then the cumulative distribution function \\(\\hat F\\) (= cumulative empirical probability) is a linear function of \\(x\\) with increase \\(\\hat f\\) for \\(K_j\\). So \\(\\hat F(x), x\\in K_j\\) is a line between the points \\((b_{j-1};~\\sum_{k=1}^{j-1}h_k)\\) (the latter is the cumulative frequency up to \\(K_{j-1}\\)) and \\((b_j; ~\\sum_{k=1}^{j}h_k)\\) (the latter is the cumulative frequency up to \\(K_{j}\\)). \\[\\hat F(x) = \\sum_{k=1}^{j-1}h_k + \\hat f(x)\\cdot(x-b_{j-1}), x\\in K_j=[b_{j-1};~b_j)\\] Relationship between \\(\\hat F(x)\\) and \\(\\hat f(x)\\): \\[\\hat F(x)=\\int_{-\\infty}^x\\hat f(t)dt~\\text{ and } \\hat f(x) = \\hat F ^\\prime (x)\\] Example 1.7 (Density and distribution functions for the sales amount.) See Table 1.3. Table 1.7: Relative frequencies and cumulative frequencies for the characteristic \\(Y\\): Sales amount. [0,10) [10,30) [30,50) [50,100] rel. Frequency 0.5 0.2 0.1 0.2 cumulative rel. Frequency 0.5 0.7 0.8 1.0 metric scaled characteristic \\(Y\\): Invoice amount, Sample of size \\(n=10\\): 46.75; 99.48; 6.29; 19.06; 64.63; 5.08; 7.89; 8.77; 6.1; 26.34, Classification: [0,10); [10,30); [30,50); [50,100] \\[\\hat f(x)=\\begin{cases}0.05 &amp;\\text{for} ~x\\in [0;10),\\\\ 0.01 &amp;\\text{for}~x\\in [10;30),\\\\ 0.005&amp;\\text{for}~x\\in [30;50),\\\\ 0.004&amp;\\text{for}~x\\in [50;100],\\\\ 0,\\text{sonst.}\\end{cases}\\] \\[\\hat F(x)=\\begin{cases}0&amp;\\text{for} ~x&lt;0,\\\\ 0 + 0.05\\cdot(x-0) = 0.05\\cdot x &amp;\\text{for}~0\\leq x&lt;10,\\\\ 0.5 + 0.01\\cdot (x-10) = 0.4 + 0.01x &amp;\\text{for}~10\\leq x&lt;30,\\\\ 0.7 + 0.005\\cdot(x-30) = 0.55 + 0.005\\cdot x &amp;\\text{for}~30\\leq x&lt; 50,\\\\ 0.8 + 0.004\\cdot(x-50) = 0.6 + 0.004\\cdot x &amp;\\text{for}~50\\leq x\\leq 100,\\\\ 1&amp;\\text{for}~x&gt; 1\\end{cases}\\] Figure 1.5: Empirical density and distribution functions for \\(Y\\): Sales amount Exercise 1.4 (Density and distribution functions for time spent in the online store.) See Table 1.3. Table 1.8: Relative frequencies and cumulative frequencies for the feature \\(X\\): Time spent in the online store. [0,10) [10,20) [20,60) [60,80] rel. Frequency 0.1 0.3 0.4 0.2 cumulative rel. Frequency 0.1 0.4 0.8 1.0 metric continuously scaled characteristic \\(X\\): Time in the online store, Sample of size \\(n=10\\): 14.94; 62.79; 79.12; 25.48; 20.55; 11.95; 21; 47.81; 10.71; 8.44, Class division: [0,10); [10,20); [20,60); [60,80] Enter the values of the density function for the classes and try to interpret the graphical distribution function! "],["sample-measures.html", "Chapter 2 Sample Measures Data Representation 2.1 Measures of Central Tendency 2.2 Measures of Variability 2.3 Quantile and Boxplots", " Chapter 2 Sample Measures You often want to summarize larger data sets in a few meaningful numbers in order to get an idea of the data distribution and to determine patterns in the data. For example, if you want to compare empirical distributions of the number of orders from two business periods: \\[\\begin{array}{r|llllll} a_j&amp;1&amp;2&amp;3&amp;4&amp;5&amp;6\\\\\\hline \\text{period 1: }h_j&amp;0.1&amp;0.3&amp;0.2&amp;0.1&amp;0.2&amp;0.1\\\\\\ \\text{period 2: }h_j&amp;0.1&amp;0.25&amp;0.1&amp;0.25&amp;0.1&amp;0.2 \\end{array}\\] How can you determine, which empirical distribution is better for the business? In this chapter we will learn How to select meaningful measures for a sample that summarize its distribution , What types of such measures exist and how to calculate and interpret them. How the whole empirical distribution can be described by quantiles and boxplots. Our guiding example for this is the question: What characterizes our typical customer? Distributions of several customer characteristics should be summarized, so that the product development team has an idea of the customer base. A distinction is made between the following types of measures: Measures of the Central Tendency in the data, which reflect the central value/the average position of the observed values. Measures of the Dispersion in the data, which quantify the variability in the values. Quantiles, which reveal the relative position of the values over their range. Data Representation Empirical data can be available in different representations as: raw sample values (measured/ observed values): \\(x_1,x_2,\\ldots, x_n\\) Example 2.1 (Sales amount.) See Table 1.3. Table 2.1: Sample of the size \\(n=10\\) for the characteristic: \\(Y\\) sales amount per order. Customer 1 2 3 4 5 6 7 8 9 10 Sales 46.75 99.48 6.29 19.06 64.63 5.08 7.89 8.77 6.1 26.34 ordered sample values (sorted in ascending order): \\(x_{[1]}\\leq x_{[2]}\\leq\\ldots\\leq x_{[n]}\\) Example 2.2 (Time in Online-Shop.) See Table 1.3. Table 2.2: Sample size \\(n=10\\) for the characteristic: \\(X\\) Time in the online store. i 1 2 3 4 5 6 7 8 9 10 Time 8.44 10.71 11.95 14.94 20.55 21 25.48 47.81 62.79 79.12 Frequency Distribution discrete: \\[ \\begin{array}{ccccc} \\text{Values}&amp;a_1&amp;a_2&amp;\\ldots&amp;a_J\\\\ \\text{Frequencies}&amp;h_1&amp;h_2&amp;\\ldots&amp;h_J\\\\ \\end{array} \\] Example 2.3 (Orders.) See Table 1.3. Table 2.3: Frequencies for a sample of size \\(n=10\\) for the characteristic \\(C\\): Number of orders. Orders 1 2 3 4 5 6 rel.Frequency 0.1 0.3 0.2 0.1 0.2 0.1 continuous: \\[ \\begin{array}{ccccc} \\text{Classes}&amp;K_1&amp;K_2&amp;\\ldots&amp;K_J\\\\ \\text{Class means/ centers}&amp;a_1&amp;a_2&amp;\\ldots&amp;a_J\\\\ \\text{Frequencies}&amp;h_1&amp;h_2&amp;\\ldots&amp;h_J\\\\ \\end{array} \\] Example 2.4 (monthly Income) Frequencies of income groups according to an online survey with \\(n=10\\). Table 2.4: Frequencies for a sample of size \\(n=10\\) for the characteristic \\(Z\\): monthly income (TEUR). Income [0,1) [1,2) [2,4) [4,5] rel.Frequency 0.1 0.1 0.5 0.3 Exercise 2.1 (Form of Data) 2.1 Measures of Central Tendency 2.1.1 Sample Mean The sample mean aka the sample average or the empirical expectation (referred to as \\(\\bar x\\) or \\(\\hat\\mu\\)) reflects the average position of the values of a feature, so that if you add all deviations of the values to the sample mean, the result is zero. The sample mean of a sample, \\(x_1, x_2, \\ldots, x_n\\), is: \\[\\begin{align} \\overline x = \\frac1n(x_1 + x_2 + \\ldots + x_n) = \\frac 1n\\sum_{i=1}^nx_i \\tag{2.1} \\end{align}\\] Example 2.5 (Sales amount.) Cont. Example 2.1. Table 2.5: Sample of size \\(n=10\\) for the characteristic \\(Y\\): Sales amount. Customer 1 2 3 4 5 6 7 8 9 10 Sales 46.75 99.48 6.29 19.06 64.63 5.08 7.89 8.77 6.1 26.34 \\(\\overline y = \\frac 1{10}(y_1 + y_2 + \\ldots + y_{10}) = \\frac 1{10}(46,75 + 99,48 + \\ldots + 26,34) = 29.039\\) Example 2.6 (Time in the online store by time of day.) If you have two samples for the same characteristic but under the influence of other factors (here: time of day), you can compare the sample means of the two samples. Table 2.6: Table 2.7: Sample size n=10 for the characteristic: time spent in the online store in two periods of the day: midday and evening on weekdays and at weekends (WE). [i] 1 2 3 4 5 6 7 8 9 10 Time_midday 0.57 6.82 13.95 30.86 36.56 37.34 37.4 38.42 39.97 51.65 Time_evening 22.41 27.87 32.02 33.93 34.35 35.08 65.4 83.23 100.48 110.81 Time_WE 23.6 24.12 34.31 39.69 47.24 48.5 67.31 92.95 94.82 102.32 midday: \\(\\overline z_1 = \\frac 1{10}(z_{1,1} + z_{1,2} + \\ldots + z_{1,10}) = \\frac 1{10}(0.57 + 6.82 + \\ldots + 51.65) = 29.354;\\) evening: \\(\\overline z_2 = \\frac 1{10}(z_{2,1} + z_{2,2} + \\ldots + z_{2,10}) = \\frac 1{10}(22.41 + 27.87 + \\ldots + 110.81) = 54.558;\\) WE: \\(\\overline z_3 = \\frac 1{10}(z_{3,1} + z_{3,2} + \\ldots + z_{3,10}) = \\frac 1{10}(23.6 + 24.12 + \\ldots + 102.32) = 57.486.\\) Apparently, on average, our customers spend much more time on the website in the evening than they do at lunchtime. Interestingly, they also spend on average less time than expected on your website at the weekend (when they presumably have a lot of free time). Using a frequency distribution form for a sample, the empirical expectation or the sample mean is calculated as follows: \\[ \\hat\\mu= a_1h_1+a_2h_2+\\ldots+a_Jh_J = \\sum_{j=1}^Ja_jh_j \\] Example 2.7 (Orders.) Cont. of Example 2.3. Table 2.8: Sample of size \\(n=10\\) for the characteristic \\(B\\): Number of orders. Number 1 2 3 4 5 6 rel.frequency 0.1 0.3 0.2 0.1 0.2 0.1 \\(\\hat\\mu_B = a_1\\cdot h_1 + a_2\\cdot h_2 + \\ldots + a_{6}\\cdot h_{6} = 1\\cdot0.1 + 2\\cdot0.3 + \\ldots + 6\\cdot0.1 = 3.3\\) Example 2.8 (monthly income.) Cont. of Example 2.4. Table 2.9: Sample size \\(n=10\\) for the characteristic \\(Z\\): monthly income of customers. Income group [0,1) [1,2) [2,4) [4,5] rel.frequency 0.1 0.1 0.5 0.3 The class centers are: \\(a_1 = 0,5\\), \\(a_2 = 1,5\\), \\(a_3 = 3\\) und \\(a_4 = 4,5\\). Also: \\(\\hat\\mu_Z = a_1\\cdot h_1 + a_2\\cdot h_2 + a_{3}\\cdot h_{3} + a_{4}\\cdot h_{4}= 0,5\\cdot0.1 + 1,5\\cdot0.1 + 3\\cdot0.5 + 4,5\\cdot0.3 = 3.05.\\) 2.1.2 Sample Median The sample median is another measure that represents the central value of given sample values. The sample median is defined in such a way that at least 50% of the sample values are above and below the median value. The animation shows how the median differs from the sample mean: For an ordered sample \\(x_{[1]}\\leq x_{[2]}\\leq\\ldots\\leq x_{[n]}\\), the sample median is defined as: \\[ x_{med}=\\begin{cases}x_{[\\frac{n+1}2]},&amp; \\text{if }n \\text{ odd},\\\\ \\frac 12\\left(x_{\\frac n2} + x_{\\frac n2+1}\\right), &amp;\\text{if }n \\text{ even}. \\end{cases} \\] Example 2.9 (Sales amount.) Cont. of Example 2.5. An ordered sample is required to calculate the empirical median, i.e: \\(\\begin{array}{ccccccccccc}\\hline [i]&amp;1&amp;2&amp;3&amp;4&amp;5&amp;6&amp;7&amp;8&amp;9&amp;10\\\\\\hline y_{[i]}&amp; 5.08&amp;6.1&amp;6.29&amp;7.89&amp;8.77&amp;19.06&amp;26.34&amp;46.75&amp;64.63&amp;99.48\\\\\\hline\\end{array}.\\) Since \\(n=10\\) even \\(y_{med}=\\frac 12 (y_{[5]} + y_{[6]}) = \\frac 12(8.77 + 19.06) = 13.915.\\) If the last observation \\(y_{[10]}\\) were invalid, we would have \\(n=9\\) odd: \\(\\begin{array}{ccccccccccc}\\hline [i]&amp;1&amp;2&amp;3&amp;4&amp;5&amp;6&amp;7&amp;8&amp;9\\\\\\hline y_{[i]}&amp; 5.08&amp;6.1&amp;6.29&amp;7.89&amp;8.77&amp;19.06&amp;26.34&amp;46.75&amp;64.63\\\\\\hline\\end{array},\\) and \\(y_{med}=y_{[5]} = 8,77.\\) Exercise 2.2 (The sample Median for the number of orders.) The same calculation logic can be applied to data with a given discrete frequency distribution. Write out the data (\\(n=10\\)) in the example 2.7 as an ordered sample (e.g.¬†1 has a relative frequency of 0.1, i.e.¬†this value only occurred once in the original sample. It is similar with the other values: 2 occurred 3 times, 3 occurred 2 times, 4 - once, 5 - 2 times and 6 - only once). Can you come up with \\(x_{med}=3\\)? In the case of a continuous frequency distribution as in the example 2.8, the sample median can be determined using the empirical distribution function. Here, \\(x_{med}\\) is a number so that: \\(\\hat F(x_{med}) = 0.5.\\) Therefore, \\(x_{med} = \\hat F^{-1}(0.5)\\). Example 2.10 (Z=monthly income) Cont. of Example 2.8 The empirical distribution function for the data (income class) is (try to understand for yourself how this comes about): \\[ \\hat F(x) = \\begin{cases} 0,&amp; x&lt;0,\\\\ 0.1\\cdot x, &amp;0\\leq x &lt; 1,\\\\ 0.1+0.1\\cdot(x - 1), &amp;1\\leq x &lt; 2,\\\\ 0.2+0.25\\cdot(x - 2), &amp;2\\leq x \\leq 4,\\\\ 0.7+0.3\\cdot(x - 4), &amp;4\\leq x \\leq 5,\\\\ 1,&amp;x&gt;5. \\end{cases} \\] Since \\(\\hat F(x)\\) takes the value 0.5 in the section where \\(2\\leq x &lt;4\\), set \\(0.2+0.25\\cdot(x_{med} - 2) = 0.5\\) and convert to \\(x_{med}\\). The result is \\(x_{med} = 3.2.\\) 2.1.3 Geometric Mean Sometimes the data is not provided in its original form but in a transformed version, e.g.¬†as growth factors. The mean value of these growth factors can be misleading as a key figure for average growth. In this case, the geometric mean is used. For a sample of growth factors \\(q_1,q_2,\\ldots,q_n\\), the geometric mean \\(q_{geom}\\) is defined as: \\[ q_{geom} = (q_1\\cdot q_2\\cdot \\ldots \\cdot q_n)^{\\frac 1n} = \\sqrt[n]{q_1\\cdot q_2\\cdot \\ldots \\cdot q_n}. \\] Example 2.11 (Customer growth.) New customer figures over four quarters with the growth factors \\(q_i\\). Table 2.10: Stichprobe vom Umfang \\(n=3\\) in Form von Zuwachsfaktoren \\(q_i\\). \\(Q_1\\) \\(Q_2\\) \\(Q_3\\) \\(Q_4\\) Quartal Q1 Q2 Q3 Q4 i 0 1 2 3 k_i 11243 10367 12698 10940 q_i - 0.9221 1.2248 0.8616 The mean value of the growth factors results in \\[ \\overline q = \\frac 13 (0.9221 + 1.2248 + 0.8616) = 1.0028 \\] and thus suggests positive growth, although the number of customers in Q4 is lower than in Q1. The geometric mean results: \\[\\begin{equation} q_{geom} = (0.9221 \\cdot 1.2248 \\cdot 0.8616)^{\\frac 13} = 0.9909 \\end{equation}\\] and provides an average increase of less than 1, which correctly reflects the fact that overall customer numbers have declined. 2.1.4 Empirical Modal Value In contrast to the above measures, modal value can be calculated for characteristics with any scale, as only the frequencies are taken into account when determining the modal value. The characteristic with the highest frequency is taken as the modal value: \\[ x_{mod} = a_m \\text{ with } h(a_m)=h_m=\\max (h_1, h_2, \\ldots, h_J) \\] Example 2.12 (Orders.) Cont. of Example 2.3. Here, the modal value \\(b_{mod}=2\\), as the relative frequency of this characteristic is \\(0.3\\) and therefore the highest frequency in the frequency distribution for the characteristic: number of orders. Example 2.13 (Means of payment.) Cont. of Example 1.4. Here, the modal value is equal to the ‚ÄúPayPal‚Äù attribute, as this attribute has the highest frequency of \\(0.4\\). Exercise 2.3 (Measures of central tendency.) 2.2 Measures of Variability Often, not only the mean position of the values of a characteristic but also the extent of the dispersion in the values is helpful for an empirical analysis. This is particularly useful when comparing two different samples that relate to the same characteristic but were obtained under different conditions. The differences, which can be attributed to customer groups, times of year, week or day, for example, can be seen not only in the mean value but also in the dispersion of the data. Even by similar mean values, such factors often data variability. 2.2.1 Sample Variance and Sample Standard Deviation The empirical/ sample variance (\\(\\hat\\sigma^2\\) or \\(s^2\\)) of a sample \\(x_1,x_2,\\ldots,x_n\\) is given by: \\[ s^2 = \\frac 1{n-1}\\left((x_1 - \\bar x)^2 + (x_2 - \\bar x)^2 + \\ldots + (x_n - \\bar x)^2\\right)=\\frac 1{n-1}\\sum_{i=1}^n(x_i - \\bar x)^2 \\] and is therefore the sum of squared deviations of the sample values from the sample mean divided by the sample size reduced by one (\\(n-1\\)). In contrast to the mean square deviation \\(\\tilde{s}^2 = \\frac 1{n}\\left((x_1 - \\bar x)^2 + (x_2 - \\bar x)^2 + \\ldots + (x_n - \\bar x)^2\\right)\\), the formula for \\(s^2\\) is divided by \\(n-1\\). The reasons can be found in the estimation theory: \\(s^2\\) provides an unbiased estimate for the true variance in expectation, \\(\\tilde{s}^2\\) on the other hand is biased and underestimates the true variance, which is particularly noticeable with smaller samples. The following alternative formula for the empirical variance makes the calculations ‚Äúby hand‚Äù somewhat easier: \\[ s^2 = \\frac 1{n-1}\\sum_{i=1}^nx_i^2 - \\frac n{n-1}(\\overline x)^2. \\] Since the sample variance has the squared units of the feature, one usually calculates the sample standard deviation \\(s=\\sqrt{s^2}\\) to get a ‚Äúfeel‚Äù for the variability in the data. Example 2.14 (Time in the online store by time of day.) Cont. of Example 2.6. We have already established that, on average, people spend more time in our online store in the evening than at lunchtime. What happens if we now compare the times in the evening during the week and at the weekend? (Here we can not only compare the sample averages of the two samples, but also compare the variability contained in the data. Table 2.11: Table 2.12: Sample size \\(n=10\\) for the characteristic \\(X\\): Time spent in the online store at lunchtime and in the evening on weekdays and at weekends (WE). [i] 1 2 3 4 5 6 7 8 9 10 time_midday 0.57 6.82 13.95 30.86 36.56 37.34 37.4 38.42 39.97 51.65 time_evening 22.41 27.87 32.02 33.93 34.35 35.08 65.4 83.23 100.48 110.81 time_WE 23.6 24.12 34.31 39.69 47.24 48.5 67.31 92.95 94.82 102.32 evening: \\[\\begin{align}s_2^2 &amp;= \\frac 1{10-1}(z^2_{2,1} + z^2_{2,2} + \\ldots + z^2_{2,10}) - \\frac {10}{10-1}\\cdot \\overline z_2^2 \\\\ &amp;= \\frac 1{9}(22.41^2 + 27.87^2 + \\ldots + 110.81^2) - \\frac {10}{10-1}\\cdot 54,558^2\\\\ &amp;= 1075.525,\\\\ s_2&amp;=\\sqrt{s_2^2} = 32.7952. \\end{align}\\] WE: \\[\\begin{align}s_3^2 &amp;= \\frac 1{10-1}(z^2_{3,1} + z^2_{3,2} + \\ldots + z^2_{3,10}) - \\frac {10}{10-1}\\cdot \\overline z_3^2 \\\\ &amp;= \\frac 1{9}(23.6^2 + 24.12^2 + \\ldots + 102.32^2) - \\frac {10}{10-1}\\cdot 57.486^2\\\\ &amp;= 895.4695,\\\\ s_3&amp;=\\sqrt{s_3^2} = 29.9244. \\end{align}\\] The times in the online store at the weekend vary less, as can be seen from the difference in the standard deviations of the two samples for the times. If frequencies are given, the empirical variance can be calculated as follows: \\[\\begin{equation*} \\hat\\sigma_X^2 = \\frac{n}{n-1}\\big[ (a_1-\\overline x)^2 h_1 + \\cdots + (a_n-\\overline x)^2 h_n\\big] = \\frac{n}{n-1} \\sum_{j=1}^J (a_j-\\overline x)^2 h_j. \\end{equation*}\\] The following alternative formula makes the calculations ‚Äúby hand‚Äù somewhat easier, even in the case of frequencies: \\[\\begin{align*} \\hat\\sigma_X^2 &amp;= \\frac{n}{n-1} \\sum_{j=1}^J a_j^2 h_j - \\frac{n}{n-1} \\mu_X^2 \\end{align*}\\] The corresponding empirical standard deviation is also the square root of the empirical variance: \\[\\begin{equation*} \\hat\\sigma_X = \\sqrt{\\hat\\sigma_X^2}. \\end{equation*}\\] Example 2.15 (Monthly Income.) Cont. of Example 2.8. Table 2.13: Sample size \\(n=10\\) for the characteristic \\(Z\\): monthly income of customers. Income group [0,1) [1,2) [2,4) [4,5] rel.frequency 0.1 0.1 0.5 0.3 The class centers are: \\(a_1 = 0.5\\), \\(a_2 = 1.5\\), \\(a_3 = 3\\) und \\(a_4 = 4.5\\). The sample mean is \\(\\hat\\mu_Z = 3.05.\\) The sample size is \\(n=10\\). So: \\[\\begin{align} \\hat\\sigma_Z^2 &amp;= \\frac{10}{9}\\cdot(a_1^2\\cdot h_1 + a_2^2\\cdot h_2 + a_{3}^2\\cdot h_{3} + a_{4}^2\\cdot h_{4} - \\mu_Z^2)\\\\ &amp;=\\frac{10}{9}(0.5^2\\cdot0.1 + 1.5^2\\cdot0.1 + 3^2\\cdot0.5 + 4.5^2\\cdot0.3 - 3.05^2)\\\\ &amp;= 1,6917.\\\\ \\hat\\sigma_Z &amp;= \\sqrt{\\hat\\sigma_Z^2} = \\sqrt{1.6917} = 1.3006 \\end{align}\\] Linear Transformations Rule For a linear transformation of the data \\(y_i = a + bx_i, i=1,\\ldots,n\\) applies: The sample mean of the transformed data is \\(\\bar y=a+b\\bar x\\) and The sample variance and standard deviation are \\(s_Y^2=b^2s_X^2\\) and \\(s_Y=|b|s_X\\) respectively. Example 2.16 (Financial derivative) A financial instrument whose value depends on the price of another security is called a (financial) derivative. For example, you purchase a derivative (\\(D\\)) on the return (\\(R\\)) of a share with \\(\\hat\\mu_R = 0.05\\) and standard deviation \\(\\hat\\sigma_R=0.01\\). The payoff of the derivative is \\(500\\) times the realized return, and the price of the derivative is \\(10\\) euros. The payoff of the derivative \\(d_i\\) is therefore a linear transformation of the stock return \\(r_i\\): \\(d_i = 500\\cdot r_i -10\\). From this information, we can deduce that the average payoff is \\(\\hat\\mu_D = 500\\cdot \\hat\\mu_R - 10 = 15\\) and \\(\\hat\\sigma_D = |500|\\cdot \\hat\\sigma_R=5\\). 2.2.2 Range The range \\(r\\) measures the difference between the largest and the smallest sample value and is therefore another way of representing the variability in the data. For a sample the range is defined by: \\[ r=\\max(x_1,x_2,\\ldots, x_n) - \\min(x_1,x_2,\\ldots, x_n) \\] In the case of an ordered sample, the range is determined as: \\[ r=x_{[n]} - x_{[1]} \\] Example 2.17 (Time in the online store by time of day.) Cont. of Example 2.14. midday: \\(r_1=51.65 - 0.57 = 51.08\\) evening: \\(r_2=110.81 - 22.41 = 88.4\\) WE: \\(r_3=102.32 - 23.6 = 78.72\\) How do you interpret the results? 2.2.3 Coefficient of variation In our example: Time on the website, we found that the times at the weekend did not vary as much as the times in the evening. However, the mean values were also very different. How can we adjust for this effect? Well, we can use another key figure to help us, namely the coefficient of variation. The coefficient of variation \\(cv\\) measures the variability in the units of the mean value: \\[ cv = \\frac{s}{\\bar x} \\] The standard deviation is therefore scaled by the sample mean. This answers the question: ‚ÄúHow large is the standard deviation ‚Äòmeasured in the mean values‚Äô‚Äù. Example 2.18 (Time in the online store by time of day.) Cont. of Example 2.14. midday: \\(cv_1=\\frac{16.4934}{29.354} = 0.5619\\) evening: \\(cv_2=\\frac{32.7952}{54.558} = 0.6011\\) WE: \\(cv_3=\\frac{29.9244}{57.486} = 0.5206\\) How do you interpret the results? Exercise 2.4 (Streuungskennzahlen.) 2.3 Quantile and Boxplots When looking at a sample or comparing two samples, it is sometimes not enough to calculate just a few numbers to get an informative picture of the data. Sometimes it is necessary to record and compare the entire empirical distributions. In this case, the empirical quantiles as additional measures and a simple graphical representation - Boxplot - should be used. 2.3.1 Quantiles A number \\(\\hat q_p\\) with the property that at least the proportion \\(p\\) of the values from the sample is below this number and at least the proportion \\(1-p\\) is above it is called the empirical \\(p\\) quantile. For an ordered sample of a metrically scaled characteristic, the empirical \\(p\\)-quantile is calculated as: \\[ \\hat q_p = \\begin{cases} x_{\\left[int(np) + 1\\right]}, &amp; \\text{if } np \\text{ is not an integer},\\\\ \\frac 12\\left(x_{[np]} + x_{[np+1]}\\right), &amp; \\text{if } np \\text{ is an integer}, \\end{cases} \\] where \\(int(np)\\) denotes the integer part of \\(np\\). Example 2.19 (Time in the online store by time of day.) Cont. of Example 2.14. We calculate some quantiles for the times on our website. Table 2.14: Table 2.15: Sample size \\(n=10\\) for the characteristic: Time on the website at lunchtime and in the evening on weekdays and at weekends (WE). [i] 1 2 3 4 5 6 7 8 9 10 time_midday 0.57 6.82 13.95 30.86 36.56 37.34 37.4 38.42 39.97 51.65 time_evening 22.41 27.87 32.02 33.93 34.35 35.08 65.4 83.23 100.48 110.81 time_WE 23.6 24.12 34.31 39.69 47.24 48.5 67.31 92.95 94.82 102.32 midday: \\[\\begin{align} \\hat q_{0.05} &amp;\\stackrel{np=0.05\\cdot 10 =0.5}{=}x_{\\left[int(0,5) + 1\\right]} = x_{[1]} = 0.57,\\\\ \\hat q_{0.25} &amp;\\stackrel{np=0.25\\cdot 10 =2.5}{=}x_{\\left[int(2,5) + 1\\right]} = x_{[3]} = 13.95,\\\\ \\hat q_{0.5} &amp;\\stackrel{np=0.5\\cdot 10 =5}{=}\\frac 12\\left(x_{[5]} + x_{[5+1]}\\right) = 36.95,\\\\ \\hat q_{0.75} &amp;\\stackrel{np=0.75\\cdot 10 =7.5 }{=}x_{\\left[int(7,5) + 1\\right]} = x_{[8]} = 38.42,\\\\ \\hat q_{0.8} &amp;\\stackrel{np=0.8\\cdot 10 =8}{=}\\frac 12\\left(x_{[8]} + x_{[8+1]}\\right) = 39.195. \\end{align}\\] Some of the calculated quantiles have special names: \\(\\hat q_{0.5}\\) corresponds to the sample median \\(x_{med}\\). \\(\\hat q_{0.25}\\) is called the lower quartile and \\(\\hat q_{0.75}\\) is called the upper quartile. The distance \\(\\hat q = \\hat q_{0.75} - \\hat q_{0.25}\\) is called the quartile distance. Together with the maximum and minimum values of the sample, these quantiles form the basis for the graphical representation of the distribution - Boxplot. 2.3.2 Boxplot The boxplot is a simple but informative representation of the empirical distribution with the following elements: a line marking \\(x_{med}=\\hat q_{0.5}\\), a box between \\(\\hat q_{0.25}\\) and \\(\\hat q_{0.75}\\) and two arrows from the box to \\(x_{min}\\) and \\(x_{max}\\) respectively. Example 2.20 (Time in the online store by time of day.) Cont of Example 2.19. The empirical distributions of the times on the website can be compared at a glance with the help of boxplots. Figure 2.1: Boxplots for the times in the online store. Example 2.21 (Stock returns) The empirical distributions of two different stocks as investment opportunities can also be compared using box plots. 2.3.3 Quantiles with continuous frequency distribution With a continuous frequency distribution, quantiles can be approximated using the empirical distribution function \\(\\hat F(x)\\). The following applies for \\(\\hat q_p\\): \\[ \\hat F(q_p) = p \\rightarrow \\hat q_p = \\hat F^{-1}(p). \\] Example 2.22 (Z=monthly Income.) Cont. of Example 2.10 The empirical distribution function for the data (income class) is: \\[ \\hat F(x) = \\begin{cases} 0,&amp; x&lt;0,\\\\ 0.1\\cdot x, &amp;0\\leq x &lt; 1,\\\\ 0.1+0.1\\cdot(x - 1), &amp;1\\leq x &lt; 2,\\\\ 0.2+0.25\\cdot(x - 2), &amp;2\\leq x \\leq 4,\\\\ 0.7+0.3\\cdot(x - 4), &amp;4\\leq x \\leq 5,\\\\ 1,&amp;x&gt;5. \\end{cases} \\] we determine some sample quantiles: \\(q_{0.25}\\): \\(\\hat F(x)\\) assumes the value \\(0.25\\) in the section with \\(2\\leq x&lt;4\\), set \\(0.2+0.25\\cdot (q_{0.25} - 2) = 0.25\\) and convert to \\(q_{0.25}\\). The result is \\(\\hat q_{0.25}= 2.2\\). \\(q_{0.75}\\): \\(\\hat F(x)\\) assumes the value \\(0.75\\) in the section with \\(4\\leq x &lt;5\\), set \\(0.7 + 0.3\\cdot(q_{0.75} - 4) = 0.75\\) and convert to \\(q_{0.75}\\). The result is \\(\\hat q_{0.75}= 4.1667\\). This means that only about 25% of the customers in the sample have an income lower than 2.2 TEUR and 25% of the customers have an income higher than 4.1667 TEUR. Exercise 2.5 (Quantile und Boxplot.) "],["concentration-measures.html", "Chapter 3 Concentration measures 3.1 Lorenz curve 3.2 Gini coefficient and concentration rate", " Chapter 3 Concentration measures Empirical distributions can be very different, especially when the uniformity of the distribution among holders of the characteristic is taken into account. You may be aware that economists are concerned about the development of income distribution in the world population. Roughly speaking, this involves calculating what proportion of the (poorest) population receives what proportion of income. Ideally, one would like to have a distribution that is not too concentrated, so that the situation where a very small part of the population has an overwhelming share of the income is avoided. Gini coefficient for income distribution (source: Wikipedia https://de.wikipedia.org/wiki/Gini-Koeffizient): Other examples of problems where one would like to measure concentration are The distribution of shares among shareholders. The distribution of corporate loans over the industry sectors. The distribution of agricultural land among farmers. Goodness of fit for classification models (quantify ‚Äúpurity‚Äù of the classes predicted by a model) ü§©. In this chapter we will learn how to represent such concentration: graphically \\(\\leadsto\\) Lorenz curve, numerically \\(\\leadsto\\) Gini coefficient and concentration rate. 3.1 Lorenz curve The Lorenz curve is a graphical representation of the concentration that visualizes the deviation from the uniform distribution. Example 3.1 (Lorenz curve) 3.1.1 Lorenz curve for a sample For an ordered sample \\(\\displaystyle x_{[1]}\\leq x_{[2]}\\leq \\cdots\\leq x_{[n]}\\) of a metrically scaled characteristic \\(X\\) and \\([i]=[1],\\ldots,[n]\\) calculate the cumulative proportion of the total number of feature carriers: \\(u_i = \\frac{[i]}n\\), the cumulative feature sums:\\(\\displaystyle S_i=x_{[1]} + x_{[2]} + \\cdots + x_{[i]}\\) and the cumulative proportion of the total sum of times: \\(v_i=\\frac{S_i}{S_n}\\). The Lorenz curve is the polygon through the points: \\[ (0,0), (u_1,v_1), \\ldots, (u_n,v_n)=(1,1), \\] The Lorenz curve embodies which cum. proportion \\(v_i\\) of the lower edge of the characteristic sum is attributable to which cum. Proportion \\(u_i\\) of the characteristic carriers. In the case of an equal distribution \\(x_1 = \\ldots = x_n\\), the characteristic sum is distributed equally among the characteristic carriers. I.e. \\(u_i=v_i, i=1,\\ldots,n\\) and the Lorenz curve corresponds to a diagonal line with a 45 degree inclination. In the case of a deviation from the uniform distribution, the feature sum is concentrated on a few feature carriers. This means that \\(u_i\\) is greater than \\(v_i\\) and there is an unequal distribution in which the ‚Äúpoorest‚Äù feature carriers receive proportionally less of the feature sum than they would under the uniform distribution. This means that \\(u_i&gt;v_i\\) and the Lorenz curve runs below the diagonal (\\(u_i=v_i\\)). The distance of the Lorenz curve from the diagonal, or the area \\(L\\) between the diagonal and the Lorenz curve, is a measure of the concentration in the distribution of \\(X\\). Example 3.2 (Lorenz curve for market shares) We examine the concentration in a market with few manufacturers. Market distribution: \\(n=5\\) manufacturers (\\(A\\), \\(B\\), \\(C\\), \\(D\\), \\(E\\)) of a certain product. product. Characteristic \\(X=\\text{market share}\\) with characteristic sum \\(S_5=100\\). \\(4\\) Distributions (years) of \\(S_5=100\\%\\) market shares on the \\(5\\) companies \\[ \\begin{array}{lcccccc} \\hline Firm &amp; A &amp; B &amp; C &amp; D &amp; E&amp; Feature~sum\\\\\\hline 1. ~ Distribution &amp; 20 &amp; 20 &amp; 20 &amp; 20 &amp; 20 &amp; 100\\\\ 2. ~ Distribution &amp; 20 &amp; 10 &amp; 20 &amp; 20 &amp; 40 &amp; 100\\\\ 3. ~ Distribution &amp; 15 &amp; 5 &amp; 5 &amp; 70 &amp; 5 &amp; 100\\\\ 4. ~ Distribution &amp; 0 &amp; 0 &amp; 0 &amp; 100 &amp; 0 &amp; 100\\\\\\hline \\end{array} \\] The points of the Lorenz curves are: \\[ \\begin{array}{lcccccc} \\hline i &amp; 0 &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5\\\\\\hline u_i &amp; 0 &amp; \\frac{1}{5} &amp; \\frac{2}{5} &amp; \\frac{3}{5} &amp; \\frac{4}{5} &amp; 1\\\\\\hline v_i\\,(\\text{1.Distribution}) &amp; 0 &amp; \\frac{1}{5} &amp; \\frac{2}{5} &amp; \\frac{3}{5} &amp; \\frac{4}{5} &amp; 1\\\\ v_i\\,(\\text{2.Distribution}) &amp; 0 &amp; \\frac{1}{10} &amp; \\frac{2}{10} &amp; \\frac{4}{10} &amp; \\frac{6}{10} &amp; 1\\\\ v_i\\,(\\text{3.Distribution}) &amp; 0 &amp; \\frac{1}{20} &amp; \\frac{2}{20} &amp; \\frac{3}{20} &amp; \\frac{6}{20} &amp; 1\\\\ v_i\\,(\\text{4.Distribution}) &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1\\\\\\hline \\end{array} \\] Graphic representation: Figure 3.1: Lorenz curves for the four market distributions Properties of the Lorenz curve: The Lorenz curve always runs below the diagonal. The area \\(L\\) increases with increasing concentration. In the case of even distribution (no concentration), the Lorenz curve is identical to the diagonal and the area: \\(L_{\\min}=0\\) In the case of maximum concentration, the entire feature sum is concentrated on one carrier. The Lorenz curve extends over \\((0,0), ((n-1)/n,0), (1,1)\\) and the area \\(L=L_{\\max}=\\displaystyle\\frac{n-1}{2n}\\). 3.1.2 Lorenz curve for frequency distributions If the data is already in the form of a discrete frequency distribution \\[ \\begin{array}[c]{cccc} \\hline a_1 &amp; a_2 &amp; \\cdots &amp; a_J\\\\ h_1 &amp; h_2 &amp; \\cdots &amp; h_J\\\\\\hline \\end{array} \\] or a continuous frequency distribution \\[\\begin{array}[c]{cccc} \\hline K_1 &amp; K_2 &amp; \\cdots &amp; K_J\\\\ a_1 &amp; a_2 &amp; \\cdots &amp; a_J\\\\ h_1 &amp; h_2 &amp; \\cdots &amp; h_J\\\\\\hline \\end{array}\\] are present, ordered in ascending order: \\(a_{[1]} &lt;a_{[2]} &lt; \\cdots &lt; a_{[J]}\\), then the polygon‚Äôs grid points are calculated as: \\[\\begin{align} u_j &amp;= h_{[1]} + \\cdots + h_{[j]},\\\\ v_j&amp;=\\frac{a_{[1]} h_{[1]} + \\cdots + a_{[j]} h_{[j]}} {a_{[1]} h_{[1]} + \\cdots + a_{[J]} h_{[J]}}, \\quad j=1,\\ldots, J. \\end{align}\\] Example 3.3 (Lorenz curve for shareholders) We examine the distribution of shares among the shareholders in an AG. The shares of an AG are held by \\(n=1000\\) shareholders. Characteristic \\(X\\): Number of shares \\[\\begin{array}{ccrrrrll} \\hline &amp; \\text{Class} &amp; \\text{Number} &amp;\\text{relative}&amp; \\text{Class mean} &amp; \\text{Prop.} &amp; \\text{cum.Prop.} &amp; \\text{cum.Prop.}\\\\ &amp; &amp; \\text{of holders} &amp;\\text{Freq.} &amp; \\text{mittel} &amp; \\text{of shares} &amp; \\text{of holders}&amp; \\text{of shares}\\\\ j &amp; K_j &amp; H_j &amp;h_j&amp; a_j &amp; a_j h_j &amp; u_j &amp; v_j\\\\\\hline 1 &amp; [1,50) &amp; 553 &amp;0,553&amp; 13 &amp; 7.189 &amp; 0.5530 &amp; 0.0625\\\\ 2 &amp; [50,100) &amp; 212 &amp;0.212 &amp; 64 &amp; 13.568 &amp; 0.7650 &amp; 0.1805\\\\ 3 &amp; [100,200) &amp; 104 &amp;0.104&amp; 142 &amp; 14.768 &amp; 0.8690 &amp; 0.3089\\\\ 4 &amp; [200,500) &amp; 81 &amp;0.081&amp; 325 &amp; 26.325 &amp; 0.9500 &amp; 0.5378\\\\ 5 &amp; [500,5000] &amp; 50 &amp;0.05&amp; 1063 &amp; 53.150 &amp; 1.0000 &amp; 1.0000\\\\\\hline \\sum &amp; &amp; 1000 &amp;&amp; &amp; 115\\\\\\hline \\end{array}\\] Graphic representation: Figure 3.2: Lorenz curve for shareholders 3.2 Gini coefficient and concentration rate The Lorenz curve as a graphical representation quickly makes it clear to what extent the distribution of a characteristic resembles a uniform distribution or is far removed from it. However, it is not always possible to precisely estimate this distance/the area between the diagonal and the Lorenz curve graphically, which is important when comparing several distributions, for example. Indicators such as the Gini coefficient and the concentration ratio summarize the degree of concentration in figures and are more practical for analysts. 3.2.1 Gini coefficient The area \\(L\\) (example 3.3) between the diagonal and the Lorenz curve reflects the extent of the concentration: Figure 3.3: Lorenz curve for shareholders compared to max. concentration This area relative to the maximum possible area \\(L_{max}\\) (max. concentration) forms the basis for measuring the degree of concentration. The (normalized) Gini coefficient is defined as \\[\\begin{align*} G &amp;= \\frac{\\text{area between diagonal and Lorenz curve}} {\\text{Area between diagonal and Lorenz curve for maximum concentration}}\\\\ &amp;= \\frac{L}{L_{\\max}} = \\frac{2n}{n-1}\\cdot L, \\end{align*}\\] with \\(\\displaystyle L_{\\max}=\\frac{n-1}{2n}\\) (maximum concentration). The Gini coefficient \\(G\\) therefore has the property \\(0\\leq G\\leq 1\\), where \\(G=0\\) corresponds to the uniform distribution and \\(G=1\\) corresponds to the maximum concentration. Calculation: \\[\\begin{align*} \\text{from sample: }G &amp;=\\frac{n}{n-1} \\left(\\left(\\sum_{i=1}^n(u_{i-1}+u_i) (v_i - v_{i-1}) \\right)-1\\right)\\\\ \\text{ with frequencies: } G&amp;=\\frac{n}{n-1} \\left(\\left(\\sum_{j=1}^J(u_{j-1}+u_j) (v_j-v_{j-1})\\right)-1\\right). \\end{align*}\\] with \\(u_0=v_0=0\\). Example 3.4 (Gini for market shares) Cont. of Example 3.2. We calculate the Gini coefficient for the distribution in the 3rd year: \\(G_3\\) \\[ \\begin{array}[t]{ccccccc} \\hline i &amp; x_{[i]} &amp; u_i &amp; v_i &amp; u_{i-1} + u_i&amp; v_i-v_{i-1}&amp; (u_{i-1}+u_i)(v_i-v_{i-1})\\\\ \\hline 0 &amp; - &amp; 0 &amp; 0 &amp; - &amp; - &amp; -\\\\ 1 &amp; 5 &amp; 0.2 &amp; 0.05 &amp; 0.2 &amp; 0.05 &amp; 0.01\\\\ 2 &amp; 5 &amp; 0.4 &amp; 0.10 &amp; 0.6 &amp; 0.05 &amp; 0.03\\\\ 3 &amp; 5 &amp; 0.6 &amp; 0.15 &amp; 1.0 &amp; 0.05 &amp; 0.05\\\\ 4 &amp; 15 &amp; 0.8 &amp; 0.30 &amp; 1.4 &amp; 0.15 &amp; 0.21\\\\ 5 &amp; 70 &amp; 1.0 &amp; 1.00 &amp; 1.8 &amp; 0.70 &amp; 1.26\\\\\\hline \\sum &amp; &amp; &amp; &amp; &amp;&amp;\\bf 1.56\\\\\\hline \\end{array} \\] The Gini coefficient is then: \\(\\displaystyle G_3 = \\frac{5}{4} (1.56-1) = 0.7\\). The same calculation logic produces: \\[ \\displaystyle G_1 = \\frac{5}{4}(1.00-1) = 0 \\] \\(\\leadsto\\) no concentration. \\[ \\displaystyle G_2=\\frac{5}{4}(1.28-1)=0.35 \\] \\(\\leadsto\\) low to moderate concentration. \\[ \\displaystyle G_4=\\frac{5}{4}(1.80-1)=1 \\] \\(\\leadsto\\) maximum concentration. Example 3.5 (Gini for the shareholders) Cont. of Example 3.3. Computations: \\[ \\begin{array}[t]{cccccc} \\hline j &amp; u_j &amp; v_j &amp; u_{j-1}+u_j &amp; v_j-v_{j-1}&amp; (u_{j-1}+u_j) (v_j-v_{j-1})\\\\\\hline 0 &amp; 0 &amp; 0\\\\ 1 &amp; 0.5530 &amp; 0.0625 &amp; 0.5530 &amp; 0.0625 &amp; 0.0346\\\\ 2 &amp; 0.7650 &amp; 0.1805 &amp; 1.3180 &amp; 0.1180 &amp; 0.1555\\\\ 3 &amp; 0.8690 &amp; 0.3089 &amp; 1.6340 &amp; 0.1284 &amp; 0.2098\\\\ 4 &amp; 0.9500 &amp; 0.5378 &amp; 1.8190 &amp; 0.2289 &amp; 0.4164\\\\ 5 &amp; 1.0000 &amp; 1.0000 &amp; 1.9500 &amp; 0.4622 &amp; 0.9012\\\\\\hline \\sum &amp; &amp; &amp; &amp; &amp; 1.7175\\\\\\hline \\end{array} \\] Gini coefficient: \\[ G=\\frac{1000}{999} (1.7175-1) = 0.7183. \\] 3.2.2 Concentration rate Lorenz curve and Gini coefficient are measures of relative concentration (relative to the other feature holders) - they are determined by the question: What percentage of the feature holders share what percentage of the feature sum? A simple concentration measure based on an absolute number \\(a\\) of feature holders is the concentration rate \\(CR_a\\). The concentration rate indicates the percentage of the feature sum that is shared by the \\(a\\) largest feature holders: \\[ CR_a = \\sum_{i=n-a+1}^n q_i\\quad\\text{ mit } q_i=\\frac{x_{[i]}} {x_{[1]} + \\cdots + x_{[n]}} \\] Example 3.6 (Concentration rate for market shares) Cont. of Example 3.2 and 3.4. We calculate the concentration rate \\(CR_2\\) of the market shares in each case for the two (\\(a=2\\)) largest companies: \\[\\begin{align*} CR_2(\\text{1.Jahr}) &amp;= 0,2 + 0,2=0,4 = 40\\\\ CR_2(\\text{2.Jahr}) &amp;= 0,2 + 0,4 = 0,6 = 60\\\\ CR_2(\\text{3.Jahr}) &amp;= 0,15 + 0,7 = 0,85 = 85\\\\ CR_2(\\text{4.Jahr}) &amp;= 0,0 + 1,0 = 1,0 = 100 \\end{align*}\\] Exercise 3.1 (Concentration measures) "],["association-between-two-features.html", "Chapter 4 Association between two features 4.1 Frequency distribution for two features 4.2 Conditional frequency distribution 4.3 Empirical independence 4.4 Dependence measures", " Chapter 4 Association between two features So far, we have looked at various features in isolation. But what do we do if we want to analyze two features together? For example, to predict the unknown value of one feature (Sales) using a known value of the another feature (Weather conditions)? In this chapter we will learn how to form the joint distribution for two features, how to modify this distribution if a certain value for one of the feature is already given \\(\\leadsto\\) conditional distribution. and how to infer empirical (in)dependence. In addition, we will learn how to indicate the strength of the association by dependence measures. Example 4.1 (Sales vs. Weather) Hypothesis: Your sales depend on the weather. The better the weather forecast, the more sales they believe you can make. Your boss still needs data-based evidence. You have ‚Äúscraped together‚Äù the following data from your colleagues‚Äô assessments. How do you proceed? Table 4.1: Sample size n=10 for the characteristics S Sales and W Weather conditions. Day Sales_Category Weather_Conditions 1 moderate schlecht 2 low schlecht 3 moderate schlecht 4 moderate gut 5 high gut 6 moderate gut 7 high gut 8 moderate gut 9 low schlecht 10 low gut Consider the answers to the following questions: What values does the characteristic \\(W\\): Weather conditions attain? Which values are possible for the characteristic \\(S\\): Sales? How often do different combinations of the values of the two characteristics occur? Is it possible to say anything about the relationship between the two characteristics? We are now dealing with the joint distribution of two characteristics \\((X,Y)\\). We have to adapt our previous approach to frequency distributions: The concept of frequency distribution is extended to frequency distributions for two features \\(\\leadsto\\) joint frequency distributions. The previous frequency distributions for one characteristic are renamed \\(\\leadsto\\) marginal frequency distributions. The frequency distribution of a characteristic for the case that the other characteristic assumes a certain value (condition) is introduced \\(\\leadsto\\) conditional frequency distributions. 4.1 Frequency distribution for two features Joint frequencies extend the previous frequency distribution to frequency distribution for two characteristics. Using a joint sample of the terms \\(X\\) and \\(Y\\) \\((x_1,y_1),\\ldots,(x_n,y_n)\\), the values of \\(X\\) are determined as \\(A_X=\\{a_1,a_2,\\ldots,a_J\\}\\) and the values of \\(Y\\) as \\(B_Y=\\{b_1,b_2,\\ldots,b_K\\}\\). 4.1.1 Joint frequencies the absolute joint frequency \\(H_{jk} = H(a_j,b_k)\\) gives the number of pairs from the sample \\((x_i,y_i)\\) that correspond to the combination of the \\((a_j,b_k)\\) expressions. the relative joint frequency\\(h_{jk} = h(a_j,b_k)\\) gives the proportion of such pairs in the sample: \\(h_{jk}=\\frac{H_{jk}}n\\). is the joint frequency distribution: \\[ \\mathbb h_{XY} = \\left(\\begin{array}{cccc} h_{11}&amp; h_{12}&amp; \\ldots&amp;h_{1K}\\\\ \\vdots&amp;\\vdots&amp;\\ldots&amp;\\vdots\\\\ h_{J1}&amp;h_{J2}&amp;\\ldots&amp;h_{JK} \\end{array}\\right) \\] or as a table: \\[ \\begin{array}[t]{c|cccc|} \\mathbb h_{XY}&amp; b_1 &amp; b_2 &amp; \\cdots &amp; b_K \\\\\\hline a_1 &amp; h_{11} &amp; h_{12} &amp; \\cdots &amp; h_{1K} \\\\ a_2 &amp; h_{21} &amp; h_{22} &amp; \\cdots &amp; h_{2K} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ a_J &amp; h_{J1} &amp; h_{J2} &amp; \\cdots &amp; h_{JK}\\\\\\hline \\end{array} \\] Example 4.2 (Sales vs. Weather) Cont. of Example 4.1 We set up the joint distribution for the characteristics \\(W=\\) weather conditions and \\(S=\\) sales. First, we calculate the absolute frequencies: \\[ \\begin{array}[t]{l|ccc|c} \\mathbf H_{WS}&amp; b_1=low &amp; b_2=moderate &amp;b_3=high&amp;\\\\\\hline a_1=good &amp; 1 &amp; 3 &amp; 2&amp; \\\\ a_2=bad &amp; 2 &amp; 2 &amp; 0&amp; \\\\\\hline &amp;&amp;&amp;&amp; \\end{array} \\] The relative frequencies are: \\[ \\begin{array}[t]{l|ccc|c} \\mathbf h_{WS}&amp; b_1=low &amp; b_2=moderate &amp;b_3=high&amp;\\mathbf h_W\\\\\\hline a_1=good &amp; 0.1 &amp; 0.3 &amp; 0.2&amp; \\\\ a_2=bad &amp; 0.2 &amp; 0.2 &amp; 0&amp; \\\\\\hline \\mathbf h_S&amp;&amp;&amp;&amp; \\end{array} \\] 4.1.2 Marginal frequencies The marginal frequencies are calculated from the joint frequencies as follows \\[\\begin{align*} H_{j\\cdot} &amp;= \\sum_{k=1}^K H_{jk} = \\text{absolute marginal frequency of }a_j\\\\\\ H_{\\cdot k} &amp;= \\sum_{j=1}^J H_{jk} = \\text{absolute marginal frequency of }b_k\\\\ h_{j\\cdot} &amp;= \\sum_{k=1}^K h_{jk} = \\text{relative marginal frequency of } a_j\\\\\\ h_{\\cdot k} &amp;= \\sum_{j=1}^J h_{jk} = \\text{relative marginal frequency of } b_k \\end{align*}\\]} &amp;= {k=1}^K h{jk} = a_j\\¬† h_{k} &amp;= {j=1}^J h{jk} = b_k \\end{align*} The marginal relative frequencies constitute the marginal frequency distributions and are given by \\[\\begin{align*} \\mathbf{h}_X&amp;= \\begin{pmatrix} h_{1\\cdot} &amp; h_{2\\cdot} &amp; \\cdots &amp; h_{J\\cdot} \\end{pmatrix} &amp;\\textbf{marginal relative frequency of } X\\\\ \\mathbf{h}_Y&amp;= \\begin{pmatrix} h_{\\cdot 1} &amp; h_{\\cdot 2} &amp; \\cdots &amp; h_{\\cdot K} \\end{pmatrix} &amp;\\textbf{marginal relative frequency of } Y \\end{align*}\\] Example 4.3 (Sales vs. Weather) Cont. of Example 4.2 We also calculate the marginal frequencies for the characteristics \\(W=\\) weather conditions and \\(S=\\) sales. \\[ \\begin{array}[t]{l|ccc|c} \\mathbf h_{WS}&amp; b_1=low &amp; b_2=moderate &amp;b_3=high&amp;\\mathbf h_W\\\\\\hline a_1=good &amp; 0.1 &amp; 0.3 &amp; 0.2&amp; 0.6\\\\ a_2=bad &amp; 0.,2 &amp; 0.2 &amp; 0&amp;0.4 \\\\\\hline \\mathbf h_S&amp;0.3&amp;0.5&amp;0.2&amp;1 \\end{array} \\] 4.1.3 Contingency table A contingency table is the joint representation of the joint frequencies and the marginal frequencies (referred to as \\(H_X\\) or \\(h_X\\) and \\(H_Y\\) or \\(h_Y\\)): \\[ \\begin{array}[t]{c|cccc|c} \\hline \\mathbf H_{XY}&amp; b_1 &amp; b_2 &amp; \\cdots &amp; b_K &amp; \\mathbf H_X\\\\\\hline a_1 &amp; H_{11} &amp; H_{12} &amp; \\cdots &amp; H_{1K} &amp; H_{1\\cdot}\\\\ a_2 &amp; H_{21} &amp; H_{22} &amp; \\cdots &amp; H_{2K} &amp; H_{2\\cdot}\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; &amp; \\vdots &amp; \\vdots\\\\ a_J &amp; H_{J1} &amp; H_{J2} &amp; \\cdots &amp; H_{JK} &amp; H_{J\\cdot}\\\\\\hline \\mathbf H_Y &amp; H_{\\cdot 1} &amp; H_{\\cdot 2} &amp; \\cdots &amp; H_{\\cdot K} &amp; n\\\\\\hline \\end{array} \\qquad \\begin{array}[t]{c|cccc|c} \\hline \\mathbf h_{XY}&amp; b_1 &amp; b_2 &amp; \\cdots &amp; b_K &amp; \\mathbf h_X\\\\\\hline a_1 &amp; h_{11} &amp; h_{12} &amp; \\cdots &amp; h_{1K} &amp; h_{1\\cdot}\\\\ a_2 &amp; h_{21} &amp; h_{22} &amp; \\cdots &amp; h_{2K} &amp; h_{2\\cdot}\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; &amp; \\vdots &amp; \\vdots\\\\ a_J &amp; h_{J1} &amp; h_{J2} &amp; \\cdots &amp; h_{JK} &amp; h_{J\\cdot}\\\\\\hline \\mathbf h_Y&amp; h_{\\cdot 1} &amp; h_{\\cdot 2} &amp; \\cdots &amp; h_{\\cdot K} &amp; 1\\\\\\hline \\end{array} \\] Example 4.4 (Rating of corporate clients) We examine the relationship between the rating of customers and the actual situation at the end (default vs.¬†no default). Characteristics \\(X\\) and \\(Y\\) of a bank‚Äôs low-performing corporate customers: \\(X\\)=rating at the beginning of the period with the characteristics: \\(B\\)=weak, \\(C\\)=medium, \\(D\\)=severely low-performing. \\(Y\\)=situation at the end of the period with the values: \\(D\\)=default, \\(N\\)=no default. Joint frequency distribution: \\[ \\begin{array}[t]{c|cc|c} \\mathbf{h}_{XY} &amp; Y=D &amp; Y=nD &amp; \\mathbf{h}_X\\\\\\hline X=B &amp; 0.05 &amp; 0.45 &amp; \\\\ \\phantom{X=}C &amp; 0.04 &amp; 0.16 &amp; \\\\ \\phantom{X=}D &amp; 0.21 &amp; 0.09 &amp; \\\\\\hline \\mathbf{h}_Y &amp; &amp;&amp; \\end{array} \\] \\(h_{1,1} = 0.05\\) (the joint relative frequency of \\(X=B\\) and \\(Y=D\\)) means that 5% of the customers initially received the rating \\(B\\) and defaulted at the end of the period. \\(h_{1,2} = 0.16\\) means that 16% of the customers initially received the rating \\(C\\) and did not report a default at the end of the period. \\(h_{3,1} = 0.21\\) means that 21% of the customers initially received the rating \\(D\\) and defaulted at the end of the period. Marginal frequency distributions: \\[ \\mathbf{h}_X = \\begin{bmatrix} B &amp; C &amp; D\\\\ 0.5 &amp; 0.2 &amp; 0.3 \\end{bmatrix}~~~~ \\mathbf{h}_Y = \\begin{bmatrix} D &amp; nD\\\\ 0.3 &amp; 0.7 \\end{bmatrix} \\] \\(h_{2,\\cdot}=0.2 = h_{X=C}\\) (the marginal frequency of \\(X=C\\)) means that 20% of the customers were rated C. \\(h_{\\cdot,1}=0.3 = h_{Y=D}\\) (the marginal frequency of \\(Y=D\\)) means that 30% of the customers had defaulted at the end of the period. 4.2 Conditional frequency distribution Absolute or relative frequencies do not directly indicate a relationship between two characteristics. Conditional frequencies are frequencies of a characteristic if the expression of the other characteristic is fixed (i.e.¬†known). Example 4.5 (Sales vs. Weather) Cont. of Example 4.1. Now you have received some more concrete data from BI‚Ä¶What is the distribution of sales when the outside temperature is over 20 degrees? \\[ \\begin{array}{r|ccc|c} \\hline \\mathbf H_{TS}&amp; S\\in\\text{[10,20)} &amp; \\text{[20,30)} &amp; \\text{[30,40]} &amp; \\mathbf H_T \\\\\\hline T\\in\\text{[10,15)} &amp; 20 &amp; 10 &amp; 5 &amp; 35\\\\ \\text{[15,20)} &amp; 10 &amp; 25 &amp; 5 &amp; 40\\\\ \\text{[20,25]}&amp; 0 &amp; 5 &amp; 20 &amp; 25\\\\\\hline \\mathbf H_S &amp; 30 &amp; 40 &amp; 30 &amp; 100\\\\\\hline \\end{array} \\] For a joint frequency distribution of the characteristics \\(X\\) and \\(Y\\) \\(\\textbf{h}_{XY}\\) we calculate: the relative conditional frequency of \\(X=a_j\\) given \\(Y=b_k\\) as \\[ h_{j|k} = h(X=a_j|Y=b_k) = \\frac{h_{jk}} {h_{\\cdot k}}, \\quad\\text{ also } h_{j|k} = \\frac{H_{jk}}{H_{\\cdot k}}, \\] the relative conditional frequency of \\(Y=b_k\\) given \\(X=a_j\\) as \\[ h_{k|j} = h(Y=b_k|X=a_j) = \\frac{h_{jk}} {h_{j\\cdot}}, \\quad\\text{ also } h_{k|j} = \\frac{H_{jk}}{H_{j\\cdot}}. \\] The conditional frequency distribution of \\(X\\) given \\(Y=b_k\\) is \\[ \\mathbf{h}_{X|Y=b_k} = \\begin{pmatrix} h_{1|k} &amp; \\cdots &amp; h_{J|k} \\end{pmatrix}. \\] - The conditional frequency distribution of \\(Y\\) given \\(X=a_j\\) is \\[ \\mathbf{h}_{Y|X=a_j}= \\begin{pmatrix} h_{1|j}&amp;\\cdots &amp; h_{K|j} \\end{pmatrix}. \\] Example 4.6 (Rating of corporate clients) Cont. of Example 4.4. Joint frequency distribution: \\[ \\begin{array}[t]{c|cc|c} \\mathbf{h}_{XY} &amp; Y=D &amp; Y=nD &amp; \\mathbf{h}_X\\\\\\hline X=B &amp; 0.05 &amp; 0.45 &amp; 0.5\\\\ \\phantom{X=}C &amp; 0.04 &amp; 0.16 &amp; 0.2\\\\ \\phantom{X=}D &amp; 0.21 &amp; 0.09 &amp; 0.3\\\\\\hline \\mathbf{h}_Y &amp; 0.3&amp;0.7&amp; 1 \\end{array} \\] Conditional frequencies: \\[ h_{B|D} = h(X=B|Y=D) = \\frac{h_{BD}}{h_{\\cdot D}} = \\frac{0.05}{0.3} = 0.1667 \\] Interpretation: \\(h_{B|D}=0.1667\\) means that among the corporate customers who defaulted at the end of the period, \\(16.67\\%\\) were initially given rating B. \\[ h_{D|B} = h(Y=D|X=B) = \\frac{h_{BD}}{h_{B\\cdot}} = \\frac{0.05}{0.5} = 0.1 \\] Interpretation: \\(h_{D|B}=0.1\\) means that \\(10\\%\\) of the corporate customers with rating B defaulted at the end of the period. Note: \\(h_{BD}=0.05\\) means that \\(5\\%\\) of the considered corporate customers were rated as B and eventually defaulted. Conditional frequency distribution of \\(X\\) given \\(Y=nD\\): \\[ \\mathbf{h}_{X|Y=nD} = \\begin{pmatrix} h_{B|nD} &amp; h_{C|nD} &amp; h_{D|nD} \\end{pmatrix} = \\begin{pmatrix} 0.6428 &amp; 0.2286 &amp; 0.1286 \\end{pmatrix} \\] Conditional frequency distribution of \\(Y\\) given \\(X=D\\): \\[ \\mathbf{h}_{Y|X=D} = \\begin{pmatrix} h_{A|D} &amp; h_{K|D} \\end{pmatrix} = \\begin{pmatrix} 0.7 &amp; 0.3 \\end{pmatrix} \\] 4.3 Empirical independence Two features \\(X\\) and \\(Y\\) are empirically independent if \\(Y\\) has no influence on \\(X\\) and vice versa. In relation to the conditional frequencies, this means that the conditional frequencies of \\(X\\) given \\(Y=b_k\\) are equal to the corresponding marginal frequencies of \\(X\\) for all \\(k=1,\\ldots, K\\). Analogously for \\(Y\\) given \\(X=a_j\\). \\[\\begin{align} h_{X=a_j|Y=b_1}&amp;=\\cdots=h_{X=a_j|Y=b_K}=h_{j\\cdot}, ~~\\text{ for all }j=1,\\ldots, J\\\\ h_{Y=b_k|X=a_1}&amp;=\\cdots=h_{Y=b_k|X=a_J}=h_{\\cdot k}, ~~ \\text{ for all } k=1, \\ldots, K \\end{align}\\] If there is no risk of misinterpretation, we write \\(h_{j|k}\\) instead of \\(h_{X=a_j|Y=b_k}\\). Criterion \\[\\begin{align*} &amp; X\\text{ and } Y \\text{ are independent}\\\\\\ \\iff &amp; h_{jk} = h_{j\\cdot} \\cdot h_{\\cdot k} ~~ \\text{ for all} j=1,\\ldots, J\\text{ and } k=1,\\ldots,K\\\\ \\iff &amp; \\mathbf{h}_{XY} = \\mathbf{h}_X^T\\cdot \\mathbf{h}_Y \\end{align*}\\]thbf{h}_{XY} = _X^T_Y \\end{align*} Example 4.7 (Rating of corporate clients) Because \\(h_{X=B,Y=D}=0.05\\not= 0.5\\cdot 0.3=h_{X=B}\\cdot h_{X=D}\\), the characteristics characteristics \\(X=\\text{rating}\\) and \\(Y=\\text{default}\\) are not independent = dependent. Example 4.8 (Grades and Fitness) For the mathematics results of a year group, the following characteristics are of interest \\[\\begin{align*} X&amp;= \\text{degree of fitness with the values } a \\text{ (=athletic)}, u \\text{ (=unathletic)},\\\\ Y&amp;= \\text{grade with the values } A,B,C,D. \\end{align*}\\] The evaluation of a year leads to the following empirical distribution: \\[ \\begin{array}[t]{c|cccc|c} \\mathbf{h}_{XY} &amp; Y=A &amp; B &amp; C &amp; D &amp; \\mathbf{h}_X\\\\\\hline X=a &amp; 0.06 &amp; 0.18 &amp; 0.24 &amp; 0.12 &amp; 0.6\\\\ \\phantom{X=}u &amp; 0.04 &amp; 0.12 &amp; 0.16 &amp; 0.08 &amp; 0.4\\\\\\hline \\mathbf{h}_Y &amp; 0.1 &amp; 0.3 &amp; 0.4 &amp; 0.2 &amp; 1 \\end{array} \\] Since \\(\\mathbf{h}_X^T\\cdot \\mathbf{h}_Y=\\mathbf{h}_{XY}\\), \\(X\\) and \\(Y\\) are empirically independent. Exercise 4.1 (Frequencies for two features.) 4.4 Dependence measures Measures for the relationship between two characteristics give us a value that quantifies the strength of the relationship (and in the case of correlation also the direction). The concrete type of dependence measures is chosen based on: the scale of the characteristics \\(X\\) and \\(Y\\) metric: Empirical correlation coefficient \\(\\rho_{XY}\\) ordinal: rank correlation \\(\\rho_{XY}^S\\) nominal: normalized contingency coefficient and chi-square coefficient \\(C^*,\\chi^2\\) the data representation: Sample: \\((x_1,y_1), (x_2,y_2), \\ldots, (x_n,y_n)\\) Frequency distribution:\\(\\mathbf{h}_{XY}\\) 4.4.1 Empirical covariance and correlation Metrically scaled characteristics carry information about both the direction of change (&gt;,&lt;) and the amount of change. Therefore, the idea here is to see whether and to what extent they move in the same direction (compared to the characteristic-related mean value). Example 4.9 (Sales vs. Weather) Cont. of Example 4.1. If you get more detailed information from BI, such as: Table 4.2: Sample size n=10 for the characteristics S sales in EUR and T temperature in ¬∞C. Day Sales Temperature 1 24.2 13.57 2 20.36 14.77 3 25.18 14.2 4 25.94 17.65 5 31.18 24.79 6 24.83 15.85 7 30.17 23.58 8 26.92 19.3 9 19.36 10.67 10 23.24 17.35 How can we then assess the direction and strength of the dependence between sales and temperature? For a sample \\((x_1,y_1), \\ldots, (x_n,y_n)\\) of the metrically scaled characteristics \\(X\\) and \\(Y\\), the sample covariance is calculated as \\[ \\hat\\sigma_{XY} = \\frac{1}{n-1} \\sum_{i=1}^n (x_i-\\bar x) (y_i-\\bar y) = \\frac{1}{n-1} \\sum_{i=1}^n x_i y_i - \\frac{n}{n-1} \\bar x \\bar y. \\] The empirical correlation coefficient is defined by \\[ \\hat\\rho_{XY} = \\frac{\\hat\\sigma_{XY}} {\\hat\\sigma_X \\hat\\sigma_Y}=\\frac{\\sum_{i=1}^n x_i y_i - n\\bar x \\bar y} {\\sqrt{\\sum_{i=1}^n x_i^2 - n \\bar x^2} \\cdot \\sqrt{\\sum_{i=1}^n y_i^2 - n \\bar y^2}}. \\] Interpretation: \\(\\hat\\sigma_{XY}\\) and \\(\\hat\\rho_{XY}\\) are measures for the linear association between two characteristics \\(X\\) and \\(Y\\). The units of covariance correspond to the product of the units of the underlying characteristics; the level of covariance can be difficult to interpret. Graphical representation: Scatterplot (scatter diagram). The representation of the sample pairs \\((x_1,y_1), \\ldots,(x_n,y_n)\\) as points in an \\(x\\)-\\(y\\) coordinate system is called a scatterplot. Choose \\(\\rho_{XY}\\): Example 4.10 (Sales vs. Weather) Cont. of Example 4.9. We calculate correlation for: Sample (\\(n=10\\)) Table 4.3: Sample size n=10 for the characteristics U turnover in EUR and T temperature in ¬∞C. Day Sales Temperature 1 24.2 13.57 2 20.36 14.77 3 25.18 14.2 4 25.94 17.65 5 31.18 24.79 6 24.83 15.85 7 30.17 23.58 8 26.92 19.3 9 19.36 10.67 10 23.24 17.35 Calculation of the empirical correlation coefficient \\(\\hat\\rho_{TU}\\). The sample means: \\[\\begin{align*} \\bar t&amp;= \\frac{1}{10} (13.57 + \\cdots + 17.35) = 17.1731\\text{ ¬∞C}\\\\ \\bar s&amp;= \\frac{1}{10} (24.2 + \\cdots + 23.24) = 25.1373\\text{ GE} \\end{align*}\\] The sample variances: \\[\\begin{align*} \\hat\\sigma_T^2 &amp;= \\frac{1}{9} (13.57^2 + \\cdots + 17.35^2 - 10\\cdot 17.17^2) = 19.4972\\text{ ¬∞C}^2\\\\ \\hat\\sigma_U^2 &amp;= \\frac{1}{9} (24.2^2 + \\cdots + 23.24^2 - 10\\cdot 25.14^2) = 14.0397\\text{ GE}^2 \\end{align*}\\] The sample covariance: \\[ \\hat\\sigma_{TU} = \\frac{1}{9} (13.57\\cdot 24.2 + \\cdots + 17.35\\cdot23.24 - 10\\cdot17.17\\cdot 25.14 = 14.9706~\\text{ ¬∞C}\\cdot\\text{ GE} \\] The correlation coefficient is thus calculated as: \\[ \\hat\\rho_{TU} = \\frac{14.9706}{\\sqrt{19.4972\\cdot 14.0397}} = 0.9048 \\] The characteristics are therefore clearly positively correlated. Exercise 4.2 (Correlation) For a given frequency distribution \\(\\textbf{h}_{XY}\\) from the sample \\((x_1,y_1), \\ldots,(x_n,y_n)\\). The empirical covariance is given by \\[\\begin{align*} \\hat\\sigma_{XY} &amp;= \\frac{1}{n-1} \\sum_{j=1}^J \\sum_{k=1}^K (a_j-\\bar x) (b_k-\\bar y) H_{jk} = \\frac{1}{n-1} \\sum_{j=1}^J \\sum_{k=1}^K a_j b_k H_{jk} - \\frac{n}{n-1}\\, \\bar x\\, \\bar y \\\\ &amp;= \\frac{n}{n-1} \\sum_{j=1}^J \\sum_{k=1}^K a_j b_k h_{jk} - \\frac{n}{n-1} \\bar x\\, \\bar y. \\end{align*}\\] The empirical correlation coefficient is given by \\[ \\hat\\rho_{XY} = \\frac{\\hat\\sigma_{XY}}{\\hat\\sigma_{X}\\hat\\sigma_{Y}}=\\frac{\\sum_{j=1}^J \\sum_{k=1}^K a_j b_k h_{jk} \\!-\\! \\bar x\\cdot \\bar y} {\\sqrt{\\sum_{j=1}^J a_j^2 h_{j\\cdot} \\!-\\! \\bar x^2} \\sqrt{\\sum_{k=1}^K b_k^2 h_{\\cdot k} \\!-\\! \\bar y^2}} \\] 4.4.2 Rank correlation If the characteristics are at least ordinally scaled, an empirical correlation coefficient can be calculated from the ranks of the sample values. From a sample \\(x_1, \\ldots, x_n\\) of an at least ordinally scaled characteristic, a more ordered sample \\(x_{[1]} \\leq x_{[2]} \\leq \\cdots\\leq x_{[n]}\\). If all values of the sample are different: If \\(x_i=x_{[k]}\\), then the rank of \\(x_i\\) is defined as \\(R(x_i)=k\\). If some values occur more than once, the rank is defined as the arithmetic mean of the possible ranks. Example 4.11 (Determining the rank) \\[ \\begin{array}{cccccc} \\hline \\text{Kunde}&amp; 1&amp;2&amp;3&amp;4&amp;5\\\\ \\text{Rating}&amp; A&amp;C&amp;B&amp;C&amp;C\\\\ \\text{m√∂gl. Rang}&amp;1&amp;3&amp;2&amp;4&amp;5\\\\\\hline \\text{Rang}&amp;1&amp;4&amp;2&amp;4&amp;4\\\\\\hline \\end{array} \\] Since rank(\\(C\\))\\(=\\frac 13(3+4+5) =4\\). Rank correlation /Spearman‚Äôs Rho For a sample \\((x_1,y_1), \\ldots, (x_n,y_n)\\) of ordinal or metrically scaled characteristics \\(X\\) and \\(Y\\), the associated ranks \\(R(x_1), \\ldots, R(x_n)\\) and \\(R(y_1),\\ldots, R(y_n)\\) are first assigned the ranks. The rank correlation coefficient/Spearman‚Äôs Rho is based on the classic correlation coefficient and replaces the sample values with the corresponding ranks: \\[ \\hat\\rho_{XY}^S = \\frac{\\sum_{i=1}^n R(x_i) R(y_i) - n\\cdot \\overline{R(x)}\\cdot \\overline{R(y)}} {\\sqrt{\\sum_{i=1}^n R(x_i)^2 - n\\cdot \\overline{R(x)}^2} \\sqrt{\\sum_{i=1}^n R(y_i)^2 - n\\cdot \\overline{R(y)}^2}}. \\] Special case: If both the sample values \\(x_1,\\ldots, x_n\\) and \\(y_1, \\ldots, y_n\\) are different, the following applies \\[ \\hat\\rho_{XY}^S = 1-\\frac{6\\sum_{i=1}^n d_i^2}{n(n^2-1)}\\quad\\text{ mit } d_i=R(x_i)-R(y_i). \\] This formula can also be used as an approximation. Example 4.12 (Comparison of two rating procedures: (1) and (2)) rating classes (best-to-worst): \\(AAA\\), \\(AA\\), \\(A\\), \\(BBB\\), \\(BB\\), \\(B\\), \\(CCC\\), \\(CC\\), \\(C\\), \\(D\\) \\[ \\begin{array}[t]{cccccccccccc} \\hline \\text{Client} &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 &amp; 8 &amp; 9 &amp; 10\\\\\\hline x_i\\text{ with (1)} &amp; B &amp; A &amp; CC &amp; BB &amp; CC &amp; D &amp; C &amp; A &amp; AA &amp; BBB\\\\ R(x_i) &amp; 6 &amp; 2.5 &amp; 7.5 &amp; 5 &amp; 7.5 &amp; 10 &amp; 9 &amp; 2.5 &amp; 1 &amp; 4\\\\\\hline y_i \\text{ with (2)} &amp; BB &amp; A &amp; BBB &amp; B &amp; CC &amp; CC &amp; CCC &amp; AA &amp; A &amp; BB\\\\ R(y_i) &amp; 5.5 &amp; 2.5 &amp; 4 &amp; 7 &amp; 9.5 &amp; 9.5 &amp; 8 &amp; 1 &amp; 2.5 &amp; 5.5\\\\\\hline d_i &amp; 0.5 &amp; 0 &amp; 3.5 &amp; -2 &amp; -2 &amp; 0.5 &amp; 1 &amp; 1.5 &amp; -1.5 &amp; -1.5\\\\\\hline \\end{array} \\] rank correlation: \\[ \\hat\\rho_{XY}^S = 0.8246 \\] \\(\\displaystyle\\hat\\rho_{XY}^S \\approx 1-\\frac{6\\cdot28.5}{10\\cdot 99}=0.8272\\) results from the approximation. Exercise 4.3 (Rank correlation) 4.4.3 Contingency coefficient 4.4.3.1 \\(\\chi^2\\) coefficient, contingency coefficient, normalized contingency coefficient If characteristics are measured on the nominal scale, a measure of the correlation cannot be determined on the basis of the values. Instead, a measure is derived from the frequencies and quantifies the distance to the case of independence. First, the sum of the squared and normalized deviations of the observed frequencies from the frequencies constructed under the assumption of independence is calculated (\\(\\leadsto\\) \\(\\chi^2\\) coefficient) However, since the size of the \\(\\chi^2\\) coefficient depends on the sample size \\(n\\), it is normalized (\\(\\leadsto\\) contingency coefficient \\(C\\)) The contingency coefficient is now free of the influence of \\(n\\) but still depends on the number of values of the individual characteristics and must be modified again. (\\(\\leadsto\\) normalized contingency coefficient \\(C^*\\)) The normalized contingency coefficient then always moves in the interval \\([0;1]\\), whereby \\(C^*=0\\) determines empirical independence and \\(C^*=1\\) a perfect correlation between the characteristics. \\[ \\chi^2 = n\\sum_{j=1}^J \\sum_{k=1}^K \\frac{\\left(h_{jk} - h_{j\\cdot}h_{\\cdot k}\\right)^2} {h_{j\\cdot} h_{\\cdot k}}, \\] \\(\\chi^2\\) measures the quadratic deviation from independence. The contingency coefficient is defined as \\[ C = \\sqrt{\\frac{\\chi^2}{n+\\chi^2}}, \\] and assumes a value \\(C\\in \\left[0,\\sqrt{\\frac{M-1}{M}}\\right]\\) with \\(M=\\min(J,K)\\). The normalized contingency coefficient is given by \\[ C^\\ast = C/\\sqrt{\\frac{M-1}{M}} \\] and has a value \\(C^\\ast\\in [0,1]\\). Example 4.13 (Grades and Fitness) The basic population of HWR students is available for the following characteristics: \\(A=\\text{level of fitness}\\) with the values \\(s=\\text{athletic}\\) and \\(u=\\text{unathletic}\\) and \\(B=\\text{exam grade}\\) with the values \\(e=\\text{excellent}\\), \\(g=\\text{good}\\) and \\(s=\\text{sufficient}\\) a sample with a size of \\(n=200\\) with frequency distribution: \\[ \\begin{array}[t]{c|ccc|c} h_{j,k} &amp; b_1=e &amp; b_2=g &amp; b_3=s &amp; h_{j\\cdot}\\\\\\hline a_1=s &amp; 0.07 &amp; 0.36 &amp; 0.17 &amp; 0.6\\\\ a_2=u &amp; 0.06 &amp; 0.275 &amp; 0.065 &amp; 0.4\\\\\\hline h_{\\cdot k} &amp; 0.13 &amp; 0.635 &amp; 0.235 &amp; 1\\\\\\hline \\end{array} \\] the frequencies under independence would be: \\[ \\begin{array}[t]{c|ccc} h_{j,\\cdot}\\cdot h_{\\cdot,k} &amp; b_1 &amp; b_2 &amp; b_3 \\\\\\hline a_1 &amp; 0.078 &amp; 0.381 &amp; 0.141\\\\ a_2 &amp; 0.052 &amp; 0.254 &amp; 0.094\\\\\\hline \\end{array} \\] We use this to calculate: \\[\\begin{align*} \\chi^2 &amp;= n\\cdot\\frac{(0.07-0.078)^2}{0.078} + \\frac{(0.36-0.381)^2}{0.381} + \\cdots + \\frac{(0.065-0.094)^2}{0.094} = 3,9713 \\\\ C&amp;= \\sqrt{\\frac{\\chi^2}{\\chi^2+n}} = 0.1395 \\in [0;0.7071];\\quad C^\\ast = 0.1973\\in [0.1]. \\end{align*}\\] This value indicates a slight dependency. Exercise 4.4 (Contingency coefficient) "],["regression.html", "Chapter 5 Regression 5.1 Linear regression 5.2 Non-linear regression", " Chapter 5 Regression We have empirically established a linear relationship between the sales \\(S\\) and the average temperature \\(T\\). Can we now predict the expected sales for a given temperature level? \\(\\leadsto\\) correlation calculation vs.¬†regression calculation: When asking about the linear relationship between two characteristics we distinguish between the following two aspects: Correlation: Determines the strength of the linear relationship between two characteristics \\(X\\) and \\(Y\\). In short: whether a linear relationship exists. Regression: Determines the form \\(Y=a + b X\\) of the linear relationship between \\(X\\) and \\(Y\\). In short: which linear relationship exists. In this chapter we will learn How to calculate the coefficients of linear regression. How to make the prediction and assess the quality of the model and How to extend the regression calculation to non-linear relationships. 5.1 Linear regression Let \\((x_1, y_1), \\ldots, (x_n,y_n)\\) be a sample of metrically scaled characteristics \\(X\\) and \\(Y\\). We assume a linear functional relationship between \\(Y\\) and \\(X\\): \\[ y=a+bx,\\quad a,b\\in \\mathbb R, \\] the term \\(a+bx\\) is called regression line; the variable \\(y\\) is called regressand or explained variable; the variable \\(x\\) is called regressor or explanatory variable. In practice, the relationship is usually not perfectly linear. Therefore one writes \\[\\begin{align*} y_1&amp;= a + b x_1 + \\varepsilon_1\\\\ &amp;\\vdots\\\\ y_n &amp;= a + b x_n + \\varepsilon_n, \\end{align*}\\] where \\(\\varepsilon_1, \\ldots, \\varepsilon_n\\) are the so-called error terms or residuals. Example 5.1 (Linear regression for stock returns on market returns.) We consider the data from 4.3: \\[ \\begin{array}{r|rr} month~i&amp; market ~return ~x_i&amp; stock~return~y_i\\\\\\hline 1&amp;0.4&amp;0.4\\\\ 2&amp;0.9&amp;1.5\\\\ 3&amp;1.2&amp;1.8\\\\ 4&amp;0.2&amp;1.0\\\\ 5&amp;-0.4&amp;-1.1\\\\ 6&amp;-0.5&amp;0.6 \\end{array} \\] 5.1.1 Calculation of the regression coefficients The regression coefficients, the intercept \\(a\\) and the slope of the regression line \\(b\\), are to be determined. The method of least squares (MkQ) determines \\(a\\) and \\(b\\) by minimizing the sum of the squared error terms: \\[ \\min_{a,b} \\sum_{i=1}^n \\varepsilon_i^2 = \\min_{a,b} \\sum_{i=1}^n (y_i-(a+bx_i))^2. \\] Let \\((x_1,y_1), \\ldots, (x_n,y_n)\\) be a sample of the characteristics \\(X\\) and \\(Y\\). The system of linear equations \\[ y_i = a + b x_i + \\varepsilon_i, \\quad i=1,\\ldots, n \\] is called linear regression with intercept \\(a\\), slope \\(b\\) and error terms (residuals) \\(\\varepsilon_1,\\ldots, \\varepsilon_n\\). The least squares estimators for \\(a\\) and \\(b\\) are given by \\[\\begin{align*} \\hat a &amp;= \\overline y-\\hat b \\overline x, \\\\ \\hat b &amp;= \\frac{\\hat\\sigma_{XY}} {\\hat \\sigma_X^2} = \\hat\\rho_{XY}\\cdot \\frac{\\hat\\sigma_Y}{\\hat\\sigma_X} = \\frac{\\sum_{i=1}^n x_i y_i - n\\overline x\\overline y} {\\sum_{i=1}^n x_i^2 - n \\overline x^2} = \\frac{\\sum_{i=1}^n (x_i-\\overline x) (y_i-\\overline y)} {\\sum_{i=1}^n (x_i-\\overline x)^2}. \\end{align*}\\] 5.1.2 Coefficient of determination The coefficient of determination \\(R^2\\) describes the proportion of the total variance of the sample \\((y_1,\\ldots,y_n)\\) that is explained by the regression line. In the case of linear regression, the coefficient of determination \\(R^2\\) is defined as \\[ R^2 = 1-\\frac{\\sum_{i=1}^n (a+b x_i-y_i)^2} {\\sum_{i=1}^n (y_i-\\overline y)^2}. \\] It holds \\[ 0\\leq R^2\\leq 1\\quad\\text{ und } \\quad R^2 = \\hat\\rho_{XY}^2. \\] \\(R^2=0\\) corresponds to the case of uncorrelatedness, and \\(R^2=1\\) corresponds to the case of perfect linear correlation between the features \\(X\\) and \\(Y\\). Example 5.2 (Sales vs Weather) Cont. of Example 4.9. We perform the linear regression from sales to temperature. Characteristics: \\(U=\\text{sales per customer}\\) \\(T=\\text{average temperature}\\) Table 5.1: Sample size n=10 for the characteristics U turnover in EUR and T temperature in ¬∞C. Day Sales Temperature 1 24.2 13.57 2 20.36 14.77 3 25.18 14.2 4 25.94 17.65 5 31.18 24.79 6 24.83 15.85 7 30.17 23.58 8 26.92 19.3 9 19.36 10.67 10 23.24 17.35 Estimate the coefficients of the regression line \\(u=a+bt\\) from the data, where \\(\\bar s= 25.1373\\); \\(\\bar t=17.1731\\); \\(s_S^2=14.0397\\), \\(s_T^2 = 19.4972\\) and \\(\\hat\\sigma_{TS}=14.9706.\\) Then \\(\\hat b=\\displaystyle\\frac{14.9706}{19.4972} = 0.7678\\) and \\(\\hat a=25.1373+ 0.7678\\cdot 17.1731 = 11.9511.\\) Sales as a function of temperature: \\(s= 11.9511 + 0.7678\\cdot t,~\\text{ f√ºr } 10\\leq t\\leq 25.\\) Coefficient of determination: \\(R^2=\\hat\\rho_{UT}^2=(0.9048)^2=0.8187.\\) Graphically: 5.2 Non-linear regression If the relationship is not linear, such as \\(v=\\alpha\\cdot \\beta^u\\): convert \\(\\rightarrow\\) into a linear relationship using a suitable transformation, e.g.¬†by logarithmizing: \\(\\ln v = \\ln \\alpha + (\\ln \\beta)\\cdot u\\). Set: \\(y=\\ln v\\), \\(a=\\ln \\alpha\\), \\(b=\\ln \\beta\\) and \\(x=u\\) \\(\\Rightarrow\\) The transformed function \\(y=a+bx\\) is a straight line with the coefficients coefficients \\(a\\), \\(b\\). The further procedure: Transform the data \\((u_i, v_i)\\) of the sample accordingly Calculate the regression coefficients \\(a\\) and \\(b\\) with the MkQ Transform backwards \\(\\alpha\\) and \\(\\beta\\) from \\(a\\) and \\(b\\); here \\(\\hat\\alpha=\\text e^a\\) and \\(\\hat\\beta=\\text e^b\\) Obtain the estimated functional equation; here \\(v=\\hat\\alpha\\cdot \\hat\\beta^u\\) Example 5.3 (Nichtlineare (log-linearisierte) Nachfrage) Econometric relation: exponential demand \\(q\\) for a certain consumer good specific consumer good as a function of the price \\(p\\) \\[ q = \\alpha\\text e^{\\beta \\cdot u}, \\beta&lt;0 \\] Transformation: \\[ q=\\alpha \\, \\text e^{\\beta\\cdot p} \\quad\\Rightarrow\\quad \\ln q = \\ln\\alpha + \\beta\\cdot p \\quad\\iff\\quad y=a+b\\, x, \\] so: \\(y=\\ln q\\), \\(x=p\\), \\(a=\\ln \\alpha\\), \\(b=\\beta\\) the original data (\\(q,p\\)) and the transformed data (\\(x,y\\)): \\[ \\begin{array}[t]{rrrrr} \\hline i &amp; q_i &amp; p_i &amp; x_i &amp; y_i\\\\\\hline 1 &amp; 2.5224 &amp; 5.75 &amp; 5.75 &amp; 0.9252\\\\ 2 &amp; 1.6603 &amp; 15.77 &amp; 15.77 &amp; 0.507\\\\ 3 &amp; 2.1616 &amp; 8.18 &amp; 8.18 &amp; 0.7708\\\\ 4 &amp; 0.0578 &amp; 17.66 &amp; 17.66 &amp; -2.8508\\\\ 5 &amp; 0.0818 &amp; 18.81 &amp; 18.81 &amp; -2.5035\\\\ 6 &amp; 3.7369 &amp; 0.91 &amp; 0.91 &amp; 1.3183\\\\ 7 &amp; 2.8806 &amp; 10.56 &amp; 10.56 &amp; 1.058\\\\ 8 &amp; 0.2824 &amp; 17.85 &amp; 17.85 &amp; -1.2644\\\\\\hline \\end{array} \\] Least-Squares estimates for \\(a\\) and \\(b\\) (\\(\\hat\\sigma_{XY} = -9.0957\\), \\(\\hat\\sigma_X^2 = 43.6473\\), \\(\\bar x= 11.9362\\) and \\(\\bar y =-0.2549\\): \\[\\begin{align*} \\hat b&amp;= \\frac{-9.0957} {43.6473} =-0.2084\\\\ \\hat a&amp;= -0.2549 -0.2084 \\cdot 11.9362 = 2.2325 \\end{align*}\\] Transformation back into the parameters \\(\\alpha\\) and \\(\\beta\\): \\[ \\hat \\alpha = \\text e^{\\hat a} = 9.323 \\text{ und } \\quad \\hat \\beta=\\hat b = -0.2084 \\] Demand function: \\[ \\hat q=9.323\\cdot \\text e^{-0.2084\\cdot x} \\] Transformation table For the transformation \\(v=f(u,\\alpha\\beta)\\rightarrow y=a+b\\,x\\) there are further examples of non-linear problems that can be can be transformed to a linear regression approach, are: \\[ \\begin{array}[t]{c|ccccc} \\hline No&amp;v=f(u) &amp; y=T(v) &amp; a &amp; b &amp; x=g(u)\\\\\\hline 1&amp;v=\\alpha + \\beta\\, u^k &amp; v=\\alpha + \\beta\\cdot u^k &amp; \\alpha &amp; \\beta &amp; u^k\\\\[5pt] 2&amp;v=\\frac{\\alpha\\, u}{\\beta+u} &amp; \\frac{1}{v} = \\frac{1}{\\alpha} + \\frac{\\beta}{\\alpha}\\, \\frac{1}{u} &amp; \\frac{1}{\\alpha} &amp; \\frac{\\beta}{\\alpha} &amp; \\frac{1}{u}\\\\[5pt] 3&amp;v = \\frac{u}{\\alpha + \\beta\\, u} &amp; \\frac{u}{v} = \\alpha + \\beta\\, u &amp; \\alpha &amp; \\beta &amp; u\\\\[5pt] 4&amp;v = \\alpha \\, \\beta^u &amp; \\ln v = \\ln \\alpha + (\\ln\\beta) u &amp; \\ln \\alpha &amp; \\ln \\beta &amp; u\\\\[5pt] 5&amp;v = \\alpha\\, u^\\beta &amp; \\ln v = \\ln \\alpha + \\beta\\, \\ln u &amp; \\ln \\alpha &amp; \\beta &amp; \\ln u\\\\[5pt]\\hline \\end{array} \\] the regressor the error term the dependent variable the independent variable the regressand the residuum Submit the regressor the error term the dependent variable the independent variable the regressand the residuum Submit the proportion of variance in the regression. the proportion of variance that was not explained by the regression. the correlation. the proportion of variance explained by the regression. the proportion of variance in the error terms. Submit Submit \\(x=\\frac 1{\\ln(u)}\\). \\(x=u\\). \\(x=\\ln(u)\\). \\(x=\\frac1u\\). \\(x=e^u\\). Submit "],["ii-probability-theory.html", "II Probability theory", " II Probability theory In Part I, we learned how to summarize, visualize and analyze the data from an existing sample (from a partial survey, that is) for one or more characteristics. However, our goal has not yet been achieved at this point, because what we really want to achieve are valid inferences about the distribution in the population based on this sample realization. At present, it is not yet clear whether such inferences are permissible at all, and if so, what must be taken into account? Which key figures from Part I provide information about which parameters of the population and why did we calculate the key figures the way we did? (Where do the formulas come from?) In order to answer these and other questions in this regard, we must first understand how a possible sample realization can be modeled as the result of a random selection from a population, i.e.¬†a random process. Probability theory describes stochastic processes such as: the roll of a 6 on a dice, the price development of a share, the default of a borrower as well as sampling using suitable stochastic models. The term characteristic from Part I now corresponds to the new term random variable from the point of view of probability theory. As with characteristics with empirical distributions, we are primarily interested in the now modeled distributions of random variables. In this section, we deal with the probabilistic background and start with the model of the random process. "],["model-of-randomness.html", "Chapter 6 Model of randomness 6.1 Probability space 6.2 Sampling as a random process", " Chapter 6 Model of randomness A random process or (if repeatable) random experiment is a process characterized by several possible, different, mutually exclusive outcomes. The result is unknown before it is carried out. The uncertainty associated with the occurrence of a specific result is described by probabilities. Each random process can be described by a probability space. In this chapter we will learn How to describe a random process via an appropriate probability space, how to define relevant events as sets of favorable outcomes and how to calculate probabilities for these events based on this. 6.1 Probability space A probability space consists of the following elements: \\(\\Omega\\): Outcomes set or sample space \\(\\mathcal F\\): Event space and \\(\\mathbb P\\): Probability measure. In this chapter, we restrict ourselves primarily to random processes with a finite number of outcomes. 6.1.1 Outcomes Outcome set \\(\\Omega\\) is the set of all possible outcomes of a stochastic process. An outcome or elementary event \\(\\omega\\) from the set \\(\\Omega\\) is the possible outcome of a random experiment, the future state of the experiment, or the characteristic value of a randomly selected object. Examples Result of a throw of the dice \\(\\Omega=\\{1,\\ldots, 6\\}\\); Price of a share at the end of the coming week: \\(\\Omega=\\mathbb R_+\\); Height of a randomly selected person: \\(\\Omega=\\mathbb R_+\\). Events are situations of the stochastic process that can occur. can occur. Formally, an event is described as a subset of \\(\\omega\\), which contains all outcomes \\(\\omega\\) that are favorable for the occurrence of the event. Special events are: elementary event \\(\\{\\omega\\}\\): consists of only one result \\(\\omega\\in\\Omega\\) safe event \\(\\Omega\\): consists of all possible results impossible event\\(\\emptyset\\): contains no result at all (empty set) Describing complex event structures with set-theoretic operations: \\[\\begin{align*} A^c &amp;= \\{\\omega\\in\\Omega| \\omega\\not\\in A\\} = \\text{ event $A$ does not occur }\\\\\\ A\\cup B&amp;= \\{\\omega\\in\\Omega| \\omega\\in A \\text{ or } \\omega\\in B\\} = \\text{ $A$ or $B$ enters }\\\\\\ A\\cap B&amp;= \\{\\omega\\in \\Omega| \\omega\\in A\\text{ and } \\omega\\in B\\} = \\text{ $A$ and $B$ enter }\\\\\\ A\\setminus B &amp;= \\{\\omega\\in\\omega|\\omega\\in A \\text{ and } \\omega\\not\\in B\\} = \\text{ $A$ occurs, but not $B$.} \\end{align*}\\] B &amp;= {|A B} = \\end{align*} Power set \\(\\mathcal P(\\Omega)\\) is the set of all subsets of \\(\\Omega\\). The power of \\(A\\) denotes the number of all elements \\(\\omega\\) in \\(A\\); notation: \\(|A|\\). Example 6.1 (Rolling a dice twice) We conduct the experiment: two-times rolling a fair dice with faces 1, 2 and 3 each on two sides of the dice. Outcomes: \\[\\begin{align*} \\Omega &amp;= \\{(1;1), (1;2), (1;3), (2;1),\\ldots, (3;3)\\}\\\\ &amp;=\\{(a,b) | a,b\\in \\{1; 2; 3\\}\\}, \\end{align*}\\] where \\(a=\\text {result in 1st throw}\\) and \\(b=\\text {result 2nd in throw}\\) The power of \\(\\Omega\\) is: \\(|\\Omega| = \\text{possibilities}^\\text{number of repetitions} = 3^2=9.\\) Events: verbal: \\(A=\\)‚ÄúDie Augensumme beider W√ºrfel betr√§gt 4‚Äù; formal: \\(A=\\{(a,b)\\in \\Omega | a+b=4\\} =\\) \\(\\{(1;3), (2;2), (3;1)\\}\\) verbal: \\(B=\\)‚ÄúDie Augenzahlen beider W√ºrfel ungerade‚Äù; formal: \\(B=\\{(a,b)\\in \\Omega | a, b \\text{ ungerade}\\} =\\) \\(\\{(1;1), (1;3), (3;1), (3;3)\\}\\) verbal: \\(A\\cup B=\\)‚ÄúDie Augensumme beider W√ºrfel betr√§gt 4 oder die Augenzahlen sind ungerade‚Äù; formal: \\(A\\cup B=\\{(a,b)\\in \\Omega | a+b=4 \\text{ oder } a,b \\text{ ungerade }\\} =\\) \\(\\{(1;1), (1;3), (2;2), (3;1), (3;3)\\}\\) verbal: \\(A\\cap B=\\)‚ÄúDie Augensumme beider W√ºrfel betr√§gt 4 und zugleich die Augenzahlen sind ungerade‚Äù; formal: \\(A\\cup B=\\{(a,b)\\in \\Omega | a+b=4 \\text{ und } a,b \\text{ ungerade }\\} =\\) \\(\\{(1;3), (3;1)\\}\\) Interactive switch to the section ‚ÄúProbability‚Äù, simulate dice rolls for different \\(n\\), compare the probabilities with the corresponding sample frequencies, What do you observe when \\(n\\) is increasing? 6.1.2 Event space All events for which a probability can be calculated are summarized in the event space \\(\\mathcal F\\). Formally, \\(\\mathcal F\\) is a set of subsets of \\(\\Omega\\), i.e.¬†a subset of the power set \\(\\mathcal P(\\Omega)\\). In the case of a finite and countable result set, \\(\\mathcal F=\\mathcal P(\\Omega)\\) is normally chosen, as this event space then contains all events that can be defined for the random experiment at hand. Example 6.2 (Rolling a dice twice) Cont. of Example 6.1. In this experiment, the result set is finite, so we can choose the power set of \\(\\Omega\\) \\(\\mathcal P(\\Omega)\\) as the event space: \\[\\begin{align} \\mathcal F &amp;= \\mathcal P(\\Omega)\\\\ &amp;=\\big\\{\\{(1;1)\\}, \\{(1;2)\\},\\ldots,\\{(3;3)\\}, \\{(1;1),(2;1)\\}, \\ldots, \\{(1;1),(2;1), (3;2)\\}, \\ldots, \\Omega,\\emptyset\\big\\} \\end{align}\\] Exercise 6.1 (Events) 6.1.3 Probability measure Probabilities of events quantify the uncertainty associated with the outcome of a random experiment. The probability measure assigns a number between \\(0\\) and \\(1\\) to each event. For \\(\\omega\\in\\Omega\\), the mapping is called \\(\\omega\\mapsto\\mathbb P(\\omega)\\) probability measure. For an event \\(A\\in\\mathcal F\\), the probability of \\(A\\) is equal to the sum of the probabilities for the outcomes contained in \\(A\\): \\(\\mathbb P(A) = \\sum_{\\omega\\in A} \\mathbb P(\\omega)\\). Every probability measure must fulfill the axioms of Kolmogorov: \\(\\mathbb P(A)\\geq 0\\), \\(\\mathbb P(\\omega )=1\\), If \\(A\\cap B=\\emptyset\\), then \\(\\mathbb P(A\\cup B)=\\mathbb P(A)+\\mathbb P(B)\\). Laplace‚Äôs probability model With \\((\\Omega, \\mathcal F,\\mathbb P)\\) a Laplace model exists if the following conditions are met: The result set \\(\\Omega\\) is finite and all results are equally probable. The power set \\(\\mathcal P(\\Omega)\\) is chosen as the event space, i.e.¬†each subset \\(A\\subseteq \\Omega\\) is an event. The probability of an event \\(A\\in\\mathcal F\\) is then calculated by \\[\\begin{equation*} \\mathbb P(A) = \\frac{|A|}{|\\Omega|} = \\frac{\\text{number of favorable outcomes for the occurrence of $A$}} {\\text{number of all possible outcomes}} \\end{equation*}\\] Example 6.3 (Rolling a die twice) Cont. of Example 6.1 and 6.2. In this experiment, the sample set is finite, the outcomes are equally probable and we have chosen the power set of \\(\\Omega\\) as the event space. Consequently, Laplace‚Äôs probability model can be applied here. \\[\\begin{align} \\mathbb P(A) &amp;= \\frac{|A|}{|\\Omega|}=\\frac 39 = \\frac 13,\\\\ \\mathbb P(B) &amp;= \\frac{|B|}{|\\Omega|}=\\frac 49. \\end{align}\\] Finite probability spaces The triple \\((\\Omega,\\mathcal F,\\mathbb P)\\) is a finite probability space if the following conditions are met: The sample set \\(\\Omega\\) is finite. The power set \\(\\mathcal P(\\Omega)\\) is chosen as the event field. The individual probabilities \\(\\mathbb P(\\omega)\\) of all outcomes \\(\\omega\\in\\Omega\\) are non-negative and add up to \\(1\\). The probability of an event \\(A\\subseteq \\Omega\\) is then calculated by \\[\\begin{equation*} \\mathbb P(A) = \\sum_{\\omega\\in A} \\mathbb P(\\omega) = \\text{sum of all function values } \\mathbb P(\\omega)\\text{ with } \\omega\\in A. \\end{equation*}\\] If \\(\\mathbb P(\\omega)=\\displaystyle\\frac{1}{N}\\) for all \\(\\omega\\in\\Omega=\\{\\omega_1, \\ldots, \\omega_N\\}\\), this is a special case of a Laplace model. Example 6.4 (Sum of the dice faces) Cont. of Example 6.1 and 6.3. If we are now interested in the sum of the faces instead of the numbers, the new probability space is \\(\\Omega = \\{2;3;4;5;6\\}\\), \\(\\mathcal F=\\mathcal P(\\Omega)\\): we still choose the power set as the event space, \\(\\mathbb P\\): \\[ \\begin{array}{c|ccccc}\\omega_i&amp;2&amp;3&amp;4&amp;5&amp;6\\\\\\hline \\mathbb P(\\{\\omega_i\\})&amp;\\frac 19&amp;\\frac 29&amp;\\frac 39&amp;\\frac 29&amp;\\frac 19 \\end{array} \\] In this new experiment, the result set is still finite and the power set of \\(\\Omega\\) serves as the event space. Nevertheless, the results are not equally probable. Consequently, the general finite probability space is used here. For the events \\(S_{&gt;4}: \\text{the sum of the numbers of points is greater than 4}\\) and \\(S_{even}: \\text{the sum of the number of points is even}\\) applies: \\[\\begin{align} \\mathbb P(S_{&gt;4}) &amp;= \\mathbb P(\\{5\\}) + \\mathbb P(\\{6\\}) = \\frac 13,\\\\ \\mathbb P(S_{even}) &amp;= \\mathbb P(\\{2\\}) + \\mathbb P(\\{4\\}) + \\mathbb P(\\{6\\})=\\frac 59. \\end{align}\\] 6.1.4 Calculating with probabilities The following properties of \\(\\mathbb P\\) apply in general models and facilitate the calculation of probabilities for complex events: \\(\\mathbb P(\\emptyset)=0\\) \\(A\\subseteq B\\) \\(\\Rightarrow\\) \\(\\mathbb P(A)\\leq \\mathbb P(B)\\) and \\(\\mathbb P(B\\setminus A)= \\mathbb P(B)-\\mathbb P(A)\\) \\(\\mathbb P(A^c) = 1-\\mathbb P(A)\\) \\(\\mathbb P(A\\cup B) = \\mathbb P(A) + \\mathbb P(B) - \\mathbb P(A\\cap B)\\) \\(\\mathbb P(A\\cap B) = \\mathbb P(A)\\cdot\\mathbb P(B),\\) for independent \\(A\\) and \\(B\\). Example 6.5 (Summe der Augenzahlen) Cont. of Example 6.4. \\(\\Omega = \\{2;3;4;5;6\\}\\), \\(\\mathcal F=\\mathcal P(\\Omega)\\): we still choose the power set as the event space, \\(\\mathbb P\\): \\[ \\begin{array}{c|ccccc}\\omega_i&amp;2&amp;3&amp;4&amp;5&amp;6\\\\\\hline \\mathbb P(\\{\\omega_i\\})&amp;\\frac 19&amp;\\frac 29&amp;\\frac 39&amp;\\frac 29&amp;\\frac 19 \\end{array} \\] The following applies \\(S_{&gt;4}: \\text{the sum of the point numbers is greater than 4}\\) with \\(\\mathbb P(S_{&gt;4}) =\\frac 13\\), \\(S_{even}: \\text{the sum of the numbers is even}\\) with \\(\\mathbb P(S_{even}) =\\frac 59.\\) For \\(S_{&gt;2}:\\text{the sum of the point numbers is greater than 2}\\) applies: \\(S_{&gt;2}^c=\\text{the sum of the numbers of points is less than or equal to 2}=\\{2\\}\\) \\(\\leadsto\\) \\(\\mathbb P(S_{&gt;2}^c)=\\mathbb P(\\{2\\})=\\frac 19\\). Then \\(\\mathbb P(S_{&gt;2})=1-\\mathbb P(S_{&gt;2}^c) = \\frac 89.\\) since \\(S_{&gt;4}\\subset S_{&gt;2}\\) \\(\\leadsto\\) $P(S_{&gt;2}S_{&gt;4})=P(S_{&gt;2}) - P(S_{&gt;4}) =, $ where \\(S_{&gt;2}\\setminus S_{&gt;4} = S_{&gt;2}\\cap S_{&gt;4}^c =\\) $. $ For \\(S_{&gt;4}\\cap S_{even}: \\text{the sum of the numbers is greater than 4 and even}\\) applies: \\(\\mathbb P(S_{&gt;4}\\cap S_{even}) = \\mathbb P(\\{6\\}) = \\frac 19.\\) For \\(S_{&gt;4}\\cup S_{even}: \\text{the sum of the point numbers is greater than 4 or even}\\) applies: \\(\\mathbb P(S_{&gt;4}\\cup S_{even}) = \\mathbb P(S_{&gt;4}) + \\mathbb P(S_{even}) - \\mathbb P(S_{&gt;4}\\cap S_{even}) = \\frac 13 + \\frac 59 - \\frac 19 = \\frac 79.\\) The calculation of probabilities in the Laplace model therefore only requires counting the favorable or possible results. For large quantities, however, this is hardly feasible without a system. Formulas of combinatorics: Choose \\(k\\) times from \\(n\\) possibilities‚Ä¶ \\[ \\begin{array}{c|c|c}\\hline \\text{draws} &amp;\\text{with array} &amp; \\text{without array}\\\\\\hline \\text{with repetition} &amp;n^k &amp;\\binom{n+k-1}{k}\\\\ \\text{without repetition}&amp;\\frac{n!}{(n-k)!}&amp;\\binom{n}{k}=\\frac{n!}{(n-k)!\\cdot k!}\\\\\\hline \\end{array} \\] Different arrangements of \\(k\\) possibilities on \\(k\\) places (without repetitions) \\(\\leadsto\\) permutations: \\(k\\)! Example 6.6 (Sampling Customers) We draw \\(3\\) customers at random from the customer pool consisting of a total of \\(5\\) customers. I.e. \\(n=5\\), \\(k=3\\). There are \\(5^3=125\\) variants (results) for the random selection with repetition (with backing up) and with consideration of the arrangement (order) of \\(3\\) from \\(5\\) customers. There are \\(\\binom{5+3-1}{3}=\\binom{7}{3}= \\frac{7!}{(7-3)!\\cdot 3!}=35\\) variants for the random selection with weighing (with putting back) but without taking into account the arrangement (order) of \\(3\\) from \\(5\\) customers. There are \\(\\frac{5!}{(5-3)!}=60\\) variants for the random selection without weighing (without putting back) but with consideration of the arrangement (order) of \\(3\\) out of \\(5\\) customers. There are \\(\\binom{5}{3}= \\frac{5!}{(5-3)!\\cdot 3!}=10\\) variants for the random selection without weighing (without putting back) and without taking into account the arrangement (the order) of \\(3\\) from \\(5\\) customers. 6.2 Sampling as a random process Now let‚Äôs look at (somewhat simplified) examples of how sampling can be modeled as a random process. Example 6.7 (Random selection of customers) We first model the sampling mechanism in a simplified form. Suppose we have a total of \\(5\\) customers (labeled \\(x_1\\) to \\(x_5\\)) and select \\(3\\) customers randomly and independently (we ‚Äúput‚Äù the drawn customers back into the customer pot before we select the next one, i.e.¬†one and the same customer can occur several times). How can we describe this random selection with a suitable probability space? Result set (\\(\\leadsto\\) sample space): Results are sample realizations or sample realizations. \\[\\begin{align} \\Omega &amp;=\\{(x_1,x_1,x_1),(x_1,x_2,x_2), \\ldots, (x_1,x_2,x_3), \\ldots, (x_5, x_2,x_4), \\ldots, (x_5,x_5,x_5)\\}\\\\ |\\Omega|&amp;=5^3 = 125. \\end{align}\\] Event space (since \\(\\Omega\\) is finite \\(\\leadsto\\) power set): \\[ \\mathcal F = \\mathcal P(\\Omega). \\] Probabilities: each result is equally probable (we draw with backing), i.e.¬†\\(\\mathbb P(\\{(x_i,x_j,x_k)\\} = \\frac 1{|\\Omega|} = \\frac 1{125}\\). The Laplace probability model therefore applies. For \\(A\\): The combination of customers 1, 2 and 3 is selected, \\(A = \\{(x_1,x_2,x_3),(x_2,x_1,x_3),\\ldots,(x_3,x_2,x_1)\\}\\)) \\(|A|=3!\\) and \\(\\mathbb P(A) = \\frac{|A|}{|\\Omega|} = \\frac 6{125}.\\) For \\(B\\): Customer 1 is selected at least once at random, \\(B = \\{(x_1,x_1,x_1),\\ldots, (x_5,x_1,x_2),\\ldots, (x_5,x_5,x_1)\\}\\)) applies \\(\\mathbb P(B) = 1-\\mathbb P(B^c)\\) with \\(|B^c|=4^3 = 64.\\) \\(\\mathbb P(B) = 1-\\frac{|B^c|}{|\\Omega|} = \\frac {61}{125}.\\) For \\(C\\): Three different customers are chosen at random (without repetition), \\(C = \\{(x_1,x_2,x_3),(x_2,x_4,x_3),\\ldots,(x_3,x_2,x_5)\\}\\)), \\(|C|=\\frac{5!}{(5-3)!} = 60\\) and \\(\\mathbb P(C) = \\frac{|C|}{|\\Omega|} = \\frac {12}{25}.\\) For the operational questions, it is less interesting which customers are specifically selected, but rather which characteristics they have. We therefore modify our modeling by looking at the specific characteristic ‚Äúuse of app or web interface‚Äù. Example 6.8 (Sampling Customers: App vs Web) Cont. of Example 6.7. We had already looked at the feature ‚ÄúUse of app or web interface‚Äù by our customers (cf.¬†1.3). Now we will recreate the sampling mechanism for this characteristic. Assume that the distribution of \\(5\\) customers is as follows: \\[ \\begin{array}{c|ccccc}\\hline Kunde~i&amp;1&amp;2&amp;3&amp;4&amp;5\\\\\\hline Usage~a_i&amp;App&amp;Web&amp;App&amp;App&amp;Web\\\\\\hline \\end{array} \\] I.e. \\(\\mathbb P(App) = 0.6\\) and \\(\\mathbb P(Web)=0.4.\\) Which different sample outcomes with what probability can we draw? Result set: \\(x_i\\) get the corresponding values (e.g.¬†\\(x_1=App\\), \\(x_2=Web\\)) \\[\\begin{align} \\Omega &amp;= \\{(App; App; App), (App; Web; Web),\\ldots, (App; Web; App),\\ldots, \\\\ &amp;~~~~(Web; Web; App),\\ldots,(Web; Web; Web) \\}\\\\ |\\Omega|&amp;=5^3 = 125. \\end{align}\\] Event space: \\(\\mathcal F=\\mathcal P(\\Omega).\\) Probability measure: Equal probabilities Events: \\(A_3=\\) 3 times app: only customers 1, 3 and 4 may be selected \\(\\leadsto\\) \\(|A_3|=3^3=27\\) and \\(\\mathbb P(A_3)=\\frac{27}{125}.\\) \\(A_0=\\) 0-times app: only customers 2 and 5 may be selected \\(\\leadsto\\) \\(|A_0|=2^3=8\\) and \\(\\mathbb P(A)=\\frac{8}{125}.\\) \\(A_1=\\) 1-time app: one seat is occupied by customer 1, 3 or 4; two other seats - by customer 2 or 5 \\(\\leadsto\\) \\(|A_1|=3\\cdot 3\\cdot 2^2=36\\) and \\(\\mathbb P(A_1)=\\frac{36}{125}.\\) \\(A_2=\\) 2 times App \\(\\leadsto\\) \\(\\mathbb P(A_2)=1-\\mathbb P(A_2^c) = 1-(P(A_0) + P(A_1) + P(A_3))= 1-\\frac{71}{125} = \\frac{54}{125}.\\) What is expected on average (across all sample outocmes)? \\[ 0\\cdot \\frac{8}{125} + 1\\cdot \\frac{36}{125}+2\\cdot \\frac{54}{125}+3\\cdot \\frac{27}{125} = 1.8 \\] or as a proportion \\(\\frac{1.8}{3}=0.6\\) \\(\\leadsto\\) so on average we have the same proportion (60%) of app users as in the entire customer base! Sometimes two or more characteristics are of interest: based on our example ‚Äúnumber of orders‚Äù (see 2.7), we could now model in connection with an additional characteristic such as ‚Äúuse of app or web interface‚Äù. What then happens during sampling? Example 6.9 (Orders and Usage) Here, too, we simplify the whole thing a little and assume that we already know the common probabilities. Now we randomly select user accounts and determine how many order transactions there were in the last month and via which system (app or web) they were made. Which different sample outcomes with what probability can we draw? Result set: \\[\\begin{align} \\Omega &amp;= \\{(App; 1), (App; 2),\\ldots, (App; 6),\\ldots, \\\\ &amp;~~~~(Web;1),\\ldots,(Web; 6) \\}\\\\ \\\\Omega|&amp;=12. \\end{align}\\] Event space: \\(\\mathcal F=\\mathcal P(\\Omega).\\) Probability measure: (assumption that we know the joint probabilities). \\[ \\begin{array}{c|cccccc} \\mathbb P((a_i,b_i))&amp;b_1=1&amp;b_2=2&amp;b_3=3&amp;b_4=4&amp;b_5=5&amp;b_6=6\\\\\\hline a_1=App&amp;0.02&amp;0.06&amp;0.12&amp;0.2&amp;0.12&amp;0.08\\\\ a_2=Web&amp;0.08&amp;0.09&amp;0.13&amp;0.05&amp;0.03&amp;0.02\\\\ \\end{array} \\] Events: \\(A_1 =\\) Orders via app \\(\\leadsto\\) \\[\\begin{align}\\mathbb P(A_1)&amp;=\\mathbb P((App;1),\\ldots,(App;6)) =\\\\&amp;~~~0.02 + \\ldots + 0.08 = 0.6.\\end{align}\\] \\(B_{\\geq 3}=\\) At least 3 orders \\(\\leadsto\\) \\[\\begin{align}\\mathbb P(B_{\\geq 3})&amp;=\\mathbb P((App;3),\\ldots,(App;6),(Web;3),\\ldots,(Web;6)) =\\\\&amp;~~0.12 + \\ldots + 0.08 + 0.13 + \\ldots + 0.02 = 0.75.\\end{align}\\] \\(A_1\\cap B_{\\geq 3}=\\) At least 3 orders via app \\(\\leadsto\\) \\(A_1\\cap B_{\\geq 3}=\\{(App;3),(App;4),(App;5),(App;6)\\}\\) \\[\\begin{align}\\mathbb P(A_1\\cap B_{\\geq 3})&amp;=\\mathbb P((App;3),(App;4),(App;5),(App;6)) =\\\\&amp;~~~0.12 + 0.2 + 0.12 + 0.08 = 0.52.\\end{align}\\] "],["conditional-probabilities-and-independence.html", "Chapter 7 Conditional probabilities and independence 7.1 Conditional probabilities 7.2 Independence 7.3 Additional information for sampling 7.4 Total probability and Bayes‚Äô formula", " Chapter 7 Conditional probabilities and independence Conditional probabilities express the fact that information about the occurrence of an event \\(B\\) can influence the assessment of the probability of an event \\(A\\). Example: The assessment of the probability that a 40-year-old man will suffer a stroke differs depending on whether it is known that he has high blood pressure or whether this information is not available. In the operational context, too, the focus is often not on joint distribution, but rather on what happens to an event of interest if another related event has already occurred. For example, if it is known that a randomly selected customer uses the app, how many orders per month could be expected from the customer? The chance for the respective number of orders is described by conditional probabilities. In other situations, some conditional probabilities are known, but you would need the ‚Äúreverse‚Äù conditional probabilities or the total (unconditional) probabilities. For example, if in a rating model you have the information on the chances of being loaded into a rating category in the event of a subsequent default, but you need the inverse of this, i.e.¬†the chance of defaulting if you are sorted into a certain rating category (default probability per rating category). In this case, the total probability (total probability of default) is also of interest. When assessing probabilities for additional information, the decisive factor is whether the events involved are dependent or independent. In this chapter we will learn How to calculate and interpret the probabilities for an event under the condition that another event has already occurred (\\(\\leadsto\\) conditional probability) and how to check independence of events using conditional probabilities. How to calculate unconditional probabilities from conditional probabilities (\\(\\leadsto\\) total probability) and how to ‚Äúswap‚Äù the event of interest and the condition in the conditional probability if necessary (\\(\\leadsto\\) Bayes‚Äô theorem). 7.1 Conditional probabilities For two events \\(A,B\\subseteq \\Omega\\) and with \\(\\mathbb P(B)&gt;0\\) the conditional probability of \\(A\\) under \\(B\\) is defined as \\[ \\mathbb P(A|B) = \\displaystyle \\frac {\\mathbb P(A\\cap B)}{ \\mathbb P(B)}. \\] Example 7.1 (Rolling a die twice) Cont. of Example 6.3. In the experiment: rolling a fair dice twice with the numbers 1, 2 and 3 on two sides of the dice, we defined Result set: \\[\\begin{align*} \\Omega &amp;= \\{(1;1), (1;2), (1;3), (2;1),\\ldots, (3;3)\\}\\\\ &amp;=\\{(a,b) | a,b\\in \\{1; 2; 3\\}\\}, \\end{align*}\\] where \\(a=\\text {result in 1st throw}\\) and \\(b=\\text {result 2nd in throw}\\) Events: \\(A=\\)‚ÄúThe sum of both dice is 4‚Äù, \\(B=\\)‚ÄúThe numbers of both dice are odd‚Äù and \\(A\\cap B=\\)‚ÄúThe sum of both dice is 4 and the numbers of both dice are odd‚Äù. Probabilities: \\[\\begin{align} \\mathbb P(A) &amp;= \\frac{|A|}{|\\Omega|}= \\frac 13,\\\\ \\mathbb P(B) &amp;= \\frac{|B|}{|\\Omega|}=\\frac 49,\\\\ \\mathbb P(A\\cap B) &amp;= \\frac{|A\\cap B|}{|\\Omega|}=\\frac 29. \\end{align}\\] If \\(B\\) is now considered certain, the chance for \\(A\\) changes: \\[ \\mathbb P(A|B)=\\frac {\\mathbb P(A\\cap B)}{ \\mathbb P(B)} = \\frac {\\frac 29}{\\frac 49} = \\frac 12. \\] The chances for \\(A\\) under the additional information about \\(B\\) have now increased (\\(\\mathbb P(A|B)=\\frac 12\\)) compared to the situation before (\\(\\mathbb P(A)=\\frac 13\\)). Interactive switch to the section ‚ÄúProbability‚Äù, choose a condition for conditional probability, simulate dice rolls for different \\(n\\), compare the conditional probabilities with the corresponding conditional sample frequencies, What do you observe when \\(n\\) is increasing? 7.2 Independence Conditional probabilities express that information about the occurrence of an event \\(B\\) can influence the assessment of the probability of an event \\(A\\). If the information about the occurrence of this event has no effect on the probability of \\(A\\), then these events are stochastically independent: \\[\\begin{equation*} \\mathbb P(A|B) = \\mathbb P(A). \\end{equation*}\\] Criteria for independence In the case of stochastic independence: \\(\\displaystyle \\mathbb P(A|B) = \\frac{\\mathbb P(A\\cap B)}{\\mathbb P(B)} =\\mathbb P(A)\\), so that \\[\\begin{equation*} \\mathbb P(A\\cap B) = \\mathbb P(A)\\cdot \\mathbb P(B), \\end{equation*}\\] where \\(\\mathbb P(B)&gt;0\\) is assumed. The events \\(A\\), \\(B\\) and \\(C\\) are independent if \\[\\begin{align*} \\mathbb P(A\\cap B) &amp;= \\mathbb P(A)\\cdot \\mathbb P(B),\\\\ \\mathbb P(A\\cap C) &amp;= \\mathbb P(A)\\cdot \\mathbb P(C),\\\\ \\mathbb P(B\\cap C) &amp;= \\mathbb P(B)\\cdot \\mathbb P(C),\\\\ \\mathbb P(A\\cap B\\cap C) &amp;= \\mathbb P(A)\\cdot \\mathbb P(B)\\cdot \\mathbb P(C). \\end{align*}\\] For independence of three events all listed criteria must be fulfilled. Example 7.2 (Rolling a die twice) Cont. of Example 7.1. In the experiment: rolling a fair dice twice with the numbers 1, 2 and 3 on two sides of the dice, the defined events \\(A\\) and \\(B\\) are dependent, since: \\[\\begin{align} \\mathbb P(A\\cap B) &amp;=\\frac 29\\not = \\frac 13\\cdot\\frac 49. \\end{align}\\] So, \\(A\\) and \\(B\\) are dependent. The same can be deduced from \\(\\mathbb P(A|B)=\\frac 12\\not=\\frac 13=\\mathbb P(A)\\). Exercise 7.1 (Conditional probabilities and independence) 7.3 Additional information for sampling When analyzing customer characteristics, it is also often necessary to model conditional variables. For example, if it is known that a randomly selected customer uses the app, how many orders per month could be expected from this customer? Example 7.3 (Orders and Usage) Wir nehmen an, dass wir die gemeinsame Wahrscheinlichkeiten bereits kennen. We assume that we already know the common probabilities. What is the distribution of the number of orders among app users? Result set: \\[\\begin{align} \\Omega &amp;= \\{(App; 1), (App; 2),\\ldots, (App; 6),\\ldots, \\\\ &amp;~~~~(Web;1),\\ldots,(Web; 6) \\}\\\\ \\\\Omega|&amp;=12. \\end{align}\\] Event space: \\(\\mathcal F=\\mathcal P(\\Omega).\\) Probability measure: (assumption that we know the joint probabilities) \\[\\begin{equation} \\begin{array}{c|cccccc} \\mathbb P((a_i,b_i))&amp;b_1=1&amp;b_2=2&amp;b_3=3&amp;b_4=4&amp;b_5=5&amp;b_6=6\\\\\\hline a_1=App&amp;0,02&amp;0,06&amp;0,12&amp;0,2&amp;0,12&amp;0,08\\\\ a_2=Web&amp;0,08&amp;0,09&amp;0,13&amp;0,05&amp;0,03&amp;0,02\\\\ \\end{array} \\tag{7.1} \\end{equation}\\] Events: \\(A_1 =\\) Use of the app: \\[\\begin{align}\\mathbb P(A_1)&amp;= 0.6.\\end{align}\\] \\(B_{i}=\\) Number of orders \\(=i\\) \\(\\leadsto\\) \\[\\begin{align}\\mathbb P(B_{i})&amp;=\\mathbb P((App;i),(Web;i)):\\end{align}\\] \\[ \\begin{array}{c|cccccc} i&amp;1&amp;2&amp;3&amp;4&amp;5&amp;6\\\\\\hline \\mathbb P(B_i) &amp;0,1&amp;0,15&amp;0,25&amp;0,25&amp;0,15&amp;0,1 \\end{array} \\] \\(A_1\\cap B_{i}=\\) \\(i\\) Orders via app \\(\\leadsto\\) \\(A_1\\cap B_{i}=\\{(App;i),(App;i),(App;i),(App;i)\\}\\) \\[\\begin{align}\\mathbb P(A_1\\cap B_{i})&amp;=\\mathbb P((App;i)).\\end{align}\\] \\(\\leadsto\\) first line in (7.1). Conditional distribution of \\(B_i\\) given \\(A_1\\) is (\\(\\mathbb P(B_i|A_1 = \\frac{\\mathbb P(A_1\\cap B_{i})}{\\mathbb P(A_1)}\\)): \\[ \\begin{array}{c|cccccc} i&amp;1&amp;2&amp;3&amp;4&amp;5&amp;6\\\\\\hline \\mathbb P(B_i|A_1) &amp;0,0333&amp;0,1&amp;0,2&amp;0,3333&amp;0,2&amp;0,1333 \\end{array} \\] So how many orders do we expect app users to place on average? \\[ 1\\cdot 0,0333 + 2\\cdot 0,1 + 3\\cdot 0,2 + 4\\cdot 0,3333 + 5\\cdot 0,2 + 6\\cdot 0,1333 = 3,9663 \\] ..and from the web users? Exercise 7.2 (Orders) 7.4 Total probability and Bayes‚Äô formula As already indicated, there are situations where some conditional probabilities are known, but one would need the total (unconditional) probabilities or the ‚Äúreverse‚Äù conditional probabilities. We illustrate the whole thing with an example of product recommendations. Example 7.4 (Product recommendations) Cont. of Example 7.1. A (purchased) model for product recommendations usually has quality specifications that quantify how well the model sorts customers into the appropriate product recommendation groups. Since these models are trained on the data sets of customers about whom it is already known which product they have bought, there is a measure of the quality of the separation after the model has been created, such as the ‚Äútrue-positive‚Äù probabilities that the customers who later bought product \\(x\\) will also be sorted into the product recommendation group \\(x\\). Or the ‚Äúfalse positive‚Äù probabilities that the product recommendation will be missed. Let‚Äôs assume that there are three product groups (\\(G = \\{G_1;G_2,G_3\\}\\)) for which the model issues recommendations for each customer (\\(E=\\{E_1,E_2,E_3\\}\\)). The basis for the model formation are the groups \\(G = \\{G_1;G_2,G_3\\}\\) of customers who have purchased the respective product. The algorithm is to derive a ‚Äúformula‚Äù from various customer characteristics to determine which product the customer will choose in the near future based on the characteristics. The model is then tested and it is determined how high the probability is that the model will make the right or wrong recommendation. Ideally, all customers from \\(G_1\\) get the recommendation \\(E_1\\) and so on. How well the model (without having the information to which group the customer belongs) assigns the corresponding recommendation can be determined for each product group by calculating the conditional probability: \\[\\begin{align} \\mathbb P(E|G)&amp;=\\begin{pmatrix} &amp;G_1&amp;G_2&amp;G_3\\\\ E_1&amp;0,8&amp;0,2&amp;0,05\\\\ E_2&amp;0,15&amp;0,6&amp;0,1\\\\ E_3&amp;0,05&amp;0,2&amp;0,85 \\end{pmatrix} \\end{align}\\] For each group, you can then see quite clearly how much of the classification is correct (e.g.¬†\\(G_1\\leadsto\\) \\(\\mathbb P(E_1|G_1)=0.8\\)) or incorrect (e.g.¬†\\(G_2\\leadsto\\) \\(\\mathbb P(E_1|G_2) + P(E_3|G_2)=0.4\\)). The important questions here are: How likely is it that recommendations for the respective product will be offered at all? (In what proportions will they spend the recommendations on the products? KPIs, tussles between the product teams, ‚Ä¶) What will be the convergence rates per product? (how many of the customers who are offered a product recommendation actually buy the product? We want high convergence rates). The first question could be answered by specifying \\(\\mathbb P(E_i)\\), the second - by specifying \\(\\mathbb P(G_i|E_i)\\). But how do we calculate these probabilities if we only have the conditional probabilities of the form \\(\\mathbb P(E_i|G_i)\\)? (and perhaps still have an idea of the customer shares, buy products \\(1,2,3\\) \\(\\leadsto\\) \\(\\mathbb P(G_i)\\)). The answer is provided by the formula of total probability and the Bayes formula. 7.4.1 Total probability We investigate how conditional probabilities can be used to determine unconditional probabilities of an event. The sets \\(A_1, \\ldots, A_n\\) form a decomposition (partition) of \\(\\Omega\\) if \\(A_i\\cap A_j=\\emptyset\\) for \\(i\\not=j\\), i.e.¬†the sets \\(A_1, \\ldots, A_n\\) are pairwise disjoint, \\(A_1\\cup\\cdots\\cup A_n=\\Omega\\). For a decomposition \\(A_1,\\ldots, A_n\\) of \\(\\Omega\\), the formula of total probability applies to \\(B\\in\\mathcal F\\): \\[\\begin{equation*} \\mathbb P(B) = \\sum_{i=1}^n \\mathbb P(B|A_i)\\, \\mathbb P(A_i). \\end{equation*}\\] Example 7.5 (Product recommendations) Cont. of Example 7.4. Assume that the proportion of customers who buy the products is as follows: \\[ \\begin{array}{c|ccc} i&amp;1&amp;2&amp;3\\\\\\hline \\mathbb P(G_i)&amp;0.5&amp;0.3&amp;0.2 \\end{array} \\] Since our company only recommends the three products, \\(G_1,G_2\\) and \\(G_3\\) form a decomposition of the entire product recommendation system \\(\\Omega\\). We can now answer the first question ‚ÄúHow likely is it that recommendations for the respective product will be offered at all? (In what proportions will they issue the recommendations for the products?)‚Äù using the total probability. \\[\\begin{align} \\mathbb P(E_1)&amp;=\\mathbb P(E_1|G_1)\\cdot \\mathbb P(G_1) + \\mathbb P(E_1|G_2)\\cdot \\mathbb P(G_2) + \\mathbb P(E_1|G_3)\\cdot \\mathbb P(G_3) \\\\ &amp;=0,8\\cdot 0.5 + 0,2\\cdot 0.3+ 0,05\\cdot 0.2 \\\\ &amp;=0.47 \\end{align}\\] Can you also calculate \\(\\mathbb P(E_2)\\) and \\(\\mathbb P(E_3)\\)? Exercise 7.3 (totale Wahrscheinlichkeit: Product recommendations) 7.4.2 Bayes formula The Bayes formula can be used to reverse the conclusion. Example: In medicine, we are interested in the probability of a disease in the event that a medical test is positive. Let \\(A\\) and \\(B\\) be two events with \\(\\mathbb P(B)&gt;0\\). Then the following applies: \\[\\begin{equation*} \\mathbb P(A|B) = \\frac{\\mathbb P(B|A)\\cdot \\mathbb P(A)}{\\mathbb P(B)}. \\end{equation*}\\] Let \\(A_1,\\ldots, A_n\\) be a decomposition of \\(\\Omega\\) with \\(\\mathbb P(A_i)&gt;0\\) and \\(\\mathbb P(B|A_i)&gt;0\\) for at least one \\(i\\). Then applies \\[\\begin{equation*} \\mathbb P(A_k|B) = \\frac{\\mathbb P(B|A_k)\\, \\mathbb P(A_k)} {\\sum_{i=1}^n \\mathbb P(B|A_i)\\, \\mathbb P(A_i)} = \\frac{\\mathbb P(B|A_k)\\, \\mathbb P(A_k)} {\\mathbb P(B)}\\quad \\text{ for } k=1,\\ldots, n. \\end{equation*}\\] Example 7.6 (Product recommendations) Cont. of Example 7.5. Now we can also answer the question about the expected convergence rates: According to the Bayes formula: \\[\\begin{align} \\mathbb P(G_1|E_1) &amp;= \\frac{\\mathbb P(E_1|G_1)\\cdot \\mathbb P(G_1)}{\\mathbb P(E_1)}\\\\ &amp;=\\frac{0,8\\cdot0.5}{0.47} = 0.8511. \\end{align}\\] Can you calculate the convergence rates for \\(G_2\\) and \\(G_3\\)? Exercise 7.4 (Bayes formula for product recommendations) 7.4.3 Product recommendations according to the ‚ÄòNaive Bayes‚Äô method If you want to develop a model for the product recommendations yourself, the conditional probabilities and the formulas from this topic will help you again. Example 7.7 (Product recommendations) Let us also assume that there are three product groups (\\(G = \\{G_1;G_2,G_3\\}\\)) for which the model is to issue recommendations for each customer (\\(E=\\{E_1,E_2,E_3\\}\\)). The model is based on the groups \\(G = \\{G_1;G_2,G_3\\}\\) of customers who have purchased the respective product. We are now to derive a ‚Äúformula‚Äù from a wide variety of customer characteristics to determine which product the customer ‚Äútends‚Äù to buy based on the characteristic attribute values. We only consider two characteristics: Customer age (\\(X_1\\)) divided into \\(\\leq 25\\) and \\(&gt; 25\\). The last calculation in euros (\\(X_2\\)) divided into \\(\\leq 50\\) and \\(&gt; 50\\). We can calculate the probabilities of the above values for each group. For example, we have calculated the following proportions: In \\(G_1\\): \\[ \\begin{array}{c|cc} \\mathbb P(X_1,X_2|G_1) &amp;\\leq 50&amp;&gt;50\\\\\\hline \\leq 25 &amp;0.1&amp;0.4\\\\ &gt; 25 &amp;0.2&amp;0.3\\\\ \\end{array} \\] In \\(G_2\\): \\[ \\begin{array}{c|cc} \\mathbb P(X_1,X_2|G_2) &amp;\\leq 50&amp;&gt;50\\\\\\hline \\leq 25 &amp;0.2&amp;0.1\\\\ &gt; 25 &amp;0.4&amp;0.3\\\\ \\end{array} \\] In \\(G_3\\): \\[ \\begin{array}{c|cc} \\mathbb P(X_1,X_2|G_3) &amp;\\leq 50&amp;&gt;50\\\\\\hline \\leq 25 &amp;0.6&amp;0.1\\\\ &gt; 25 &amp;0.1&amp;0.2\\\\ \\end{array} \\] But what we really need are the ‚Äúinverse‚Äù conditional probabilities (probability for product category given the data constellations). So, given that \\[ \\begin{array}{c|ccc} i&amp;1&amp;2&amp;3\\\\\\hline \\mathbb P(G_i)&amp;0.5&amp;0.3&amp;0.2 \\end{array} \\] and with the help of the total probability of choice: \\[\\begin{align} \\mathbb P(X_1\\leq 25 \\text{ und } X_2\\leq 50) &amp;=0.1\\cdot 0.5 + 0.2\\cdot 0.3 + 0.6\\cdot 0.2= 0.23.\\\\ \\mathbb P(X_1\\leq 25 \\text{ und } X_2&gt; 50) &amp;= \\ldots \\text{analogeous}\\ldots = 0.25\\\\ \\mathbb P(X_1&gt; 25 \\text{ und } X_2\\leq 50) &amp;= \\ldots \\text{analogeous}\\ldots = 0.24\\\\ \\mathbb P(X_1&gt; 25 \\text{ und } X_2&gt; 50) &amp;= \\ldots \\text{analogeous}\\ldots = 0.28\\\\ \\end{align}\\] And with the help of the Bayes formula: for the constellation Nr. 1 \\(K_1 = X_1\\leq 25 \\text{ und } X_2\\leq 50\\): \\[\\begin{align} \\mathbb P(G_1|K_1) &amp;=\\frac{0.1\\cdot 0.5}{0.23} = 0.2174\\\\ \\mathbb P(G_2|K_1) &amp;= \\ldots \\text{analogeous}\\ldots =0.2609\\\\ \\mathbb P(G_3|K_1) &amp;= \\ldots \\text{analogeous}\\ldots =0.5217 \\end{align}\\] "],["random-variables.html", "Chapter 8 Random variables 8.1 Discrete random variable 8.2 Continuous random variables 8.3 Some important distributions", " Chapter 8 Random variables Nice to be able to calculate probabilities for various events üòÑ! As business analysts, we mostly work with numbers. This means that we usually transfer the results of a random process to numerical values. For example, we summarize a purchase decision (the interaction of unknown customer preferences and complex consequences of customer interactions with their environment) into a few numerical values, such as the number of products purchased, orders placed or the average invoice amount. We are primarily interested in the resulting numbers, while we can‚Äôt or don‚Äôt want to model the customer‚Äôs complex decision-making process. However, these numbers that we are looking at are not fixed but vary from customer to customer, month to month, etc. If we had to indicate future sales today, we would hardly be able to give a concrete value. We know approximately what values the sales volume can attain, but these are associated with uncertainty and therefore depend on complex events (random processes) that are partly controlled by chance. Probabilistically speaking, this future sales volume is a random variable. In this chapter we will learn: How to define the random variables and what kind of examples exist (\\(\\leadsto\\) discrete vs.¬†continuous random variables), how to derive distribution of random variables, how to calculate the key parameters of this distribution, as the expected value and the variance, but also the covariance and the quantiles, and what typical distributions for random variables exist and how to use them. A random variable \\(X\\) is a variable whose values are derived from the result of a random operation. A number \\(x\\in \\mathbb R\\), which the random variable \\(X\\) assumes based on an outcome of an associated random process, is called the realization or the value of \\(X\\). Examples: Sum of two dice, Stock return in the next month, Number of app users in a survey. We differentiate between two types: discrete and continuous random variables, depending on what values they can assume. The distribution of a random variable \\(X\\) indicates which values in \\(\\mathbb R\\) are assumed with which probabilities. This distribution is derived from the original random process. Since random variables have numbers as values, we consider the following events: \\[\\begin{equation*} \\{X=x\\},\\quad \\{X\\not=x\\}, \\quad \\{X\\leq x\\},\\quad \\{X&gt;x\\}, \\quad \\{a\\leq X\\leq b\\}. \\end{equation*}\\] These events are assigned probabilities by the distribution of the random variable. 8.1 Discrete random variable A random variable \\(X\\) is called discrete if it assumes a finite or countably infinite number of values \\(x_1,x_2,\\ldots, x_n, \\ldots\\). Such as dice numbers, number of orders, number of customers per hour, indicator of a loan default or of a product purchase. The probability distribution of \\(X\\) is given by the probabilities \\[\\begin{equation*} \\mathbb P(X=x_i)=p_i, \\quad i=1,2,\\ldots. \\end{equation*}\\] We denote the set of values of \\(X\\) with \\(\\mathcal T=\\{x_1,x_2,\\ldots\\}\\). For the probabilities \\(p_1,p_2,\\ldots\\) applies (according to the axioms): \\[\\begin{gather*} \\phantom{\\quad i=1,2,\\ldots} 0\\leq p_i\\leq 1,\\quad i=1,2,\\ldots,\\\\ p_1+p_2+\\cdots = \\sum_{i\\geq 1} p_i=1. \\end{gather*}\\] The probability \\(\\mathbb P(X\\in A)\\) for \\(A\\subseteq \\mathcal T\\) is given as \\[\\displaystyle \\mathbb P(X\\in A) = \\sum_{i: x_i\\in A} p_i.\\] Example 8.1 (Win when rolling two dice) Example 6.4 cont. In the experiment ‚Äúsum of two fair dice with faces 1, 2 and 3‚Äù we found: \\(\\Omega = \\{2;3;4;5;6\\}\\) \\(\\omega\\in\\Omega,\\) \\(\\omega=\\) the sum of the dice, \\(\\mathcal F=\\mathcal P(\\Omega)\\), \\(\\mathbb P\\): \\[ \\begin{array}{c|ccccc}\\omega_i&amp;2&amp;3&amp;4&amp;5&amp;6\\\\\\hline \\mathbb P(\\{\\omega_i\\})&amp;\\frac 19&amp;\\frac 29&amp;\\frac 39&amp;\\frac 29&amp;\\frac 19 \\end{array} \\] Based on this, we now define a random variable \\(X=\\) The random win (in euros): \\[ X=\\begin{cases} 2\\cdot \\omega, &amp;\\text{ if } \\omega \\text{ odd},\\\\ - 1, &amp;\\text{ if } \\omega \\text{ even},\\\\ \\end{cases} \\] What are the values of \\(X\\) and what are their probabilities? Well, for each result \\(\\omega\\) we can determine the value of \\(X\\), specifically: \\[ \\begin{array}{c|ccccc}\\omega_i&amp;2&amp;3&amp;4&amp;5&amp;6\\\\\\hline x_i&amp;-1&amp;6&amp;-1&amp;10&amp;-1\\\\ \\mathbb P(\\{\\omega_i\\})&amp;\\frac 19&amp;\\frac 29&amp;\\frac 39&amp;\\frac 29&amp;\\frac 19 \\end{array} \\] The probabilities of the \\(X\\) values correspond to the probabilities of the respective outcomes. Only, some values in the above list are doubled. Therefore, to create a distribution of \\(X\\), we write down each value only once and sum the probabilities of the outcomes that give us that value to determine the final probability of that \\(X\\)-value: \\[ \\begin{array}{c|ccc} x_i&amp;-1&amp;6&amp;10\\\\\\hline \\mathbb P(x_i)=p_i&amp;\\frac 59&amp;\\frac 29&amp;\\frac 29 \\end{array} \\] This is the distribution of \\(X\\). Probability that \\(X\\) is positive is calculated as: \\[\\begin{equation*} \\mathbb P(X&gt;0)=\\mathbb P(X\\in \\{6;10\\}) = \\mathbb P(X=6) + \\mathbb P(X=10) = \\frac{4}{9 }. \\end{equation*}\\] 8.1.1 Probability and distribution functions Probability function If you specify the distribution of \\(X\\) like in Example 8.1, then you only have probabilities for numbers that are in the value set \\(\\mathcal T\\). That is, other real numbers \\(x\\not\\in \\mathcal T\\) have the probability of zero, i.e.¬†\\(\\mathbb P(X=x)=0\\) for \\(x\\not \\in \\mathcal T\\). The probability function assigns a value to every \\(x\\in \\mathbb R\\). The probability function \\(f(x)\\) of a discrete random variable \\(X\\) for \\(x\\in \\mathbb R\\) is defined by \\[\\begin{equation*} f(x) = \\begin{cases} \\mathbb P(X=x_i)=p_i, &amp;\\quad x=x_i\\in \\{x_1,x_2,\\ldots, \\},\\\\ 0,&amp;\\quad\\text{else}. \\end{cases} \\end{equation*}\\] Properties of the probability function: \\(f(x)\\geq 0\\), \\(x\\in \\mathbb R\\), \\(\\sum_{i\\geq 1} f(x_i)=1\\) Distribution function The (cumulative) distribution function (cdf) of a discrete random variable is given as \\[\\begin{equation*} F(x) = \\mathbb P(X\\leq x) = \\sum_{i: x_i\\leq x} f(x_i). \\end{equation*}\\] Properties of the distribution function of a discrete random variable: \\(F\\) is a right-sided continuous step function, \\(x\\mapsto F(x)\\) is monotonically increasing, \\(0\\leq F(x)\\leq 1\\), for all \\(x\\in \\mathbb R,\\) \\(\\lim_{x\\rightarrow-\\infty} F(x)=0\\) and \\(\\lim_{x\\rightarrow\\infty} F(x)=1\\). Example 8.2 (Win when rolling two dice) Example 8.1 cont. Probability function: \\[\\begin{align} f(x)&amp;=\\begin{cases}\\frac 59, \\text{if }x=-1,\\\\ \\frac 29, \\text{if }x\\in\\{6;10\\},\\\\ ~0, \\text{else.} \\end{cases} \\end{align}\\] Figure 8.1: Probability function for X Distribution function: \\[\\begin{align} F(x)=\\begin{cases} ~0, \\text{if } x&lt; -1,\\\\ \\frac 59, \\text{if }-1\\leq x&lt;6,\\\\ \\frac 79, \\text{if }~~6\\leq x&lt; 10,\\\\ ~1, \\text{if }~~x\\geq 10. \\end{cases} \\end{align}\\] Calculation example: if \\(6\\leq x&lt; 10\\) then \\[ F(x)=\\mathbb P(X\\leq x) = \\mathbb P(X=-1) + \\mathbb P(X=6) =\\frac 59+\\frac 29 = \\frac 79. \\] Figure 8.2: Distribution function for X Submit Submit Submit Submit Example 8.3 (Costs for customer orders) Example 6.9 cont. In the example with orders via app or web, we had the following common probabilities: \\[ \\begin{array}{c|cccccc} \\mathbb P((a_i,b_i))&amp;b_1=1&amp;b_2=2&amp;b_3=3&amp;b_4=4&amp;b_5=5&amp;b_6=6\\\\\\hline a_1=App&amp;0.02&amp;0.06&amp;0.12&amp;0.2&amp;0.12&amp;0.08 \\\\ a_2=Web&amp;0.08&amp;0.09&amp;0.13&amp;0.05&amp;0.03&amp;0.02 \\\\ \\end{array} \\] We also found that app users place more orders on average. Assume you process the orders placed in the online shop through a service partner. When customers places their orders in the shop, you have to pay a certain fee to the service partner. Furthermore, the fee depends on the number of orders and the interface: app or web. We define the costs that result from all ordering processes on a user account as a random variable \\(K\\). \\(K\\) is calculated as: \\[ K = \\begin{cases} 1.5\\cdot \\text{No. Orders}, &amp;\\text{ if via app and number. Orders }\\leq 4,\\\\ 1\\cdot \\text{No. Orders} + 1, &amp;\\text{ if via web and number. Orders }\\leq 4,\\\\ 6, &amp;\\text{ if via app and number. Orders }&gt;4,\\\\ 5, &amp;\\text{ if via web and display. Orders }&gt;4. \\end{cases} \\] How are the costs distributed? Values of \\(K\\): \\[ \\begin{array}{c|cccccc} (App,b_i)&amp;(App,1)&amp;(App,2)&amp;(App,3)&amp;(App,4)&amp;(App,5)&amp;(App,6)\\\\\\hline k_{1,i}&amp;1.5&amp;3&amp;4.5&amp;6&amp;6&amp;6\\\\ \\mathbb P(App,b_i)&amp;0.02&amp;0.06&amp;0.12&amp;0.2&amp;0.12&amp;0.08\\\\ \\end{array} \\] \\[ \\begin{array}{c|cccccc} (Web,b_i)&amp;(Web,1)&amp;(Web,2)&amp;(Web,3)&amp;(Web,4)&amp;(Web,5)&amp;(Web,6)\\\\\\hline k_{2,i}&amp;2&amp;3&amp;4&amp;5&amp;5&amp;5\\\\ \\mathbb P(Web,b_i)&amp;0.08&amp;0.09&amp;0.13&amp;0.05&amp;0.03&amp;0.02\\\\ \\end{array} \\] Distribution of \\(K\\): \\[ \\begin{array}{c|cccccccc} k_i&amp;1.5&amp;2&amp;3&amp;4&amp;4.5&amp;5&amp;6\\\\\\hline p_i&amp;0.02&amp;0.08&amp;0.15&amp;0.13&amp;0.12&amp;0.1&amp;0.4 \\end{array} \\] 8.1.2 Expected value and variance As in the descriptive statistics, it makes sense to define some meaningful parameters, that essentially describe the distribution of a random variable. For both types of random variables, we define the mean value \\(\\leadsto\\) expected value, a measure for the spread \\(\\leadsto\\) variance and standard deviation. For two discrete random variables, we also define covariance and correlation. Expected value and variance are the most important metrics, as many families of distributions (more on this later) are clearly defined by these two metrics. The two key metrics give us information about the mean position and variability of the values of random variables. Expected value Definition 8.1 (Expected value/ Expectation) The expected value \\(\\mathbb E(X)\\) of a discrete random variable \\(X\\) with value set \\(\\mathcal T=\\{x_1,x_2,\\ldots\\}\\) and probability function \\(f(x)\\) is: \\[\\begin{equation*} \\mathbb E(X) = x_1 f(x_1) + x_2 f(x_2) + \\cdots = \\sum_{i\\geq 1} x_i f(x_i). \\end{equation*}\\] Properties of the expected value \\(\\mathbb E(a X + b) = a \\mathbb E (X) + b\\) (linear transformation) \\(\\mathbb E(X+Y) = \\mathbb E(X) + \\mathbb E(Y)\\) (expected value of a sum = sum of expected values) Variance and standard deviation Definition 8.2 (Variance and standard Deviation) The variance of a random variable \\(X\\) is \\[\\begin{equation*} \\sigma^2 = \\text{Var}(X) = \\mathbb E((X-\\mathbb E(X))^2)\\\\ =\\sum_{i\\geq 1} (x_i-\\mathbb E(X))^2 f(x_i), \\end{equation*}\\] The standard deviation is \\(\\sigma = \\sqrt{\\text{Var}(X)}.\\) Best suited for calculating variance: \\[\\begin{equation*} \\text{Var}(X)=\\mathbb E(X^2) - (\\mathbb E(X))^2. \\end{equation*}\\] For discrete random variables this means: \\[\\begin{align} \\text{Var}(X)&amp;=\\sum_{i\\geq1} x_i^2f(x_i)- (\\mathbb E(X))^2. \\end{align}\\] Properties of Variance \\(\\text{Var}(aX+b)= a^2\\text{Var}(X)\\) (linear transformation) \\(\\text{Var}(X+Y) = \\text{Var}(X) + \\text{Var}(Y)\\) if \\(X\\) and \\(Y\\) are stochastically independent. This results in the calculation rules for linear transformations (\\(Y=a X + b\\)): \\[\\begin{align} \\mathbb E(Y) &amp;= \\mathbb E (a X + b)=a \\mathbb E(X) + b,\\\\ \\text{Var}(Y)&amp;=\\text{Var}(a X + b) = a^2\\text{Var}(X). \\end{align}\\] Example 8.4 (Win when rolling two dice) Cont. of 8.1. Example 8.5 (Costs for customer orders) Example 8.3 cont. In this example, we derived the following probability distribution for the cost \\(K\\): \\[ \\begin{array}{c|ccccccc} k_i&amp;1.5&amp;2&amp;3&amp;4&amp;4.5&amp;5&amp;6\\\\\\hline p_i&amp;0.02&amp;0.08&amp;0.15&amp;0.13&amp;0.12&amp;0.1&amp;0.4 \\end{array} \\] That is, \\(\\mathbb E(K) = \\sum_{i=1}^7 k_i\\cdot p_i = 1.5\\cdot 0.02 + \\ldots + 6\\cdot 0.4 = 4.6.\\) \\(\\text{Var}(K) = \\sum_{i=1}^7 k^2_i\\cdot p_i - \\mathbb E(K)^2= 1.5^2\\cdot 0.02 + \\ldots + 6^2\\cdot 0.4 -4,6^2 = 1.965.\\) Because of rising inflation the service cost is revised. The new cost includes a \\(20\\%\\) increase (an increase by the factor \\({\\color{red}{1.2}}\\)) and an additional fee of \\(\\color{blue}1\\) euro compared to the old costs, such that \\[K_{new} = {\\color{red}{1.2}}\\cdot K+{\\color{blue}{1}}.\\] The new parameters are: \\(\\mathbb E(K_{new}) = \\mathbb E ({\\color{red}{1.2}}\\cdot K+{\\color{blue}{1}})={\\color{red}{1.2}}\\cdot \\mathbb E(K)+{\\color{blue}{1}} = 1.2\\cdot 4.6+1=6.52.\\) \\(\\text{Var}(K_{new}) = \\text{Var}({\\color{red}{1.2}}\\cdot K+{\\color{blue}{1}})= ({\\color{red}{1.2}})^2\\cdot \\text{Var}(K) = 1.44\\cdot 1.965=2.8296.\\) Submit Submit Submit Submit 8.1.3 Covariance and correlation Whenever we are dealing with several random variables as in example 6.9, the question of the association between the variables arises. If we can find a positive association pattern between the number of orders and the usage of app, then we could possibly boost the sales by promoting the mobile app usage. A measure of linear dependence between two random variables is their covariance and their associated correlation. Definition 8.3 (Covariance and Correlation) The covariance of two random variables \\(X\\) and \\(Y\\) is determined by \\[\\begin{equation*} \\sigma_{XY}=\\text{Cov}(X,Y) =\\mathbb E[(X-\\mathbb E(X))\\, (Y-\\mathbb E(Y))] = \\mathbb E[X\\cdot Y] - \\mathbb E(X)\\, \\mathbb E(Y). \\end{equation*}\\] Specifically for discrete random variables: \\[ \\text{Cov}(X,Y) =\\sum_{i,j}x_i\\cdot y_j\\cdot f(x_i,y_j) - \\left(\\sum_ix_i\\cdot f(x_i)\\right)\\cdot\\left( \\sum_jy_j\\cdot f(y_j)\\right). \\] The correlation coefficient is determined by \\[\\begin{equation*} \\rho_{XY}=\\frac{\\sigma_{XY}}{\\sigma_X\\, \\sigma_Y} = \\frac{\\text{Cov}(X,Y)} {\\sqrt{\\text{Var}(X)}\\, \\sqrt{\\text{Var}(Y)}}. \\end{equation*}\\] \\(X\\) and \\(Y\\) are called uncorrelated if \\(\\text{Cov}(X,Y)=0\\). \\(-1\\leq \\rho_{XY}\\leq 1\\) If \\(X, Y\\) are independent, then they are also uncorrelated. Example 8.6 (Orders and Usage) In the example 6.9 we looked at the joint distribution of the app usage and orders. Now we define two random variables: \\(A=\\begin{cases}1, &amp;\\text{ falls } App\\\\0, &amp;\\text{ falls } Web\\end{cases}\\) (an indicator of app usage). \\(B=b_i\\), \\(b_i\\in\\{1; 2; \\ldots 6\\}\\) the number of orders. The joint probabilities are: \\[ \\begin{array}{c|cccccc} \\mathbb P((a_i,b_i))&amp;b_1=1&amp;b_2=2&amp;b_3=3&amp;b_4=4&amp;b_5=5&amp;b_6=6\\\\\\hline a_1=1&amp;0.02&amp;0.06&amp;0.12&amp;0.2&amp;0.12&amp;0.08\\\\ a_2=0&amp;0.08&amp;0.09&amp;0.13&amp;0.05&amp;0.03&amp;0.02\\\\ \\end{array} \\] Are the two quantities correlated? Expected values \\[\\begin{align} \\mathbb E(A)&amp;=1\\cdot0,6 + 0\\cdot0,4 = 0,6,\\\\ \\mathbb E(B)&amp;=1\\cdot0,1 + 2\\cdot0,15 +\\ldots + 6\\cdot0,1=3,5. \\end{align}\\] Variances: \\[\\begin{align} \\text{Var}(A)&amp;=1^2\\cdot0,6 + 0^2\\cdot0,4 - 0,6^2 = 0,24,\\\\ \\text{Var}(B)&amp;=1^2\\cdot0,1 + 2^2\\cdot0,15 +\\ldots + 6^2\\cdot0,1 - 3,5^2=2,05. \\end{align}\\] Covariance: \\[\\begin{align} \\text{Cov}(A,B)&amp;=\\mathbb E(A\\cdot B) - \\mathbb E(A)\\, \\mathbb E(B)\\\\ &amp;=1\\cdot 1\\cdot0.02 + 1\\cdot 2\\cdot0.06 + 1\\cdot 3\\cdot0.12 + 1\\cdot 4\\cdot0.2 \\\\ &amp;+ 1\\cdot 5\\cdot0.12 + 1\\cdot 6\\cdot0.08 \\text{ (for A=1 and B=1;2;$\\ldots$ 6)} + \\\\ &amp;+ 0\\cdot 1\\cdot0.08 + \\dots + 0\\cdot 6\\cdot0.02\\text{ (for A=0 and B=1;2;$\\ldots$ 6)} - \\\\ &amp;- 0,6\\cdot3,5 = 0,28. \\end{align}\\] Correlation: \\[\\begin{align} \\rho_{A,B}&amp;=\\frac{\\text{Cov}(A,B)}{\\sqrt{\\text{Var}(A)}\\cdot\\sqrt{\\text{Var}(B)}}\\\\ &amp;= \\frac{0,28}{\\sqrt{0,24}\\cdot\\sqrt{2,05}} = 0,3992. \\end{align}\\] So the two random variables are positively correlated. Yuhu! We have a positive association between the app usage and the number of orders. It means, that by increasing the incentive for the app usage, we could increase our sales. 8.2 Continuous random variables We have seen how to model discrete random variables. However, there are certainly cases where the framework of discrete random variables is not sufficient. For example, when we look at sizes, lengths, volumes, but also when we examine sales, times in spend in our online shop, waiting times or asset returns. These variables take on many different values, which makes modeling difficult for us because we have to take all their probabilities into account ü§î. In addition, due to varying measurement precision, we usually cannot distinguish between the online shopping time of \\(12\\) minutes and \\(12,01\\) minutes. Then why not just look at a range, e.g.¬†\\((11,5, 12.5)\\) minutes directly? In this case, instead of looking at fixed values, it makes sense to look at entire regions/ intervals (\\(\\leadsto\\) continuous random variables) and at the density of these regions as the basis for the probability calculation, with the probabilities represented as areas under this density on the corresponding interval. This implies that the probability of hitting a very specific value (the point probability) becomes zero. A random variable \\(X\\) is said to be continuous if there is a function (density function) \\(x\\mapsto f(x)\\geq 0\\), so that for every interval \\([a,b]\\) we have: \\[\\begin{equation*} \\mathbb P(a\\leq X\\leq b)=\\int_a^b f(x) dx. \\end{equation*}\\] The set of values \\(\\mathcal T\\subseteq\\mathbb R\\) of a continuous random variable is uncountable. It corresponds either to the whole \\(\\mathbb R\\) or to the subsets of \\(\\mathbb R\\) (intervals). 8.2.1 Density function Density properties: \\(f(x)\\geq 0\\) for all \\(x\\in \\mathbb R\\) \\(\\int_{\\mathbb R} f(x)\\, dx=1\\) The point probability is zero: \\(\\mathbb P(X=x)=0\\) for all \\(x\\in \\mathbb R\\) if \\(X\\) represents a continuous random variable. Graphical representation: Figure 8.3: Example: Density function Probabilities are represented by areas under density \\(f(x)\\). Calculation of the probability that \\(X\\) assumes values in set \\(B\\) using the density function: \\[\\begin{equation*} \\mathbb P(X\\in B) = \\int_B f(x)\\, dx = \\int_a^b f(x)dx \\end{equation*}\\] 8.2.2 Distribution function The (cumulative) distribution function of a continuous Random variable is given as \\[\\begin{equation*} F(x) = \\mathbb P(X\\leq x) = \\int_{-\\infty}^x f(t)\\, dt. \\end{equation*}\\] Properties of the distribution function for continuous random variables: \\(F\\) is a continuous function \\(x\\mapsto F(x)\\) is monotonically increasing \\(0\\leq F(x)\\leq 1\\), for all \\(x\\in \\mathbb R\\) \\(\\lim_{x\\rightarrow-\\infty} F(x)=0\\) and \\(\\lim_{x\\rightarrow\\infty} F(x)=1\\). Graphical representation: Figure 8.4: Example: Probability function of a continuous random variable If the distribution function \\(F\\) is given, one can obtain the density \\(f\\) by deriving the former: \\[\\begin{equation*} f(x) = F^\\prime(x),\\quad\\text{ differentiable for $F$ in $x$} \\end{equation*}\\] Calculating the probability that \\(X\\) takes values in \\(B\\) using the distribution function: \\[\\begin{equation*} \\mathbb P(X\\in B) = \\mathbb P(a\\leq X \\leq b) = F(b) - F(a). \\end{equation*}\\] Example 8.7 (Time in the online shop as a continuous random variable) We model the random variable \\(X=\\) time in the online shop in minutes as a continuous random variable with the density: \\[ f(x) = \\begin{cases}0,001\\cdot x , &amp;\\text{ for } 0\\leq x\\leq 20,\\\\ 0,025 -0,00025\\cdot x, &amp;\\text{ for } 20&lt; x\\leq 100.\\end{cases} \\] That is, Figure 8.5: Probability density of X=Time in online shop We compute the event probabilities: \\(\\{X&lt;20\\}=\\text{&quot;Time in shop less than 20 minutes&quot;}\\): \\[\\begin{align} \\mathbb P(X&lt;20)&amp;= \\displaystyle \\int_0^{20} (0,001\\cdot x )dx = \\text{Area of the red triangle}\\\\ &amp;= \\frac{1}{2}\\cdot 20\\cdot 0,02 = 0,2. \\end{align}\\] \\(\\{X&lt;60\\}=\\text{&quot;Time in shop less than 60 minutes&quot;}\\): \\[\\begin{align} \\mathbb P(X&lt;60)&amp;= 1-\\mathbb P(X\\geq 60)=1-\\displaystyle \\int^{100}_{60} (0,025 -0,00025\\cdot x)dx \\\\ &amp;= 1- \\text{Area of the transparent triagle} = 1-\\frac{1}{2}\\cdot 40\\cdot 0,01 = 0,8. \\end{align}\\] Figure 8.6: Probability density of X=Time in online shop with probabilities as areas under the density fucntion. Example 8.7 (Time in the online shop as a continuous random variable II) Computing the cumulative probability function from the density function: \\[F(x)=\\int_{-\\infty}^{x}\\overbrace{f(t)}^{density~of~X}dt\\stackrel{x&gt;0}{=}\\int_{0}^{x}f(t)dt.\\] Since the density change form depending on whether \\(0&lt;x\\leq 20\\) or \\(20&lt;x \\leq100\\), we have to consider the following cases: case: \\(x\\in [{0},{20}]\\): \\[\\begin{align} F(x) &amp;= \\int_{{0}}^x f(t)~ dt = \\int_{0}^x (0.001\\cdot t )~ dt=\\left[0.001\\cdot \\frac{t^2}2 0\\right]_{0}^x \\\\ &amp; = 0.001\\cdot \\frac{x^2}2 - \\left(0.001\\cdot \\frac{0^2}2 \\right)\\\\ &amp;=0.0005x^2. \\end{align}\\] case: \\(x\\in ({20},{100}]\\): \\[\\begin{align} \\displaystyle F(x) &amp;= \\int_{0}^x f(t)~ dt\\\\ &amp;= \\int_{0}^{20} (0.001\\cdot t )~ dt + \\int_{20}^x (0.025 -0.00025\\cdot t)~ dt\\\\ &amp;=0.2 + \\left[0.025\\cdot t -0.00025\\cdot \\frac{t^2}2 \\right]_{20}^x\\\\ &amp;=0.2 + 0.025\\cdot x -0.00025\\cdot \\frac{x^2}2 - \\left(0.025\\cdot 20 -0.00025\\cdot \\frac{20^2}2\\right) \\\\ &amp;= -0.25 + 0.025x -0.000125x^2. \\end{align}\\] That is: \\[ F(x) = \\begin{cases} 0,&amp;x&lt;0,\\\\ 0,0005x^2,&amp;0\\leq x\\leq 20,\\\\ -0.25 + 0.025x -0.000125x^2,&amp;20&lt;x\\leq 100,\\\\ 1,&amp;x&gt;100. \\end{cases} \\] Figure 8.7: Cumulative distribution function of X=Time in online shop Using the cumulative distribution function, we can calculate the event probabilities: \\(\\{X&lt;20\\}=\\text{&quot;Time in shop less than 20 minutes&quot;}\\): \\[\\begin{align}\\mathbb P(X&lt;20)&amp;= \\mathbb P(X\\leq20)=F(20) = 0,0005\\cdot 20^2= 0.2.\\end{align}\\] \\(\\{X&lt;60\\}=\\text{&quot;Zeit im Shop weniger als 60 Minuten&quot;}\\):\\[\\begin{align}\\mathbb P(X&lt;60)&amp;=\\mathbb P(X\\leq60)=F(60) =-0.25 + 0.025\\cdot 60 -0.000125\\cdot 60^2= 0.8.\\end{align}\\] \\(\\{X &gt;30\\}=\\text{&quot;Time in shop less than 30 minutes&quot;}\\):\\[\\begin{align} \\mathbb P(X&gt;30)&amp;=1-\\mathbb P(X\\leq30)= 1-F(30) \\\\&amp;= 1-(-0.25 + 0.025\\cdot 30 -0.000125\\cdot 30^2)\\\\ &amp;= 0.6125.\\end{align}\\] Figure 8.8: Cumulative probability function of X=Time in online shop with probabilities \\(f(x)=\\begin{cases}2x,&amp;\\text{ if }0\\leq x \\leq 1,\\\\0,&amp;\\text{ if } x&lt;0,\\\\1,&amp;\\text{ if }x&gt;1.\\end{cases}\\) \\(f(x)=\\begin{cases}x^2,&amp;\\text{ if }0\\leq x \\leq 1,\\\\0,&amp;\\text{ if } x&lt;0,\\\\1,&amp;\\text{ if }x&gt;1.\\end{cases}\\) \\(f(x)=\\begin{cases}2x,&amp;\\text{ if }0\\leq x \\leq 1,\\\\0,&amp;\\text{ if } x&lt;0,x&gt;1.\\end{cases}\\) \\(f(x)=\\begin{cases}\\frac13 x^3,&amp;\\text{ if }0\\leq x \\leq 1,\\\\0,&amp;\\text{ if } x&lt;0,\\\\1,&amp;\\text{ if }x&gt;1.\\end{cases}\\) Submit Submit Submit Submit Submit Hint: use conditional probability and note that \\(\\mathbb P((0.1 \\leq X \\leq 0.3)\\cap (X\\leq 0.5))=\\mathbb P(0.1 \\leq X \\leq 0.3)\\) 8.2.3 Expected value and variance Expected value and variance are the most important metrics, as many families of distributions (more on this later) are clearly defined by these two metrics. The two measures give us information about the mean position and variability of the values of random variables. Expected value Definition 8.4 (Expected Value/ Expectation (continuous rv)) The expected value \\(\\mathbb E(X)\\) of a continuous random variable \\(X\\) with density \\(f(x)\\) is \\[\\begin{equation*} \\mathbb E(X) = \\int_{-\\infty}^\\infty x\\, f(x)\\, dx. \\end{equation*}\\] Properties of the expected value (the same as for discrete rv) \\(\\mathbb E(a X + b) = a \\mathbb E (X) + b\\) (linear transformation) \\(\\mathbb E(X+Y) = \\mathbb E(X) + \\mathbb E(Y)\\) (expected value of a sum = sum of expected values) Variance and standard deviation (the same as for discrete rv) Definition 8.5 (Variance and standard deviation (continuous rv)) The variance of a continuous random variable \\(X\\) is \\[\\begin{equation*} \\sigma^2 = \\text{Var}(X) = \\mathbb E((X-\\mathbb E(X))^2)= = \\int_{-\\infty}^\\infty (x-\\mathbb E(X))^2\\, f(x)\\, dx. \\end{equation*}\\] The standard deviation is \\(\\sigma = \\sqrt{\\text{Var}(X)}.\\) Best suited for calculating variance: \\[\\begin{equation*} \\text{Var}(X)=\\mathbb E(X^2) - (\\mathbb E(X))^2. \\end{equation*}\\] For continuous random variables this means: \\[\\begin{align} \\text{Var}(X)&amp;=\\int_{-\\infty}^{\\infty} x^2f(x)dx- (\\mathbb E(X))^2. \\end{align}\\] Properties of Variance \\(\\text{Var}(aX+b)= a^2\\text{Var}(X)\\) (linear transformation) \\(\\text{Var}(X+Y) = \\text{Var}(X) + \\text{Var}(Y)\\) if \\(X\\) and \\(Y\\) are stochastically independent. This results in the calculation rules for linear transformations (\\(Y=a X + b\\)): \\[\\begin{align} \\mathbb E(Y) &amp;= \\mathbb E (a X + b)=a \\mathbb E(X) + b,\\\\ \\text{Var}(Y)&amp;=\\text{Var}(a X + b) = a^2\\text{Var}(X). \\end{align}\\] Example 8.8 (Time in the online shop as a continuous random variable) The random variable \\(X=\\) time in the online shop in minutes has the density: \\[ f(x) = \\begin{cases}0,001\\cdot x , &amp;\\text{ for } 0\\leq x\\leq 20,\\\\ 0,025 -0,00025\\cdot x, &amp;\\text{ for } 20&lt; x\\leq 100.\\end{cases} \\] We have: Expected value: \\[\\begin{align} \\mathbb E(X) &amp;= \\int_{-\\infty}^{\\infty}x\\cdot f(x)dx = \\int_{0}^{20}x\\cdot f(x)dx + \\int_{20}^{100}x\\cdot f(x)dx\\\\ &amp;=\\int_{0}^{20}x\\cdot(0,001\\cdot x )dx + \\int_{20}^{100}x\\cdot (0,025 -0,00025\\cdot x)dx\\\\ &amp;=\\int_{0}^{20}0,001\\cdot x^2 dx + \\int_{20}^{100}0,025\\cdot x -0,00025\\cdot x^2dx\\\\ &amp;=\\Big[0,001\\cdot \\frac{x^3}3\\Big]_{0}^{20} + \\Big[0,025\\cdot \\frac{x^2}2 -0,00025\\cdot \\frac{x^3}3\\Big]_{20}^{100}\\\\ &amp;=2,6667 + 37,3333 = 40. \\end{align}\\] Variance: \\[\\begin{align} \\text{Var}(X) &amp;= \\mathbb E(X^2) - (\\mathbb E(X))^2 = \\int_{-\\infty}^{\\infty}x^2\\cdot f(x)dx - 40^2\\\\ &amp;\\ldots \\text{ (analogous to the expectation calculations above) }\\\\ &amp;=\\int_{0}^{20}0.001\\cdot x^3 dx + \\int_{20}^{100}\\big(0.025\\cdot x^2 -0.00025\\cdot x^3\\big)dx - 40^2\\\\ &amp;=\\Big[0.001\\cdot \\frac{x^4}4\\Big]_{0}^{20} + \\Big[0.025\\cdot \\frac{x^3}3 -0.00025\\cdot \\frac{x^4}4\\Big]_{20}^{100}- 40^2\\\\ &amp;=40 + 2026.6666667 - 40^2= 466.6666667 = (21.6025)^2. \\end{align}\\] Submit Submit 8.2.4 Quantiles of continuous random variables In the first part of statistics course, we determined empirical quantiles. They helped us to understand which value divides the ordered observations so that at least a proportion \\(p\\) of the observations are less than or equal and at least a proportion \\(1-p\\) - greater or equal. In other words, \\(\\hat q_p\\) has answered the question: which value is fallen below with frequency \\(p\\) or exceeded with frequency \\(1-p\\)? The same logic is used to define quantiles for random variables, except that it is now about the probability of falling below or exceeding this value. Definition 8.6 (Quantile (continuous rv)) The \\(p\\)-quantile of a continuous random variable \\(X\\) with density \\(f(x)&gt;0\\) on an interval in \\(\\mathbb R\\) is the number \\(q_p\\) with \\(0&lt;p&lt; 1\\), for which applies: \\[\\begin{equation*} \\mathbb P(X&lt;q_p)=\\mathbb P(X\\leq q_p)=F(q_p)=p. \\end{equation*}\\] The quantile is (for continuous random variables) clearly determined by the inverse of the distribution function, \\[\\begin{equation*} q_p=F^{(-1)}(p). \\end{equation*}\\] Figure 8.9: Quantile of s atandard normal distribution for p=0,9 Special cases: \\(p=0.25\\), then \\(Q_p\\) is called the lower quartile \\(p=0.50\\), then \\(Q_p\\) is called the median \\(p=0.75\\), then \\(Q_p\\) is called the upper quartile Example 8.9 (Time in the online shop as a continuous random variable) Example 8.7 cont. The random variable \\(X=\\) time in the online shop in minutes has the distribution function: \\[\\small{ F(x) = \\begin{cases} 0,&amp;x&lt;0,\\\\ 0,0005x^2,&amp;0\\leq x\\leq 20,\\\\ -0.25 + 0.025x -0.000125x^2,&amp;20&lt;x\\leq 100,\\\\ 1,&amp;x&gt;100. \\end{cases}} \\] \\(0.1\\) quantile: The distribution function takes the value \\(0.1\\) on the intercept \\(0\\leq x \\leq 20\\), so we set \\[F(q_{0.1})=0.1\\rightarrow 0.0005q_{0.1}^ 2 = 0.1\\] and solve for \\(q_{0.1}\\): \\[q_{0.1} = \\pm\\sqrt{\\frac{0.1}{0.0005}} = \\pm 14, 1421,\\] so \\(q_{0.1} =14.1421\\), since \\(0\\leq q_{0.1} \\leq 20\\) must hold. With a probability of 10%, the time spent in the online shop is shorter than \\(14.1421\\) minutes. \\(0.9\\) quantile: The distribution function takes the value \\(0.9\\) on the intercept \\(20\\leq x \\leq 100\\), so we set \\[F(q_{0.9})=0.9\\rightarrow -0.25 + 0.025 q_{ 0.9} - 0.000125q_{0.9}^2 = 0.9.\\] Then (\\(abc\\) formula for quadratic equations \\(ax^2 + bx + c=0\\): \\(x_{1,2}=\\frac{-b\\pm\\sqrt{b^2 - 4ac}}{2a}\\)) we have \\[q_{0.9} = \\frac{-0.025\\pm\\sqrt{0.025^2 - 4\\cdot(-0.000125)\\cdot(-1.15)}}{2\\cdot ( -0.000125)}.\\] In addition, \\(q_{0.9}\\) should be between \\(20\\) and \\(100\\). So, let‚Äôs take the ‚Äú\\(+\\) version‚Äù and calculate \\(q_{0.9} = 71.7157\\). With a probability of 10%, time spent in the online shop is longer than \\(71.7157\\) minutes. 0.1111 0.25 0.3333 0.5 0.6667 Submit 8.3 Some important distributions We have already modeled some random variables and it was always quite time-consuming to derive the distribution of random variables. We had to look at the original random process and determine for each result what value the random variable takes on for this result and derive the probability of the values based on that. Or with the continuous random variables, we had to laboriously calculate the distribution function, the expected value and the variance based on the density üí©. Luckily üòç there are some pretty universal distribution models with variable parameters for which nice properties are already known and key figures can be determined using the parameters, so you don‚Äôt have to use them every time must be derived again. Figure 8.10: Distributional models: Bernoulli, Binomial, and Normal distributions for random variables 8.3.1 Bernoulli distribution Let \\(A\\in\\mathcal F\\) be an event with \\(p=\\mathbb P(A)\\). The random variable \\(X=1_A\\) indicating whether an event \\(A\\) occurs or not (the indicator of \\(A\\)): \\[\\begin{equation*} 1_A= \\begin{cases} 1,&amp;\\text{ if $A$ occurs},\\\\ 0,&amp;\\text{ if $A$ does not occur}. \\end{cases} \\end{equation*}\\] is Bernoulli distributed with parameter \\(p\\) (\\(X\\sim Ber(p)\\)). The indicator function \\(1_A\\) takes the value \\(1\\) if event \\(A\\) occurs and the value \\(0\\) if \\(A\\) does not occur. Examples of \\(A\\): an even number is obtained when rolling the dice; a loan customer does not repay his loan; a patient is cured by a specific therapy; the product of a delivery of goods is defective. A random variable \\(X\\) is Bernoulli distributed with parameter \\(p\\in (0,1)\\) if: Set of values: \\(\\mathcal T=\\{0,1\\}\\) Probability function: \\(f(1)=p\\), \\(f(0)=1-p\\) Parameters: Expected value: \\[\\begin{equation*} \\mathbb E(X)=p. \\end{equation*}\\] Variance: \\[\\begin{equation*} \\text{Var}(X)=p(1-p). \\end{equation*}\\] You can easily check: \\[\\begin{align*} \\mathbb E(X) &amp;= 1\\cdot f(1) + 0\\cdot f(0) = p\\\\[3pt] \\text{Var}(X) &amp;= 1^2\\cdot f(1) + 0^2\\cdot f(0) - (\\mathbb E(X))^2 % = p - p^2 = p(1-p). \\end{align*}\\] Example 8.10 (Indicator of app usage) In example 8.6, we defined the random variable \\(A=\\)‚Äúapp usage indicator‚Äù: \\(A=\\begin{cases}1, &amp;\\text{ if } App\\\\0, &amp;\\text{ if } Web.\\end{cases}\\) \\(A\\) only takes two values \\(\\{0;1\\}\\) and based on the occurrence of the event \\(App=\\)‚Äúa randomly selected customer uses the app‚Äù. The probability \\(\\mathbb P(App)=0.6\\). This means: \\(A\\sim Ber(p=0.6)\\) \\(\\mathbb E(A) = 0.6\\) and \\(\\text{Var}(A) = 0.6\\cdot (1-0.6) = 0.24.\\) We took a little longer when we had to calculate these metrics ‚Äúfrom scratch‚Äù in 8.6 üòè Figure 8.11: Bernoulli distribution with p=0.6 8.3.2 Binomial distribution The binomial distribution results when the sum of hits is formed from \\(n\\) independent repetitions of a Bernoulli experiment with constant probability \\(p\\). I.e., if a Bernoulli random process is repeated \\(n\\) times with constant probability \\(p=\\mathbb P(A)\\), then is the random variable \\[\\begin{equation*} X=\\text{&quot;Number of attempts in which $A$ occurs&#39;&#39;} \\end{equation*}\\] binomially distributed with parameters \\(n\\) and \\(p\\). A random variable is called binomially distributed with the parameters \\(n\\) and \\(p\\), for short \\(X\\sim B(n,p)\\), if the following applies: \\[\\begin{equation*} \\mathbb P(X=k) = f(k) = \\binom{n}{k} p^k\\, (1-p)^{n-k},\\quad\\text{ for } k=0,1,\\ldots, n, \\end{equation*}\\] and \\(f(k)=0\\) otherwise.Remember: Binomial coefficient: \\(\\displaystyle\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\\) with \\(n!=1\\cdot 2\\cdot \\ldots \\cdot n\\) and \\(0!=1\\). Parameters: \\[\\begin{equation*} \\mathbb E(X) = np\\quad\\text{ and }\\quad \\text{Var}(X) = n p(1-p) \\end{equation*}\\] Additive property: The sum of independent binomial distributed random variables with the same parameter \\(p\\) is again binomially distributed: if \\(X\\sim B(n,p)\\) and \\(Y\\sim B(m,p)\\) are independent, then \\(X+Y\\sim B(n+m,p)\\). Application examples: Number of loans in a portfolio that are not being serviced (assumption: loans default independently of each other.) Number of patients cured by therapy; Number of products in a delivery that are defective. Example 8.11 (Number of app users) If we select \\(n=10\\) customers randomly (with replacement) from the customer pool and state whether they are app users or not, then we repeat the Bernoulli process: ‚ÄòDrawing a random customer and determining the \\(App\\)-property‚Äô independently ten times. The probability of success, here the probability of selecting an app user, corresponds to \\(\\mathbb P(App)=0.6\\) the proportion of app users in the entire customer pool. So \\(X=\\)‚ÄôThe number of app users among the randomly selected ten customers‚Äô is binomially distributed: \\(X\\sim B(\\color{green}{n=10}; \\color{blue}{p=0.6})\\) We can now calculate the probabilities üòé: \\[\\begin{align} \\mathbb P(X=\\color{red}{5}) &amp;=\\binom{\\color{green}{10}}{\\color{red}{5}} \\color{blue}{0.6}^\\color{red}{5}\\,(1-\\color{blue}{0.6})^{\\color{green}{10}-\\color{red}{5}} = 0.2007,\\\\ \\mathbb P(X&lt;2) &amp;=\\mathbb P(X=0) + \\mathbb P(X=1)=\\binom{\\color{green}{10}}{0} \\color{blue}{0.6}^0\\cdot(1-\\color{blue}{0.6}) ^{{\\color{green}{10}}-0} + \\binom{\\color{green}{10}}{1} \\color{blue}{0.6}^1\\,(1-\\color{blue}{0.6})^{\\color{green}{10}-1} \\\\ &amp;= 0.0017,\\\\ \\mathbb P(X=8) &amp;=\\binom{\\color{green}{10}}{8} \\color{blue}{0.6}^8\\,(1-\\color{blue}{0.6})^{\\color{green}{10}-8} = 0.1209. \\end{align} \\] Parameters: \\[\\begin{align} \\mathbb E(X) &amp;= n\\cdot \\color{blue}{p} = 10\\cdot \\color{blue}{0.6} = 6\\\\ \\text{Var}(X)&amp;= n\\cdot \\color{blue}{p}\\cdot(1-\\color{blue}{p}) = 10\\cdot \\color{blue}{0.6}\\cdot (1-\\color{blue}{0.6}) = 2.4. \\end{align}\\] Figure 8.12: Binomial distribution with n=10 and p=0.6 Example 8.12 (Loan portfolio) Consider a homogeneous loan portfolio with \\(500\\) loans and default probability of \\(1\\)%. Random variable \\(A = \\text{&#39;default of a loan&#39;}\\) with values \\(a\\in \\{0 (\\text{no default}),1 (\\text{default})\\}\\) is Bernoulli distributed with \\(p=0.01\\). Random variable \\(P = \\text{&#39;Number of defaults in the loan portfolio&#39;}\\) (assumptions: defaults occur independently of one another). Under the assumptions, \\(P\\) is \\(\\text{binomially distributed}\\) with \\(\\color{blue}{p=0.01}\\) and \\(\\color{green}{n=500}\\): \\(P\\sim B(500,0.01)\\). \\[\\begin{align*} \\mathbb E(P) &amp;= \\color{green}{n}\\color{blue}{p} = \\color{green}{500}\\cdot \\color{blue}{0.01} = 5,\\\\ \\text{Var}(P) &amp;=\\color{green}{n}\\color{blue}{p}(1-\\color{blue}{p})=\\color{green}{500}\\cdot \\color{blue}{0.01}\\cdot (1-\\color{blue}{0.01}) = 4.95,\\\\ \\mathbb P(P&gt;1) &amp;=1-\\mathbb P(P\\leq 1) =1-\\sum_{k=0}^1\\binom{n}{k}p^k(1-p)^{n-k} \\\\ &amp;=1- \\Big(\\frac{500!}{0!500!}(\\color{blue}{0,01})^0(1-\\color{blue}{0.01})^{500} + \\frac{500!}{1!499!}(\\color{blue}{0.01})^1(1-\\color{blue}{0.01})^{499}\\Big)=0,9602. \\end{align*}\\] Figure 8.13: Binomial distribution with n=500 and p=0.01 Calculating with binomial distribution Example 8.11: \\(X\\sim B(n=10; p=0.6)\\) \\(\\rightarrow\\mathbb P(X=5)\\) using gretl\\(\\rightarrow\\)Tools\\(\\rightarrow\\) p-value finder: - Example 8.11: \\(X\\sim B(n=10; p=0.6)\\) \\(\\rightarrow\\mathbb P(X&lt;2)\\) using \\(\\mathbb P(X&lt;2)=\\mathbb P(X\\leq 1)\\): Example 8.12: \\(X\\sim B(n=500; p=0.01)\\) \\(\\rightarrow\\mathbb P(X&gt;1)\\): Submit Submit \\()\\) Submit Submit Submit Submit \\()\\) Submit Submit 8.3.3 Normal distribution The normal distribution is the best known and most important distribution. In many applications, the empirical distribution of a feature can be approximated sufficiently well using a normal distribution. In particular, the normal distribution is a good model if the underlying variable results from the additive overlap of a large number of independent, random individual effects. The theoretical justification for this empirically observable phenomenon is provided by the Central Limit Theorem (\\(\\leadsto\\) next chapter) The normal distribution is often called the Gauss distribution, named after the mathematician Carl Friedrich Gau√ü (1777‚Äì1855). A random variable \\(X\\) is said to be normally distributed with parameters \\(\\mu\\in \\mathbb R\\) and \\(\\sigma&gt;0\\), for short \\(X\\sim N(\\mu,\\sigma^2)\\), if it has the has the following density: \\[\\begin{equation*} \\varphi(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right), \\quad x\\in \\mathbb R. \\end{equation*}\\] Key parameters: \\[\\begin{equation*} \\mathbb E(X)=\\mu\\quad\\text{ and }\\quad \\text{Var}(X)=\\sigma^2. \\end{equation*}\\] With \\(\\mu=0\\) and \\(\\sigma^2=1\\) you get the standard normal distribution \\(N(0,1)\\) with density \\[\\begin{equation*} \\varphi(x)=\\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right), \\quad x\\in \\mathbb R. \\end{equation*}\\] The density of the standard normal distribution is denoted as \\(\\varphi(x)\\) and the distribution function as \\(N(x)\\). Properties of normal distribution The density is symmetrical about \\(\\mu\\), i.e. \\[\\begin{equation*} \\varphi(\\mu-x) = \\varphi(\\mu+x), \\quad x\\in \\mathbb R. \\end{equation*}\\] In particular in the case \\(\\mu=0\\): \\(\\varphi(-x)=\\varphi(x)\\). The following applies to the distribution function of the standard normal distribution: \\(N(-x) = 1- N(x)\\) Standardization: \\(X\\) is \\(N(\\mu,\\sigma^2)\\)-distributed \\(\\iff\\) \\(\\displaystyle\\frac{X-\\mu}{\\sigma}\\) is \\(N(0,1)\\)-distributed . \\(Y\\) is \\(N(0,1)\\)-distributed \\(\\iff\\) \\(\\mu + \\sigma Y\\) is \\(N(\\mu,\\sigma^2)\\)-distributed. Probabilities within \\(\\sigma\\) bounds \\[\\begin{align*} \\mathbb P(|X-\\mu|\\leq \\sigma) &amp;= 0.6827\\\\ \\mathbb P(|X-\\mu|\\leq 2\\sigma) &amp;= 0.9545\\\\ \\mathbb P(|X-\\mu|\\leq 3\\sigma) &amp;= 0.9973 \\end{align*}\\] Additive property: If \\(X\\sim N(\\mu_X,\\sigma_X^2)\\) and \\(Y\\sim N(\\mu_Y,\\sigma_Y^2)\\) are independent, then \\[\\begin{equation*} X+Y\\sim N(\\mu_X+\\mu_Y, \\sigma_X^2+\\sigma_Y^2). \\end{equation*}\\] Probability with which an \\(N(\\mu,\\sigma^2)\\)-distributed random variable lies between the boundaries \\(a\\) and \\(b\\): \\[\\begin{equation*} \\mathbb P(a\\leq X\\leq b) = \\mathbb P\\left(\\frac{a-\\mu}{\\sigma}\\leq \\frac{X-\\mu}{\\sigma} \\leq \\frac{b-\\mu}{\\sigma}\\right) = N\\left(\\frac{b-\\mu}{\\sigma}\\right) - N\\left(\\frac{a-\\mu}{\\sigma}\\right) \\end{equation*}\\] Example 8.13 (Average temperature) We model the average temperature in Berlin in July as a random variable \\(T: \\text{&#39;Average temperature in Berlin in July&#39;}\\) with values \\(t\\in \\mathbb R\\). \\(T\\stackrel{}{\\sim}{} N(\\mu_T,\\sigma_T)\\) with \\(\\mu_T=\\mathbb E(T)=20\\), \\(\\text{Var}(T)=16\\) and \\(\\sigma_T = 4\\). Probability: \\[\\begin{align*} \\mathbb P(|T-20|\\leq 4) &amp;\\approx 0.68,\\\\ \\mathbb P(|T-20|\\leq 8) &amp;\\approx 0.95,\\\\ \\mathbb P(15\\leq T\\leq 25)&amp; = \\mathbb P(T\\leq 25)-\\mathbb P(T&lt;15) =N\\Big(\\frac{25-20}{4}\\Big) - N\\Big(\\frac{15-20}{4}\\Big)\\\\ &amp; = 0.8944 - 0.1056 = 0.7887. \\end{align*}\\] Figure 8.14: Distributional models for continuous random variables Calculating with normal distribution The distribution function \\(N(x)\\) of a (standard) normally distributed random variable does not have a closed form. Therefore, we use statistical software to calculate the probabilities and to obtain the quantiles. Probabilities (Tools\\(\\rightarrow\\) p-value finder) Quantiles (Tools\\(\\rightarrow\\) statistical tables) Let‚Äôs answer some quiz questions to practice calculations with normal distribution. Submit Submit Submit Submit \\(B(26,0.0228)\\) \\(B(52;0.0228)\\) \\(N(26,25)\\) \\(N(52,5)\\) \\(N(52,25)\\) Submit "],["iii-introduction-to-inferential-statistics.html", "III Introduction to inferential statistics", " III Introduction to inferential statistics In the descriptive statistics part of the course, we analyzed existing data given in a particular sample, which was a small randomly selected part of the population. Probability modelling helped us to understand how the random sampling process can be described in terms of probabilities and what happens during random sampling. Now we turn to the essential problem, of how to infer unknown parameters of the population using the limited sample. Our key question is: ‚ÄúHow and under what conditions can we draw valid conclusions about the population based on estimates obtained from a single sample?‚Äù. "],["parameter-estimation.html", "Chapter 9 Parameter estimation 9.1 Basic ideas of inductive statistics 9.2 Estimating the expectation \\(\\mu\\) of a normal population", " Chapter 9 Parameter estimation To answer the question on inferences about the population regarding parameter estimation, let‚Äôs take a close look at distributions of estimators like \\(\\bar X\\) to find some criteria of ‚Äúgood‚Äù estimators to define and quantify the uncertainty of the estimate. In this chapter we will learn: How random sampling results in estimators, that are random variables, What is the distribution of the sample mean in case of normal distribution in the population, How to take the uncertainty of the estimate into account in the estimation procedure \\(\\leadsto\\) interval estimation. 9.1 Basic ideas of inductive statistics In inductive statistics, we want to generalize our findings, which are based on a single sample, on the whole population of interest. With the help of random sampling and appropriate estimators, we conduct inference for parameters of interest in the population. Exercise 9.1 (Basic Concepts) 9.1.1 Parameter estimation theoretically Before we conduct any data analysis, we think of a model for a real-world problem. Example 9.1 (New customers) You want to expand your customer base to a specific population group and are interested in: to what proportion will the new customers use the app (\\(Y\\)), specifically: what would be the proportion of app users in your new customer base (\\(p\\)), what average sales amount can you expect from new customers (\\(X\\)), specifically: what would be the average sales amount of your new customers (\\(\\mathbb E(X)=\\mathbb \\mu_X\\)). \\(X\\) and \\(Y\\) are random variables. We are interested in the distribution parameters of these random variables. We will build upon the following concept of a random sample‚Ä¶ Modeling the quantities of interest App usage is the property of interest, so: \\[ Y=\\begin{cases}1,&amp;\\text{if app usage},\\\\0,&amp;\\text{no app usage}.\\end{cases} \\] \\(Y\\sim Ber(p)\\). Of interest is the share of app users (\\(p\\)). sales amount is size \\(X\\) and the average sales amount is of interest: \\(\\mathbb E(X)=\\mu_x\\). Random sample We have often talked about a sample and have seen how sampling can be modeled (See ‚ÄúRandom events and Probability‚Äù). Now we formally define what we mean by a random sample as a random process. Let \\(X_1,\\ldots, X_n\\) be random variables. \\((X_1,\\ldots, X_n)\\) is called a random sample of size \\(n\\) (sample variables) to \\(X\\) if the distributions of \\(X\\) and \\(X_i\\), \\(i=1,2,\\ldots, n\\), match and \\(X_1,\\ldots, X_n\\) are independent. The observed realization \\((x_1,\\ldots, x_n)\\) of random sample \\((X_1,\\ldots, X_n)\\) is called a sample realization. The set \\(\\mathcal X\\) of all possible sample realizations is called the sample space: \\(\\mathcal X\\subseteq\\mathbb R^n\\). Example 9.2 (New customers) Example 9.1 cont. The plan is to randomly select \\(n\\) customers and survey them. App Usage: The sample variables for \\(Y\\) are: \\(\\color{blue}{Y_1, Y_2,\\ldots,Y_n},\\) where \\(Y_i\\) takes the value 1 or 0, depending on whether the \\(i\\)th selected person uses the app or not. So the value of \\(Y_i\\) is initially unknown because we don‚Äôt know which person will be interviewed in the \\(i\\)th position. \\(\\leadsto\\) \\(Y_1,\\ldots, Y_n\\) are random variables! sales amount: The sample variables for \\(X\\) are: \\({\\color{red}{X_1, X_2,\\ldots,X_n}},\\) where \\(X_i\\) represents the estimate of the bill amount of the \\(i\\)th selected person. So the value of \\(X_i\\) is initially unknown because we don‚Äôt know which person will be interviewed in the \\(i\\)th position. \\(\\leadsto\\) \\(X_1,\\ldots, X_n\\) are random variables! Estimation In descriptive statistics we have used the formulas for computing the empirical mean and the empirical frequencies in order to summarize the sample given. Now we look at those formulas from the probabilistic point of view, realize that there are many possible samples from a population and the formulas (which correspond to estimators) return different values depending on a particular sample realization. Example 9.3 (New customers) Example 9.2 cont. Calculation formulas (estimators) for the parameters of interest: the app user share \\(p\\): \\[\\begin{align} \\color{blue}{\\overline{Y}}&amp;\\color{blue}{=\\frac 1n (Y_1 + Y_2 + \\ldots Y_n)} \\leadsto\\color{blue}{\\frac{\\text{Number of 1s (=app usage)}}{\\text{Number of values}}}\\\\ &amp;\\color{blue}{= \\text{Sample frequency of ones (=app usage)}} \\end{align}\\] the average sales amount (\\(\\mathbb E(X)\\)): \\[\\color{red}{\\overline{X}=\\frac 1n (X_1 + X_2 + \\ldots X_n) \\leadsto\\text{sample mean}}.\\] \\(\\overline{Y}\\) and \\(\\overline{X}\\) are functions of the sample variables, so they are themselves random variables. 9.1.2 Parameter estimation practically Let us return to our estimation problem. Assume the proportion of app users is \\(0.6\\) and the mean sales amount in the population of all potential new customers is \\(25\\) Euro. In the reality we do not know those values and we have to estimate them. So imagine, we conduct a survey among the potential new customers and ask the survey participants to indicate whether they would use the app or not, and how high would be their willingness to pay. Example 9.4 (New customers) Example 9.3 cont. We need to set the sample size (\\(n\\)) and then conduct a survey to get concrete sample values (\\(x_i\\) and \\(y_i\\) for \\(i=1,\\ldots,n\\)). In the animation you can see what happens when a sample is taken: Sample size \\(n\\): 10 20 50 100 Sample realizations: The survey results for App usage (yes/no) as realized sample values: \\(\\color{blue}{\\{1,2,\\ldots, n\\}=} ~\\) As realizations of the corresponding 0-1 Bernoulli random variable: \\(\\color{blue}{\\{y_1,y_2,\\ldots,y_n\\}=}~\\) Willingness to pay in form of self-estimated sales amount with sample values: \\(\\color{red}{\\{x_1,x_2,\\ldots,x_n\\}=}~\\) Calculation of the resulting estimates: The estimate for the proportion of app users: \\(\\color{blue}{\\bar y=\\frac 1n(y_1+y_2 + \\ldots + y_n)=}\\) The estimate for the average sales amount: \\(\\color{red}{\\bar x=\\frac 1n(x_1+x_2 + \\ldots + x_n)= }\\) draw a new sample The estimates change the sample. The estimated values change from sample to sample. The estimation result deviates from the true parameter. The estimation result perfectly agrees with the population parameter to be estimated. Submit Nothing changes. Precision increases as the sample size increases. The estimates vary more from sample to sample. The estimates vary less from sample to sample. Submit 9.2 Estimating the expectation \\(\\mu\\) of a normal population In the last section, we considered the normal distribution for modelling random variables. We have seen, that addition and scaling of normal random variables results again in a normally distributed random variable. Let us look at a sample from a normal distribution and consider estimating its expectation/ mean \\(\\mu\\) from that sample. A natural estimator for \\(\\mu\\) is the sample mean \\(\\bar X\\). For a concrete sample, it takes on a value, \\(\\bar x\\), but over all possible samples, it is a random variable with a distribution. We can actually derive its distribution for a normal population using the properties of normal distribution. 9.2.1 Distribution of the sample mean for a normal population If we make a random selection (with replacement) from a normally distributed population and write down a possible result as \\(n\\) random variable ‚Äúplaceholders‚Äù as \\(X_1, X_2, \\ldots, X_n\\), or calculate the mean \\(\\bar X\\) , then: \\(X_i\\sim N(\\mu_X,\\sigma_X^2)\\) by assumption, since we are selecting from a normally distributed population. \\(X_i\\) independent, as you move with replacement \\(\\overline X = \\frac 1n\\sum_{i=1}^n\\) using the additive property of the normal distribution, \\(\\overline X\\sim N(\\mu_X,\\frac{\\sigma_X^2}n)\\), since \\(\\mathbb E(\\overline X) = \\frac 1n\\cdot n\\cdot \\mu_X=\\color{blue}{\\mu_X}\\), \\({\\text{Var}(\\overline X)}=\\frac 1{n^2}\\cdot n\\cdot \\sigma_X^2=\\color{red}{\\frac {\\sigma^2_X}{n}}\\), and \\(\\sqrt{\\text{Var}(\\overline X)} = \\color{red}{\\frac {\\sigma_X}{\\sqrt n}}\\). Hence the sample mean is distributed as follows: \\[\\overline X\\sim N\\left (\\color{blue}{\\mu_X};\\color{red}{\\frac {\\sigma^2_X}{n}}\\right),\\] and the standardized sample mean: \\[\\frac{\\overline X-\\color{blue}{\\mu_X}}{\\color{red}{\\frac {\\sigma_X}{\\sqrt n}}}\\sim N(0;1).\\] Submit Submit Submit 9.2.2 Constructing a confidence interval for \\(\\mu\\) of a normal population We have figured out that our estimator \\(\\bar X\\) for \\(\\mu\\) is a random variable and its values vary from sample to sample. This means, that whenever we have a fixed sample, we should still anticipate this variability in our conclusions. One way to do it, is to construct a confidence interval for \\(\\mu\\), which covers the true parameter with a high probability. An interval means, that besides the estimate \\(\\bar x\\), we can calculate a lower and an upper bound (realized confidence interval) for \\(\\mu\\). Confidence intervals cover the true parameter with a certain high probability, chosen by the person conducting the analysis. Normally this probability is \\(90\\%,95\\%,99\\%.\\) The remaining probability of a failure to cover is denoted as \\(\\alpha\\) and takes on values \\(10\\%,5\\%,1\\%\\) respectively. The coverage probability is then \\(1-\\alpha\\). Again, we assume that we sample from a normally distributed population with unknown \\(\\mu\\) but known variance \\(\\sigma^2\\). Thus, we would like to conduct interval estimation for \\(X\\sim N(\\mu,\\sigma^2)\\). The starting point is the point estimator \\(\\overline X\\), which is \\(N(\\mu,\\sigma^2/n)\\)-distributed. The standardized sample mean \\(\\displaystyle\\frac{\\overline X-\\mu}{\\sigma/\\sqrt{n}}\\) is \\(N(0,1)\\)-distributed, therefore: \\[\\begin{equation*} \\mathbb P\\left(-N_{1-\\alpha/2} \\leq \\frac{\\overline X-\\mu}{\\sigma/\\sqrt{n}}\\leq N_{1-\\alpha/2}\\right)=1-\\alpha, \\end{equation*}\\] with \\(N_{1-\\alpha/2}\\) the \\(1-\\alpha/2\\) quantile of the standard normal distribution. Rearranging the term inside the probability results in: \\[\\begin{equation*} 1-\\alpha = \\mathbb P\\left(\\overline X - N_{1-\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\overline X + N_{1-\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\right), \\end{equation*}\\] and the \\((1-\\alpha)\\) confidence interval is given by: \\[\\begin{equation*} \\left[\\overline X - N_{1-\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}, \\overline X + N_{1-\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\right]. \\end{equation*}\\] The realized confidence interval can be computed as: \\[\\begin{equation*} \\left[\\overline x - N_{1-\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}, \\overline x + N_{1-\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\right], \\end{equation*}\\] where \\(\\overline x\\) and \\(n\\) are taken from the given sample. Example 9.5 (Monthly returns) Compute the \\(95\\%\\)-confidence interval for the mean of monthly stock returns. random monthly return \\(X\\sim N(\\mu,\\sigma^2)\\) with known \\(\\sigma^2 = 0.25\\) Data: \\(12\\) monthly returns in \\(\\%\\) \\[\\begin{array}{c|cccccccccccc} i&amp;1&amp;2&amp;3&amp;4&amp;5&amp;6&amp;7&amp;8&amp;9&amp;10&amp;11&amp;12\\\\\\hline x_i&amp;0.35&amp;0.89&amp;1.02&amp;0.52&amp;0.12&amp;-0.22&amp;-0.48&amp;-0.37&amp;0.07&amp;0.29&amp;0.41&amp;0.72 \\end{array}\\] Confidence level \\(1-\\alpha=0.95\\) \\(\\Rightarrow\\) error probability \\(\\alpha=0.05.\\) Sample mean: \\(\\displaystyle \\overline x=\\frac 1{12}(0.35 + \\ldots + 0.72)=\\frac{3.32}{12}=0.2767\\%\\) \\((1-\\alpha/2)\\)-quantile \\(N_{1-\\alpha/2} = N_{0.975} =1.96\\) Realized confidence interval: \\[\\begin{align*} &amp;\\left[\\overline x - N_{1-\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}, \\overline x + N_{1-\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\right] = \\\\ &amp;\\left[0.2767-1.96\\cdot \\frac{\\sqrt{0.25}}{\\sqrt{12}}; 0.2767+ 1.96\\cdot\\frac{\\sqrt{0.25}} {\\sqrt{12}}\\right] = [-0.0062\\%; 0.5596\\%] \\end{align*}\\] Submit "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
